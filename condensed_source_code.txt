# Condensed Python Source Files
# Generated: 2025-10-27T23:48:40.359786
# Root directory: .
# Total files: 64
================================================================================

# TABLE OF CONTENTS
================================================================================
  1. alembic/env.py
  2. condense_python.py
  3. condense_python_source_only.py
  4. main.py
  5. scripts/ai_manual_check.py
  6. src/zistudy_api/__init__.py
  7. src/zistudy_api/api/__init__.py
  8. src/zistudy_api/api/dependencies.py
  9. src/zistudy_api/api/routes/__init__.py
 10. src/zistudy_api/api/routes/ai.py
 11. src/zistudy_api/api/routes/answers.py
 12. src/zistudy_api/api/routes/auth.py
 13. src/zistudy_api/api/routes/jobs.py
 14. src/zistudy_api/api/routes/study_cards.py
 15. src/zistudy_api/api/routes/study_sets.py
 16. src/zistudy_api/api/routes/tags.py
 17. src/zistudy_api/app.py
 18. src/zistudy_api/celery_app.py
 19. src/zistudy_api/config/__init__.py
 20. src/zistudy_api/config/settings.py
 21. src/zistudy_api/core/__init__.py
 22. src/zistudy_api/core/logging.py
 23. src/zistudy_api/core/security.py
 24. src/zistudy_api/db/__init__.py
 25. src/zistudy_api/db/migrations.py
 26. src/zistudy_api/db/models.py
 27. src/zistudy_api/db/repositories/__init__.py
 28. src/zistudy_api/db/repositories/answers.py
 29. src/zistudy_api/db/repositories/api_keys.py
 30. src/zistudy_api/db/repositories/jobs.py
 31. src/zistudy_api/db/repositories/refresh_tokens.py
 32. src/zistudy_api/db/repositories/study_cards.py
 33. src/zistudy_api/db/repositories/study_sets.py
 34. src/zistudy_api/db/repositories/tags.py
 35. src/zistudy_api/db/repositories/users.py
 36. src/zistudy_api/db/session.py
 37. src/zistudy_api/domain/__init__.py
 38. src/zistudy_api/domain/enums.py
 39. src/zistudy_api/domain/schemas/__init__.py
 40. src/zistudy_api/domain/schemas/ai.py
 41. src/zistudy_api/domain/schemas/answers.py
 42. src/zistudy_api/domain/schemas/auth.py
 43. src/zistudy_api/domain/schemas/base.py
 44. src/zistudy_api/domain/schemas/common.py
 45. src/zistudy_api/domain/schemas/jobs.py
 46. src/zistudy_api/domain/schemas/study_cards.py
 47. src/zistudy_api/domain/schemas/study_sets.py
 48. src/zistudy_api/domain/schemas/tags.py
 49. src/zistudy_api/services/__init__.py
 50. src/zistudy_api/services/ai/__init__.py
 51. src/zistudy_api/services/ai/agents.py
 52. src/zistudy_api/services/ai/clients.py
 53. src/zistudy_api/services/ai/generation_service.py
 54. src/zistudy_api/services/ai/pdf.py
 55. src/zistudy_api/services/ai/pdf_strategies.py
 56. src/zistudy_api/services/ai/prompts.py
 57. src/zistudy_api/services/answers.py
 58. src/zistudy_api/services/auth.py
 59. src/zistudy_api/services/job_processors.py
 60. src/zistudy_api/services/jobs.py
 61. src/zistudy_api/services/study_cards.py
 62. src/zistudy_api/services/study_sets.py
 63. src/zistudy_api/services/tags.py
 64. src/zistudy_api/tools.py

================================================================================


================================================================================
# FILE: alembic/env.py
================================================================================

from __future__ import annotations

import asyncio
from logging.config import fileConfig

from alembic import context
from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config

from zistudy_api.config.settings import get_settings
from zistudy_api.db import Base

config = context.config

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = Base.metadata


def get_database_url() -> str:
    settings = get_settings()
    print(f"[alembic] resolved database url: {settings.database_url}")
    return settings.database_url


def run_migrations_offline() -> None:
    url = get_database_url()
    print(f"[alembic] offline migrations using {url}")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


async def run_migrations_online() -> None:
    configuration = config.get_section(config.config_ini_section, {})
    configuration["sqlalchemy.url"] = get_database_url()

    connectable = async_engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.begin() as connection:
        print("[alembic] running migrations online")
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()


def do_run_migrations(connection: Connection) -> None:
    context.configure(connection=connection, target_metadata=target_metadata)

    with context.begin_transaction():
        print("[alembic] applying migrations...")
        context.run_migrations()


def main() -> None:
    if context.is_offline_mode():
        run_migrations_offline()
    else:
        asyncio.run(run_migrations_online())


if __name__ == "__main__":
    main()


================================================================================
# FILE: condense_python.py
================================================================================

#!/usr/bin/env python3
"""
Script to condense all Python files in the project into a single file.
"""

import os
from pathlib import Path
from datetime import datetime


def condense_python_files(root_dir: str, output_file: str):
    """
    Condense all Python files in the directory into one file.

    Args:
        root_dir: Root directory to search for Python files
        output_file: Path to the output file
    """
    root_path = Path(root_dir)
    python_files = sorted(root_path.rglob("*.py"))

    # Filter out the output file itself and __pycache__ directories
    python_files = [
        f for f in python_files
        if "__pycache__" not in str(f) and f.name != os.path.basename(output_file)
    ]

    with open(output_file, "w", encoding="utf-8") as outfile:
        # Write header
        outfile.write(f"# Condensed Python Files\n")
        outfile.write(f"# Generated: {datetime.now().isoformat()}\n")
        outfile.write(f"# Root directory: {root_dir}\n")
        outfile.write(f"# Total files: {len(python_files)}\n")
        outfile.write("=" * 80 + "\n\n")

        for py_file in python_files:
            relative_path = py_file.relative_to(root_path)

            # Write file separator and header
            outfile.write("\n" + "=" * 80 + "\n")
            outfile.write(f"# FILE: {relative_path}\n")
            outfile.write("=" * 80 + "\n\n")

            try:
                with open(py_file, "r", encoding="utf-8") as infile:
                    content = infile.read()
                    outfile.write(content)

                    # Ensure file ends with newline
                    if content and not content.endswith("\n"):
                        outfile.write("\n")

            except Exception as e:
                outfile.write(f"# ERROR reading file: {e}\n")

            outfile.write("\n")

    print(f"Successfully condensed {len(python_files)} Python files into {output_file}")
    print(f"\nFiles included:")
    for i, py_file in enumerate(python_files, 1):
        print(f"  {i}. {py_file.relative_to(root_path)}")


if __name__ == "__main__":
    # Current directory
    current_dir = "."
    output_path = "condensed_python_code.txt"

    print(f"Condensing Python files from: {os.path.abspath(current_dir)}")
    print(f"Output file: {os.path.abspath(output_path)}\n")

    condense_python_files(current_dir, output_path)

    # Show file size
    file_size = os.path.getsize(output_path)
    print(f"\nOutput file size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)")


================================================================================
# FILE: condense_python_source_only.py
================================================================================

#!/usr/bin/env python3
"""
Script to condense Python source files (excluding venv, tests, etc.) into a single file.
"""

import os
from pathlib import Path
from datetime import datetime


def should_exclude(path: Path) -> bool:
    """
    Check if a path should be excluded from condensing.

    Args:
        path: Path to check

    Returns:
        True if the path should be excluded
    """
    exclude_patterns = [
        ".venv",
        "__pycache__",
        ".git",
        ".pytest_cache",
        "node_modules",
        ".mypy_cache",
        ".ruff_cache",
        "tests",  # Exclude tests
        "alembic/versions",  # Exclude migration files
    ]

    path_str = str(path)
    return any(pattern in path_str for pattern in exclude_patterns)


def condense_python_files(root_dir: str, output_file: str):
    """
    Condense all Python source files into one file.

    Args:
        root_dir: Root directory to search for Python files
        output_file: Path to the output file
    """
    root_path = Path(root_dir)
    all_python_files = sorted(root_path.rglob("*.py"))

    # Filter out excluded directories and the output file itself
    python_files = [
        f for f in all_python_files
        if not should_exclude(f) and f.name != os.path.basename(output_file)
    ]

    with open(output_file, "w", encoding="utf-8") as outfile:
        # Write header
        outfile.write(f"# Condensed Python Source Files\n")
        outfile.write(f"# Generated: {datetime.now().isoformat()}\n")
        outfile.write(f"# Root directory: {root_dir}\n")
        outfile.write(f"# Total files: {len(python_files)}\n")
        outfile.write("=" * 80 + "\n\n")

        # Write table of contents
        outfile.write("# TABLE OF CONTENTS\n")
        outfile.write("=" * 80 + "\n")
        for i, py_file in enumerate(python_files, 1):
            relative_path = py_file.relative_to(root_path)
            outfile.write(f"{i:3d}. {relative_path}\n")
        outfile.write("\n" + "=" * 80 + "\n\n")

        # Write file contents
        for py_file in python_files:
            relative_path = py_file.relative_to(root_path)

            # Write file separator and header
            outfile.write("\n" + "=" * 80 + "\n")
            outfile.write(f"# FILE: {relative_path}\n")
            outfile.write("=" * 80 + "\n\n")

            try:
                with open(py_file, "r", encoding="utf-8") as infile:
                    content = infile.read()
                    outfile.write(content)

                    # Ensure file ends with newline
                    if content and not content.endswith("\n"):
                        outfile.write("\n")

            except Exception as e:
                outfile.write(f"# ERROR reading file: {e}\n")

            outfile.write("\n")

    print(f"âœ“ Successfully condensed {len(python_files)} Python files into {output_file}")
    print(f"\nFiles included:")
    for i, py_file in enumerate(python_files, 1):
        print(f"  {i:3d}. {py_file.relative_to(root_path)}")


if __name__ == "__main__":
    # Current directory
    current_dir = "."
    output_path = "condensed_source_code.txt"

    print(f"Condensing Python source files from: {os.path.abspath(current_dir)}")
    print(f"Output file: {os.path.abspath(output_path)}")
    print(f"Excluding: .venv, tests, __pycache__, alembic/versions, etc.\n")

    condense_python_files(current_dir, output_path)

    # Show file size
    file_size = os.path.getsize(output_path)
    print(f"\nâœ“ Output file size: {file_size:,} bytes ({file_size / 1024:.2f} KB)")


================================================================================
# FILE: main.py
================================================================================

from __future__ import annotations

import os
import signal
import subprocess
import sys
import time
from pathlib import Path

import uvicorn
from pydantic import ValidationError

from zistudy_api.config.settings import Settings, get_settings


def _celery_worker_main(loglevel: str) -> None:
    from zistudy_api.celery_app import celery_app

    celery_app.worker_main(
        [
            "worker",
            f"--loglevel={loglevel}",
        ]
    )


def _run_migrations(max_attempts: int = 8, base_delay: float = 1.0) -> None:
    from asyncpg import PostgresError
    from sqlalchemy.exc import OperationalError

    from zistudy_api.db.migrations import run_migrations

    attempt = 1
    while True:
        try:
            run_migrations()
            return
        except (OperationalError, PostgresError) as exc:
            if attempt >= max_attempts:
                raise
            wait = base_delay * attempt
            print(
                f"[main] Database not ready (attempt {attempt}/{max_attempts}): {exc}. "
                f"Retrying in {wait:.1f}s...",
                file=sys.stderr,
            )
            time.sleep(wait)
            attempt += 1


def _run_api(settings: Settings) -> None:
    from fastapi import FastAPI

    from zistudy_api.app import app
    reload_enabled = settings.environment == "local"
    host = settings.api_host
    port = settings.api_port
    app_target: FastAPI | str = "zistudy_api.app:app" if reload_enabled else app
    uvicorn.run(
        app_target,
        host=host,
        port=port,
        log_level=settings.log_level.lower(),
        reload=reload_enabled,
    )


def _run_worker(settings: Settings) -> None:
    if settings.environment == "local":
        from watchfiles import run_process

        run_process(
            Path.cwd(),
            target=_celery_worker_main,
            args=(settings.celery_loglevel,),
            target_type="function",
        )
        return

    _celery_worker_main(settings.celery_loglevel)


def _start_worker_subprocess(settings: Settings) -> subprocess.Popen[bytes]:
    try:
        env = os.environ.copy()
        if settings.environment == "local":
            command = (
                f"celery -A zistudy_api.celery_app:celery_app worker "
                f"--loglevel={settings.celery_loglevel}"
            )
            return subprocess.Popen(
                [
                    sys.executable,
                    "-m",
                    "watchfiles",
                    "--filter",
                    "python",
                    "--target-type",
                    "command",
                    command,
                ],
                env=env,
            )

        return subprocess.Popen(
            [
                sys.executable,
                "-m",
                "celery",
                "-A",
                "zistudy_api.celery_app:celery_app",
                "worker",
                f"--loglevel={settings.celery_loglevel}",
            ],
            env=env,
        )
    except FileNotFoundError as exc:  # pragma: no cover - defensive guard
        raise RuntimeError(
            "Unable to locate Celery executable. Ensure Celery is installed or run the worker "
            "via `ZISTUDY_PROCESS_TYPE=worker uv run main.py`."
        ) from exc


def _bootstrap_settings():
    try:
        return get_settings()
    except ValidationError as exc:
        missing = sorted(
            {
                ".".join(str(part) for part in error["loc"])
                for error in exc.errors()
                if error.get("type") == "missing"
            }
        )
        if missing:
            print(
                "[main] Missing required configuration. "
                "Set environment variables (prefix ZISTUDY_) for: "
                f"{', '.join(missing)}",
                file=sys.stderr,
            )
        raise


def main() -> None:
    try:
        settings = _bootstrap_settings()
    except ValidationError:
        sys.exit(1)

    process_type = settings.process_type

    if process_type == "worker":
        _run_worker(settings)
        return

    _run_migrations()

    if process_type == "api-with-worker":
        worker_process: subprocess.Popen[bytes] | None = None
        try:
            worker_process = _start_worker_subprocess(settings)
        except RuntimeError as exc:
            print(f"[main] {exc}", file=sys.stderr)
        try:
            _run_api(settings)
        finally:
            if worker_process and worker_process.poll() is None:
                worker_process.send_signal(signal.SIGTERM)
                try:
                    worker_process.wait(timeout=10)
                except subprocess.TimeoutExpired:
                    worker_process.kill()
        return

    _run_api(settings)


if __name__ == "__main__":
    main()


================================================================================
# FILE: scripts/ai_manual_check.py
================================================================================

from __future__ import annotations

import argparse
import asyncio
import json
import os
import time
import uuid
from pathlib import Path
from typing import Any

import fitz  # type: ignore[import-untyped]
import httpx

from zistudy_api.config.settings import get_settings


def _create_pdf_bytes(text: str) -> bytes:
    document = fitz.open()
    try:
        page = document.new_page(width=595, height=842)  # A4
        page.insert_textbox(
            fitz.Rect(40, 40, 555, 800),
            text,
            fontsize=11,
            fontname="helv",
        )
        return bytes(document.tobytes())
    finally:
        document.close()


async def _register_and_login(
    client: httpx.AsyncClient,
    email: str,
    password: str,
) -> str:
    register_payload = {
        "email": email,
        "password": password,
        "full_name": "AI Manual Tester",
    }
    response = await client.post("/api/v1/auth/register", json=register_payload)
    if response.status_code not in (201, 409):
        raise RuntimeError(f"Failed to register: {response.status_code} {response.text}")

    login_payload = {"email": email, "password": password}
    login_response = await client.post("/api/v1/auth/login", json=login_payload)
    login_response.raise_for_status()
    token = login_response.json()["access_token"]
    return token


async def _trigger_generation(
    client: httpx.AsyncClient,
    token: str,
    payload: dict[str, Any],
    pdf_bytes: bytes | None,
) -> tuple[int, dict[str, Any]]:
    files: list[tuple[str, tuple[str | None, bytes | str, str | None]]] = [
        ("payload", (None, json.dumps(payload), "application/json")),
    ]
    if pdf_bytes:
        files.append(("pdfs", ("context.pdf", pdf_bytes, "application/pdf")))

    response = await client.post(
        "/api/v1/ai/study-cards/generate",
        headers={"Authorization": f"Bearer {token}"},
        files=files,
    )
    response.raise_for_status()
    job_summary = response.json()
    return job_summary["id"], job_summary


async def _poll_job(
    client: httpx.AsyncClient,
    token: str,
    job_id: int,
    timeout: float,
    interval: float,
) -> dict[str, Any]:
    deadline = time.monotonic() + timeout
    headers = {"Authorization": f"Bearer {token}"}
    last_status: str | None = None
    while True:
        response = await client.get(f"/api/v1/jobs/{job_id}", headers=headers)
        response.raise_for_status()
        payload = response.json()
        status = payload.get("status")
        if status != last_status:
            print(f"[ai-smoke] Job {job_id} status -> {status}")
            last_status = status
        if status in {"completed", "failed"}:
            return payload
        if time.monotonic() >= deadline:
            raise TimeoutError(
                f"Job {job_id} did not complete within {timeout} seconds. "
                f"Last payload: {json.dumps(payload, indent=2)}"
            )
        await asyncio.sleep(interval)


async def _fetch_generated_cards(
    client: httpx.AsyncClient,
    token: str,
    limit: int,
) -> list[dict[str, Any]]:
    headers = {"Authorization": f"Bearer {token}"}
    response = await client.get("/api/v1/study-cards", headers=headers, params={"page_size": limit})
    response.raise_for_status()
    body = response.json()
    return body.get("items", [])


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Manual smoke-test the AI study card generation workflow."
    )
    parser.add_argument(
        "--base-url",
        default=os.environ.get("ZISTUDY_API_BASE_URL", "http://localhost:8000"),
        help="FastAPI base URL (default: %(default)s)",
    )
    parser.add_argument(
        "--email",
        help="Email to reuse. Defaults to a generated value per run.",
    )
    parser.add_argument(
        "--password",
        default="Secret123!",
        help="Password to use when registering/logging in (default: %(default)s)",
    )
    parser.add_argument(
        "--topics",
        nargs="*",
        default=["Clinical reasoning"],
        help="Topics to include in the generation request.",
    )
    parser.add_argument(
        "--learning-objectives",
        nargs="*",
        default=["Differentiate shock states", "Prioritise stabilisation steps"],
        help="Learning objectives supplied to the AI.",
    )
    parser.add_argument(
        "--card-count",
        type=int,
        default=2,
        help="Target number of cards to request (default: %(default)s).",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.2,
        help="Temperature override for the generation request.",
    )
    parser.add_argument(
        "--context-text",
        default=(
            "Patient presents with septic shock. Discuss initial resuscitation priorities, "
            "antibiotic timing, and markers for escalation."
        ),
        help="Optional text that will be embedded into a PDF and uploaded as context.",
    )
    parser.add_argument(
        "--pdf-path",
        type=Path,
        help="Use an existing PDF as context instead of generating one from --context-text.",
    )
    parser.add_argument(
        "--job-timeout",
        type=float,
        default=120.0,
        help="Maximum seconds to wait for the job to complete (default: %(default)s).",
    )
    parser.add_argument(
        "--job-interval",
        type=float,
        default=5.0,
        help="Polling interval in seconds while waiting for the job (default: %(default)s).",
    )
    parser.add_argument(
        "--summary-limit",
        type=int,
        default=5,
        help="Number of newly created cards to display at the end (default: %(default)s).",
    )
    return parser


async def main_async(args: argparse.Namespace) -> None:
    settings = get_settings()
    if not settings.gemini_api_key:
        raise SystemExit(
            "ZISTUDY_GEMINI_API_KEY is not set. Add it to your environment (see .env)."
        )

    base_url = args.base_url.rstrip("/")
    email = args.email or f"ai-smoke-{uuid.uuid4().hex[:8]}@example.com"

    async with httpx.AsyncClient(base_url=base_url, timeout=httpx.Timeout(60.0)) as client:
        token = await _register_and_login(client, email=email, password=args.password)
        print(f"[ai-smoke] Authenticated as {email}")

        if args.pdf_path:
            pdf_bytes = args.pdf_path.read_bytes()
            print(f"[ai-smoke] Using supplied PDF: {args.pdf_path}")
        else:
            pdf_bytes = _create_pdf_bytes(args.context_text)
            print("[ai-smoke] Generated inline PDF context from --context-text.")

        payload = {
            "topics": args.topics,
            "learning_objectives": args.learning_objectives,
            "target_card_count": args.card_count,
            "temperature": args.temperature,
            "include_retention_aid": True,
        }

        job_id, job_summary = await _trigger_generation(client, token, payload, pdf_bytes)
        print(f"[ai-smoke] Job {job_id} enqueued (status={job_summary['status']}).")

        job_result = await _poll_job(
            client,
            token,
            job_id=job_id,
            timeout=args.job_timeout,
            interval=args.job_interval,
        )
        status = job_result["status"]
        print(f"[ai-smoke] Job {job_id} finished with status: {status}")
        if status == "failed":
            raise SystemExit(f"Job failed: {job_result.get('error')}")

        result_body = job_result["result"]
        summary = result_body["summary"]
        print(
            "[ai-smoke] Generated summary:",
            json.dumps(summary, indent=2),
        )
        if result_body.get("retention_aid"):
            print("[ai-smoke] Retention aid markdown preview:")
            print(result_body["retention_aid"]["markdown"][:600], "...\n")

        cards = await _fetch_generated_cards(client, token, args.summary_limit)
        print(f"[ai-smoke] Retrieved {len(cards)} persisted card(s). Sample:")
        for card in cards[: args.summary_limit]:
            payload = card["data"].get("payload", {})
            question = payload.get("question") or payload.get("prompt")
            print(f"  - [{card['card_type']}] Q: {question!r}")

        print("[ai-smoke] Done.")


def main(argv: list[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(argv)
    try:
        asyncio.run(main_async(args))
    except KeyboardInterrupt:
        raise SystemExit("Aborted by user.") from None


if __name__ == "__main__":
    main()


================================================================================
# FILE: src/zistudy_api/__init__.py
================================================================================



================================================================================
# FILE: src/zistudy_api/api/__init__.py
================================================================================

from __future__ import annotations

from fastapi import APIRouter, FastAPI

from zistudy_api.api.routes import ai, answers, auth, jobs, study_cards, study_sets, tags


def include_api_routes(app: FastAPI) -> None:
    """Register all API routers with the FastAPI application."""

    api_router = APIRouter(prefix="/api/v1")
    api_router.include_router(ai.router)
    api_router.include_router(answers.router)
    api_router.include_router(auth.router)
    api_router.include_router(jobs.router)
    api_router.include_router(study_sets.router)
    api_router.include_router(study_cards.router)
    api_router.include_router(tags.router)
    app.include_router(api_router)


__all__ = ["include_api_routes"]


================================================================================
# FILE: src/zistudy_api/api/dependencies.py
================================================================================

from __future__ import annotations

from typing import Annotated, AsyncIterator

from fastapi import Depends, HTTPException, status
from fastapi.security import APIKeyHeader, HTTPAuthorizationCredentials, HTTPBearer
from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.config.settings import Settings, get_settings
from zistudy_api.db.repositories.api_keys import ApiKeyRepository
from zistudy_api.db.repositories.refresh_tokens import RefreshTokenRepository
from zistudy_api.db.repositories.users import UserRepository
from zistudy_api.db.session import get_session
from zistudy_api.domain.schemas.auth import SessionUser
from zistudy_api.services.ai import (
    AgentConfiguration,
    AiStudyCardService,
    DocumentIngestionService,
    GeminiGenerativeClient,
    IngestedPDFContextStrategy,
    NativePDFContextStrategy,
    PDFContextStrategy,
    StudyCardGenerationAgent,
)
from zistudy_api.services.auth import AuthService
from zistudy_api.services.jobs import JobService
from zistudy_api.services.study_cards import StudyCardService
from zistudy_api.services.study_sets import StudySetService
from zistudy_api.services.tags import TagService

AsyncSessionDependency = Annotated[AsyncSession, Depends(get_session)]

bearer_scheme = HTTPBearer(
    auto_error=False,
    scheme_name="JWTBearer",
    description="Paste a JWT access token obtained from `/api/v1/auth/login`.",
)
api_key_header = APIKeyHeader(name="x-api-key", auto_error=False)

TokenDependency = Annotated[HTTPAuthorizationCredentials | None, Depends(bearer_scheme)]
APIKeyDependency = Annotated[str | None, Depends(api_key_header)]


def get_study_set_service(session: AsyncSessionDependency) -> StudySetService:
    return StudySetService(session)


def get_study_card_service(session: AsyncSessionDependency) -> StudyCardService:
    return StudyCardService(session)


def get_tag_service(session: AsyncSessionDependency) -> TagService:
    return TagService(session)


def get_job_service(session: AsyncSessionDependency) -> JobService:
    return JobService(session)


def get_auth_service(session: AsyncSessionDependency) -> AuthService:
    user_repo = UserRepository(session)
    refresh_repo = RefreshTokenRepository(session)
    api_key_repo = ApiKeyRepository(session)
    return AuthService(
        session=session,
        user_repository=user_repo,
        refresh_tokens=refresh_repo,
        api_keys=api_key_repo,
    )


AuthServiceDependency = Annotated[AuthService, Depends(get_auth_service)]
JobServiceDependency = Annotated[JobService, Depends(get_job_service)]


async def get_ai_study_card_service(
    session: AsyncSessionDependency,
    settings: Annotated[Settings, Depends(get_settings)],
) -> AsyncIterator[AiStudyCardService]:
    if not settings.gemini_api_key:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Gemini API is not configured.",
        )

    ingestion_service = DocumentIngestionService()
    client = GeminiGenerativeClient(
        api_key=settings.gemini_api_key,
        model=settings.gemini_model,
        endpoint=settings.gemini_endpoint,
        timeout=settings.gemini_request_timeout_seconds,
    )
    agent_config = AgentConfiguration(
        default_model=settings.gemini_model,
        default_temperature=settings.ai_generation_default_temperature,
        default_card_count=settings.ai_generation_default_card_count,
        max_card_count=settings.ai_generation_max_card_count,
        max_attempts=settings.ai_generation_max_attempts,
    )
    agent = StudyCardGenerationAgent(client=client, config=agent_config)
    pdf_strategy: PDFContextStrategy
    if settings.gemini_pdf_mode == "native":
        pdf_strategy = NativePDFContextStrategy(ingestor=ingestion_service)
    else:
        pdf_strategy = IngestedPDFContextStrategy(ingestor=ingestion_service)
    service = AiStudyCardService(
        session=session,
        agent=agent,
        pdf_strategy=pdf_strategy,
    )
    try:
        yield service
    finally:
        await client.aclose()


AiStudyCardServiceDependency = Annotated[AiStudyCardService, Depends(get_ai_study_card_service)]


async def get_current_session_user(
    token: TokenDependency,
    api_key: APIKeyDependency,
    auth_service: AuthServiceDependency,
) -> SessionUser:
    if token and token.credentials:
        return await auth_service.parse_access_token(token.credentials)
    if api_key:
        return await auth_service.authenticate_api_key(api_key)
    raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Authentication required")


async def get_optional_session_user(
    token: TokenDependency,
    api_key: APIKeyDependency,
    auth_service: AuthServiceDependency,
) -> SessionUser | None:
    if token and token.credentials:
        return await auth_service.parse_access_token(token.credentials)
    if api_key:
        return await auth_service.authenticate_api_key(api_key)
    return None


__all__ = [
    "AiStudyCardServiceDependency",
    "APIKeyDependency",
    "AsyncSessionDependency",
    "AuthServiceDependency",
    "JobServiceDependency",
    "TokenDependency",
    "get_ai_study_card_service",
    "get_auth_service",
    "get_current_session_user",
    "get_optional_session_user",
    "get_job_service",
    "get_study_card_service",
    "get_study_set_service",
    "get_tag_service",
]


================================================================================
# FILE: src/zistudy_api/api/routes/__init__.py
================================================================================

from . import answers, auth, jobs, study_cards, study_sets, tags

__all__ = ["answers", "auth", "jobs", "study_cards", "study_sets", "tags"]


================================================================================
# FILE: src/zistudy_api/api/routes/ai.py
================================================================================

from __future__ import annotations

import base64
from typing import Annotated, cast

from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile, status
from pydantic import ValidationError

from zistudy_api.api.dependencies import JobServiceDependency, get_current_session_user
from zistudy_api.domain.schemas.ai import StudyCardGenerationRequest
from zistudy_api.domain.schemas.auth import SessionUser
from zistudy_api.domain.schemas.jobs import JobSummary
from zistudy_api.services.job_processors import process_ai_generation_job
from zistudy_api.services.jobs import ProcessorTask

router = APIRouter(prefix="/ai", tags=["AI"])

PDF_CONTENT_TYPES = {
    "application/pdf",
    "application/x-pdf",
    "application/acrobat",
    "applications/vnd.pdf",
    "text/pdf",
    "text/x-pdf",
}


@router.post(
    "/study-cards/generate",
    response_model=JobSummary,
    status_code=status.HTTP_202_ACCEPTED,
)
async def generate_study_cards(
    payload: Annotated[str, Form(description="JSON encoded StudyCardGenerationRequest")],
    pdfs: Annotated[list[UploadFile], File(default_factory=list)],
    job_service: JobServiceDependency,
    _: Annotated[SessionUser, Depends(get_current_session_user)],
) -> JobSummary:
    """Queue an asynchronous job that generates study cards from the supplied PDFs."""
    try:
        request_model = StudyCardGenerationRequest.model_validate_json(payload)
    except ValidationError as exc:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid generation payload.",
        ) from exc

    encoded_documents: list[dict[str, str | None]] = []
    for upload in pdfs:
        try:
            if upload.content_type and upload.content_type not in PDF_CONTENT_TYPES:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Unsupported content type: {upload.content_type}",
                )

            data = await upload.read()
            if data:
                encoded_documents.append(
                    {
                        "filename": upload.filename,
                        "content": base64.b64encode(data).decode("ascii"),
                    }
                )
        finally:
            await upload.close()

    job_payload = {
        "request": request_model.model_dump(mode="json"),
        "documents": encoded_documents,
    }

    summary = await job_service.enqueue(
        job_type="ai_generate_study_cards",
        owner_id=_.id,
        payload=job_payload,
        processor_task=cast(ProcessorTask, process_ai_generation_job),
    )
    return summary


__all__ = ["generate_study_cards", "router"]


================================================================================
# FILE: src/zistudy_api/api/routes/answers.py
================================================================================

from __future__ import annotations

from typing import Annotated

from fastapi import APIRouter, Depends, HTTPException, Query, status

from zistudy_api.api.dependencies import AsyncSessionDependency, get_current_session_user
from zistudy_api.domain.schemas.answers import (
    AnswerCreate,
    AnswerHistory,
    AnswerRead,
    AnswerStats,
    StudySetProgress,
)
from zistudy_api.domain.schemas.auth import SessionUser
from zistudy_api.services.answers import AnswerService

router = APIRouter(prefix="/answers", tags=["Answers"])


def get_answer_service(session: AsyncSessionDependency) -> AnswerService:
    return AnswerService(session)


AnswerServiceDependency = Annotated[AnswerService, Depends(get_answer_service)]
CurrentUserDependency = Annotated[SessionUser, Depends(get_current_session_user)]


@router.get("/history", response_model=AnswerHistory)
async def answer_history(
    current_user: CurrentUserDependency,
    service: AnswerServiceDependency,
    page: int = 1,
    page_size: int = 20,
) -> AnswerHistory:
    """Return a paginated timeline of answers submitted by the current user."""
    return await service.list_history(user_id=current_user.id, page=page, page_size=page_size)


@router.get("/cards/{study_card_id}/stats", response_model=AnswerStats)
async def card_stats(
    study_card_id: int,
    current_user: CurrentUserDependency,
    service: AnswerServiceDependency,
) -> AnswerStats:
    """Summarise accuracy metrics for a single study card."""
    return await service.stats_for_card(study_card_id=study_card_id, user_id=current_user.id)


@router.get("/study-sets/progress", response_model=list[StudySetProgress])
async def study_set_progress(
    study_set_ids: Annotated[list[int], Query()],
    current_user: CurrentUserDependency,
    service: AnswerServiceDependency,
) -> list[StudySetProgress]:
    """Report aggregated progress metrics across the requested study sets."""
    return await service.study_set_progress(user_id=current_user.id, study_set_ids=study_set_ids)


@router.post("", response_model=AnswerRead, status_code=status.HTTP_201_CREATED)
async def submit_answer(
    payload: AnswerCreate,
    current_user: CurrentUserDependency,
    service: AnswerServiceDependency,
) -> AnswerRead:
    """Persist a learner's answer and return the typed read model."""
    try:
        return await service.submit_answer(user_id=current_user.id, payload=payload)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


@router.get("/{answer_id}", response_model=AnswerRead)
async def get_answer(
    answer_id: int,
    current_user: CurrentUserDependency,
    service: AnswerServiceDependency,
) -> AnswerRead:
    """Retrieve a single answer owned by the current user."""
    try:
        return await service.get_answer(answer_id, user_id=current_user.id)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


__all__ = ["router"]


================================================================================
# FILE: src/zistudy_api/api/routes/auth.py
================================================================================

from __future__ import annotations

from typing import Annotated

from fastapi import APIRouter, Depends, status

from zistudy_api.api.dependencies import (
    get_auth_service,
    get_current_session_user,
)
from zistudy_api.domain.schemas.auth import (
    APIKeyCreate,
    APIKeyRead,
    RefreshRequest,
    SessionUser,
    TokenPair,
    UserCreate,
    UserLogin,
    UserRead,
)
from zistudy_api.services.auth import AuthService

router = APIRouter(prefix="/auth", tags=["Auth"])


AuthServiceDependency = Annotated[AuthService, Depends(get_auth_service)]
CurrentUserDependency = Annotated[SessionUser, Depends(get_current_session_user)]


@router.post("/register", response_model=UserRead, status_code=status.HTTP_201_CREATED)
async def register_user(
    payload: UserCreate,
    auth_service: AuthServiceDependency,
) -> UserRead:
    """Create a new user account and return the persisted profile."""
    return await auth_service.register_user(payload)


@router.post("/login", response_model=TokenPair)
async def login_user(
    payload: UserLogin,
    auth_service: AuthServiceDependency,
) -> TokenPair:
    """Exchange credentials for a short-lived access token pair."""
    return await auth_service.authenticate(payload)


@router.post("/refresh", response_model=TokenPair)
async def refresh_token(
    payload: RefreshRequest,
    auth_service: AuthServiceDependency,
) -> TokenPair:
    """Rotate refresh credentials and issue a fresh access/refresh token pair."""
    return await auth_service.refresh(payload)


@router.post("/logout", status_code=status.HTTP_204_NO_CONTENT)
async def logout_user(
    user: CurrentUserDependency,
    auth_service: AuthServiceDependency,
) -> None:
    """Revoke all refresh tokens associated with the current user."""
    await auth_service.revoke_refresh_tokens(user.id)


@router.get("/me", response_model=SessionUser)
async def get_me(user: CurrentUserDependency) -> SessionUser:
    """Return the authenticated session user."""
    return user


@router.get("/api-keys", response_model=list[APIKeyRead])
async def list_api_keys(
    user: CurrentUserDependency,
    auth_service: AuthServiceDependency,
) -> list[APIKeyRead]:
    """List API keys that belong to the current user."""
    return await auth_service.list_api_keys(user.id)


@router.post("/api-keys", response_model=APIKeyRead, status_code=status.HTTP_201_CREATED)
async def create_api_key(
    payload: APIKeyCreate,
    user: CurrentUserDependency,
    auth_service: AuthServiceDependency,
) -> APIKeyRead:
    """Create a new API key for the current user."""
    return await auth_service.create_api_key(user.id, payload)


@router.delete("/api-keys/{api_key_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_api_key(
    api_key_id: int,
    user: CurrentUserDependency,
    auth_service: AuthServiceDependency,
) -> None:
    """Delete an API key owned by the current user."""
    await auth_service.delete_api_key(user.id, api_key_id)


__all__ = ["router"]


================================================================================
# FILE: src/zistudy_api/api/routes/jobs.py
================================================================================

from __future__ import annotations

from typing import Annotated

from fastapi import APIRouter, Depends, HTTPException, status

from zistudy_api.api.dependencies import AsyncSessionDependency, get_current_session_user
from zistudy_api.domain.schemas.auth import SessionUser
from zistudy_api.domain.schemas.jobs import JobSummary
from zistudy_api.services.jobs import JobService

router = APIRouter(prefix="/jobs", tags=["Jobs"])

CurrentUserDependency = Annotated[SessionUser, Depends(get_current_session_user)]


@router.get("/{job_id}", response_model=JobSummary)
async def get_job(
    job_id: int,
    current_user: CurrentUserDependency,
    session: AsyncSessionDependency,
) -> JobSummary:
    """Fetch a job owned by the current user, raising 404 when missing."""
    service = JobService(session)
    try:
        return await service.get_job(job_id, owner_id=current_user.id)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


__all__ = ["router"]


================================================================================
# FILE: src/zistudy_api/api/routes/study_cards.py
================================================================================

from __future__ import annotations

from typing import Annotated

from fastapi import APIRouter, Depends, HTTPException, Request, status

from zistudy_api.api.dependencies import get_current_session_user, get_study_card_service
from zistudy_api.domain.enums import CardType
from zistudy_api.domain.schemas.auth import SessionUser
from zistudy_api.domain.schemas.study_cards import (
    CardSearchRequest,
    PaginatedStudyCardResults,
    StudyCardCollection,
    StudyCardCreate,
    StudyCardImportPayload,
    StudyCardRead,
    StudyCardUpdate,
)
from zistudy_api.services.study_cards import StudyCardService

router = APIRouter(prefix="/study-cards", tags=["Study Cards"])


StudyCardServiceDependency = Annotated[StudyCardService, Depends(get_study_card_service)]
CurrentUserDependency = Annotated[SessionUser, Depends(get_current_session_user)]


@router.post("", response_model=StudyCardRead, status_code=status.HTTP_201_CREATED)
async def create_study_card(
    payload: StudyCardCreate,
    service: StudyCardServiceDependency,
    _: CurrentUserDependency,
) -> StudyCardRead:
    """Create a study card and return the stored representation."""
    return await service.create_card(payload)


@router.get("/{card_id}", response_model=StudyCardRead)
async def get_study_card(
    card_id: int,
    service: StudyCardServiceDependency,
) -> StudyCardRead:
    """Fetch a study card by identifier."""
    try:
        return await service.get_card(card_id)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


@router.put("/{card_id}", response_model=StudyCardRead)
async def update_study_card(
    card_id: int,
    payload: StudyCardUpdate,
    service: StudyCardServiceDependency,
    _: CurrentUserDependency,
) -> StudyCardRead:
    """Apply updates to a study card and return the refreshed payload."""
    try:
        return await service.update_card(card_id, payload)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


@router.delete("/{card_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_study_card(
    card_id: int,
    service: StudyCardServiceDependency,
    _: CurrentUserDependency,
) -> None:
    """Remove a study card from the catalog."""
    try:
        await service.delete_card(card_id)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


@router.get("", response_model=StudyCardCollection)
async def list_study_cards(
    service: StudyCardServiceDependency,
    card_type: CardType | None = None,
    page: int = 1,
    page_size: int = 20,
) -> StudyCardCollection:
    """List study cards with optional filtering by type."""
    return await service.list_cards(card_type=card_type, page=page, page_size=page_size)


@router.post("/search", response_model=PaginatedStudyCardResults)
async def search_study_cards(
    payload: CardSearchRequest,
    service: StudyCardServiceDependency,
) -> PaginatedStudyCardResults:
    """Search study cards using the provided query filters."""
    return await service.search_cards(payload)


@router.get("/not-in-set/{study_set_id}", response_model=StudyCardCollection)
async def cards_not_in_set(
    study_set_id: int,
    service: StudyCardServiceDependency,
    card_type: CardType | None = None,
    page: int = 1,
    page_size: int = 20,
) -> StudyCardCollection:
    """Return cards not yet associated with the specified study set."""
    return await service.list_cards_not_in_set(
        study_set_id=study_set_id,
        card_type=card_type,
        page=page,
        page_size=page_size,
    )


@router.post("/import", response_model=list[StudyCardRead], status_code=status.HTTP_201_CREATED)
async def import_study_cards(
    payload: StudyCardImportPayload,
    service: StudyCardServiceDependency,
    _: CurrentUserDependency,
) -> list[StudyCardRead]:
    """Bulk import typed study cards."""
    return await service.import_card_batch(payload)


@router.post(
    "/import/json", response_model=list[StudyCardRead], status_code=status.HTTP_201_CREATED
)
async def import_study_cards_json(
    request: Request,
    service: StudyCardServiceDependency,
    _: CurrentUserDependency,
) -> list[StudyCardRead]:
    """Bulk import cards from a raw JSON payload."""
    raw_body = await request.body()
    try:
        return await service.import_cards_from_json(raw_body.decode("utf-8"))
    except ValueError as exc:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(exc)) from exc


================================================================================
# FILE: src/zistudy_api/api/routes/study_sets.py
================================================================================

from __future__ import annotations

from typing import Annotated

from fastapi import APIRouter, Depends, HTTPException, status

from zistudy_api.api.dependencies import (
    JobServiceDependency,
    get_current_session_user,
    get_optional_session_user,
    get_study_set_service,
)
from zistudy_api.domain.enums import CardType
from zistudy_api.domain.schemas.auth import SessionUser
from zistudy_api.domain.schemas.jobs import JobSummary
from zistudy_api.domain.schemas.study_sets import (
    AddCardsToSet,
    BulkAddToSets,
    BulkDeleteStudySets,
    BulkOperationResult,
    CloneStudySetsRequest,
    ExportStudySetsRequest,
    PaginatedStudySets,
    RemoveCardsFromSet,
    StudySetCardsPage,
    StudySetCreate,
    StudySetForCard,
    StudySetUpdate,
    StudySetWithMeta,
)
from zistudy_api.services.job_processors import process_clone_job, process_export_job
from zistudy_api.services.study_sets import StudySetService

router = APIRouter(prefix="/study-sets", tags=["Study Sets"])


StudySetServiceDependency = Annotated[StudySetService, Depends(get_study_set_service)]
CurrentUserDependency = Annotated[SessionUser, Depends(get_current_session_user)]
OptionalUserDependency = Annotated[SessionUser | None, Depends(get_optional_session_user)]


@router.post(
    "",
    response_model=StudySetWithMeta,
    status_code=status.HTTP_201_CREATED,
)
async def create_study_set(
    payload: StudySetCreate,
    service: StudySetServiceDependency,
    user: CurrentUserDependency,
) -> StudySetWithMeta:
    """Create a study set owned by the authenticated user."""
    return await service.create_study_set(payload, user.id)


@router.get(
    "/{study_set_id}",
    response_model=StudySetWithMeta,
)
async def get_study_set(
    study_set_id: int,
    service: StudySetServiceDependency,
) -> StudySetWithMeta:
    """Retrieve a study set including metadata such as tags and ownership."""
    try:
        return await service.get_study_set(study_set_id)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


@router.put(
    "/{study_set_id}",
    response_model=StudySetWithMeta,
)
async def update_study_set(
    study_set_id: int,
    payload: StudySetUpdate,
    service: StudySetServiceDependency,
    user: CurrentUserDependency,
) -> StudySetWithMeta:
    """Update study set metadata after confirming the caller has modify rights."""
    try:
        if not await service.can_modify(study_set_id, user.id):
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Forbidden")
        return await service.update_study_set(study_set_id, payload)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


@router.delete("/{study_set_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_study_set(
    study_set_id: int,
    service: StudySetServiceDependency,
    user: CurrentUserDependency,
) -> None:
    """Delete a study set owned or managed by the current user."""
    try:
        if not await service.can_modify(study_set_id, user.id):
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Forbidden")
        await service.delete_study_set(study_set_id)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


@router.get("", response_model=PaginatedStudySets)
async def list_study_sets(
    service: StudySetServiceDependency,
    session_user: OptionalUserDependency,
    show_only_owned: bool = False,
    search: str | None = None,
    page: int = 1,
    page_size: int = 20,
) -> PaginatedStudySets:
    """List study sets visible to the caller with optional filters."""
    user_id = session_user.id if session_user else None
    total, items = await service.list_accessible_study_sets(
        user_id=user_id,
        show_only_owned=show_only_owned,
        search_query=search,
        page=page,
        page_size=page_size,
    )
    return PaginatedStudySets(items=items, total=total, page=page, page_size=page_size)


@router.post(
    "/add-cards",
    status_code=status.HTTP_204_NO_CONTENT,
)
async def add_cards_to_set(
    payload: AddCardsToSet,
    service: StudySetServiceDependency,
    user: CurrentUserDependency,
) -> None:
    """Append cards to a study set after verifying modify permissions."""
    try:
        if not await service.can_modify(payload.study_set_id, user.id):
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Forbidden")
        await service.add_cards(payload)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc
    except ValueError as exc:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(exc)) from exc


@router.post(
    "/remove-cards",
    status_code=status.HTTP_204_NO_CONTENT,
)
async def remove_cards_from_set(
    payload: RemoveCardsFromSet,
    service: StudySetServiceDependency,
    user: CurrentUserDependency,
) -> None:
    """Remove cards from a study set when the caller has modify rights."""
    try:
        if not await service.can_modify(payload.study_set_id, user.id):
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Forbidden")
        await service.remove_cards(
            study_set_id=payload.study_set_id,
            card_ids=payload.card_ids,
            card_type=payload.card_type,
        )
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc


@router.get("/{study_set_id}/can-access", response_model=dict[str, bool])
async def can_access_study_set(
    study_set_id: int,
    service: StudySetServiceDependency,
    session_user: OptionalUserDependency,
) -> dict[str, bool]:
    """Return booleans indicating whether the caller can access or modify the set."""
    user_id = session_user.id if session_user else None
    try:
        entity = await service.get_study_set(study_set_id)
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc

    can_access = entity.study_set.can_access(user_id)
    can_modify = await service.can_modify(study_set_id, user_id)
    return {"can_access": can_access, "can_modify": can_modify}


@router.get(
    "/{study_set_id}/cards",
    response_model=StudySetCardsPage,
)
async def list_cards_in_study_set(
    study_set_id: int,
    service: StudySetServiceDependency,
    card_type: CardType | None = None,
    page: int = 1,
    page_size: int = 20,
) -> StudySetCardsPage:
    """List cards that belong to a study set with optional filtering."""
    try:
        page_data = await service.list_cards_in_set(
            study_set_id=study_set_id,
            card_type=card_type,
            page=page,
            page_size=page_size,
        )
    except KeyError as exc:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(exc)) from exc
    return page_data


@router.post("/clone", response_model=JobSummary, status_code=status.HTTP_202_ACCEPTED)
async def clone_study_sets_endpoint(
    payload: CloneStudySetsRequest,
    service: StudySetServiceDependency,
    job_service: JobServiceDependency,
    user: CurrentUserDependency,
) -> JobSummary:
    """Enqueue an asynchronous clone job for the selected study sets."""
    for study_set_id in payload.study_set_ids:
        meta = await service.get_study_set(study_set_id)
        if not meta.study_set.can_access(user.id):
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Forbidden")

    summary = await job_service.enqueue(
        job_type="study_set_clone",
        owner_id=user.id,
        payload={
            "study_set_ids": payload.study_set_ids,
            "title_prefix": payload.title_prefix,
            "owner_id": user.id,
        },
        processor_task=process_clone_job,
    )
    return summary


@router.post("/export", response_model=JobSummary, status_code=status.HTTP_202_ACCEPTED)
async def export_study_sets_endpoint(
    payload: ExportStudySetsRequest,
    service: StudySetServiceDependency,
    job_service: JobServiceDependency,
    user: CurrentUserDependency,
) -> JobSummary:
    """Enqueue an asynchronous export job for the selected study sets."""
    for study_set_id in payload.study_set_ids:
        meta = await service.get_study_set(study_set_id)
        if not meta.study_set.can_access(user.id):
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Forbidden")

    summary = await job_service.enqueue(
        job_type="study_set_export",
        owner_id=user.id,
        payload={
            "study_set_ids": payload.study_set_ids,
            "owner_id": user.id,
        },
        processor_task=process_export_job,
    )
    return summary


@router.post(
    "/bulk-add",
    response_model=BulkOperationResult,
)
async def bulk_add_cards_to_study_sets(
    payload: BulkAddToSets,
    service: StudySetServiceDependency,
    user: CurrentUserDependency,
) -> BulkOperationResult:
    """Add cards to multiple study sets in one request, reporting failures."""
    # Ensure user can modify each set before attempting bulk operation
    forbidden: list[int] = []
    permitted_ids: list[int] = []
    for set_id in payload.study_set_ids:
        try:
            if not await service.can_modify(set_id, user.id):
                forbidden.append(set_id)
            else:
                permitted_ids.append(set_id)
        except KeyError:
            forbidden.append(set_id)

    if not permitted_ids:
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Forbidden")

    filtered_payload = BulkAddToSets(
        study_set_ids=permitted_ids,
        card_ids=payload.card_ids,
        card_type=payload.card_type,
    )
    result = await service.bulk_add_cards(filtered_payload)
    if forbidden:
        result.errors.extend([f"Set {set_id}: Forbidden" for set_id in forbidden])
        result.error_count += len(forbidden)
    return result


@router.post(
    "/bulk-delete",
    response_model=BulkOperationResult,
)
async def bulk_delete_study_sets(
    payload: BulkDeleteStudySets,
    service: StudySetServiceDependency,
    user: CurrentUserDependency,
) -> BulkOperationResult:
    """Delete multiple study sets owned by the caller, aggregating errors."""
    result = await service.bulk_delete_study_sets(
        study_set_ids=payload.study_set_ids,
        user_id=user.id,
    )
    if result.success_count == 0 and result.error_count > 0:
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=result.errors)
    return result


@router.get(
    "/for-card/{card_id}",
    response_model=list[StudySetForCard],
)
async def study_sets_for_card(
    card_id: int,
    service: StudySetServiceDependency,
    session_user: OptionalUserDependency,
) -> list[StudySetForCard]:
    """List study sets containing a specific card, filtered by caller access."""
    user_id = session_user.id if session_user else None
    sets = await service.get_study_sets_for_card(card_id=card_id, user_id=user_id)
    return sets


================================================================================
# FILE: src/zistudy_api/api/routes/tags.py
================================================================================

from __future__ import annotations

from typing import Annotated

from fastapi import APIRouter, Depends, Query, status

from zistudy_api.api.dependencies import get_tag_service
from zistudy_api.domain.schemas.tags import TagCreate, TagRead, TagSearchResponse, TagUsage
from zistudy_api.services.tags import TagService

router = APIRouter(prefix="/tags", tags=["Tags"])


TagServiceDependency = Annotated[TagService, Depends(get_tag_service)]
NamesQuery = Annotated[list[str] | None, Query()]
SearchQuery = Annotated[str, Query(min_length=1, max_length=64)]
LimitQuery = Annotated[int, Query(ge=1, le=100)]
PopularLimitQuery = Annotated[int, Query(ge=1, le=50)]


@router.get("", response_model=list[TagRead])
async def list_tags(
    service: TagServiceDependency,
    names: NamesQuery = None,
) -> list[TagRead]:
    """Return tags, optionally filtering to a provided name list."""
    return await service.list_tags(names)


@router.post("", response_model=list[TagRead], status_code=status.HTTP_201_CREATED)
async def create_tags(
    payload: list[TagCreate],
    service: TagServiceDependency,
) -> list[TagRead]:
    """Ensure the supplied tag names exist and return their canonical form."""
    tag_names = [tag.name for tag in payload]
    return await service.ensure_tags(tag_names, commit=True)


@router.get("/search", response_model=TagSearchResponse)
async def search_tags(
    service: TagServiceDependency,
    query: SearchQuery,
    limit: LimitQuery = 20,
) -> TagSearchResponse:
    """Search tags by prefix or substring and return a bounded result set."""
    total, tags = await service.search_tags(query, limit)
    return TagSearchResponse(items=tags, total=total)


@router.get("/popular", response_model=list[TagUsage])
async def popular_tags(
    service: TagServiceDependency,
    limit: PopularLimitQuery = 10,
) -> list[TagUsage]:
    """Return the most frequently used tags."""
    return await service.popular_tags(limit)


================================================================================
# FILE: src/zistudy_api/app.py
================================================================================

from __future__ import annotations

from contextlib import asynccontextmanager
from typing import Mapping, cast

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from starlette import status as http_status

from zistudy_api.api import include_api_routes
from zistudy_api.config.settings import Settings, get_settings
from zistudy_api.core.logging import configure_logging
from zistudy_api.db.session import lifespan_context
from zistudy_api.domain.schemas.common import ErrorBody, ErrorEnvelope

HTTP_STATUS_MESSAGES: Mapping[int, str] = cast(
    Mapping[int, str],
    getattr(http_status, "HTTP_STATUS_CODES", {}),
)


@asynccontextmanager
async def _lifespan(_: FastAPI):
    async with lifespan_context():
        yield


def create_app(settings: Settings | None = None) -> FastAPI:
    """Application factory for the ZiStudy API."""

    settings = settings or get_settings()
    configure_logging(settings)

    app = FastAPI(
        title=settings.app_name,
        version=settings.api_version,
        lifespan=_lifespan,
    )

    if settings.cors_origins:
        from fastapi.middleware.cors import CORSMiddleware

        app.add_middleware(
            CORSMiddleware,
            allow_origins=settings.cors_origins,
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )

    include_api_routes(app)

    @app.exception_handler(HTTPException)
    async def http_exception_handler(request: Request, exc: HTTPException) -> JSONResponse:
        message = (
            exc.detail
            if isinstance(exc.detail, str)
            else HTTP_STATUS_MESSAGES.get(exc.status_code, "Error")
        )
        details = exc.detail if isinstance(exc.detail, dict) else None
        payload = ErrorEnvelope(
            error=ErrorBody(
                code=exc.status_code,
                message=message,
                details=details,
            )
        ).model_dump(mode="json")
        return JSONResponse(status_code=exc.status_code, content=payload)

    @app.exception_handler(Exception)
    async def unhandled_exception_handler(
        request: Request, exc: Exception
    ) -> JSONResponse:  # pragma: no cover
        payload = ErrorEnvelope(
            error=ErrorBody(
                code=http_status.HTTP_500_INTERNAL_SERVER_ERROR,
                message="Internal Server Error",
                details={"reason": str(exc)},
            )
        ).model_dump(mode="json")
        return JSONResponse(status_code=http_status.HTTP_500_INTERNAL_SERVER_ERROR, content=payload)

    return app


app = create_app()


__all__ = ["app", "create_app"]


================================================================================
# FILE: src/zistudy_api/celery_app.py
================================================================================

from __future__ import annotations

from celery import Celery

from zistudy_api.config.settings import get_settings


def _create_celery() -> Celery:
    settings = get_settings()
    app = Celery(
        "zistudy",
        broker=settings.celery_broker_url,
        backend=settings.celery_result_backend,
    )
    app.conf.update(
        task_always_eager=settings.celery_task_always_eager,
        task_eager_propagates=settings.celery_task_always_eager,
        timezone="UTC",
        enable_utc=True,
        include=["zistudy_api.services.job_processors"],
    )
    app.autodiscover_tasks(["zistudy_api.services"], force=True, related_name="tasks")
    app.autodiscover_tasks(["zistudy_api.services.job_processors"])
    return app


celery_app = _create_celery()

# Ensure task modules are imported so Celery registers them when the worker starts.

__all__ = ["celery_app"]


================================================================================
# FILE: src/zistudy_api/config/__init__.py
================================================================================



================================================================================
# FILE: src/zistudy_api/config/settings.py
================================================================================

from __future__ import annotations

from functools import lru_cache
from typing import Literal

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    """Application configuration sourced from environment variables or .env files."""

    model_config = SettingsConfigDict(
        env_prefix="ZISTUDY_",
        env_file=(".env", ".env.local"),
        env_file_encoding="utf-8",
        extra="ignore",
    )

    environment: Literal["local", "test", "production"] = "local"
    app_name: str = "ZiStudy API"
    api_version: str = "0.2.0"
    log_level: Literal["TRACE", "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"] = "INFO"
    log_json: bool = False
    cors_origins: list[str] = Field(default_factory=lambda: ["*"])
    api_host: str = "0.0.0.0"
    api_port: int = 8000
    process_type: Literal["api", "worker", "api-with-worker"] = "api"
    database_url: str = Field(..., description="SQLAlchemy-compatible database URL.")
    db_echo: bool = False
    db_pool_size: int = 10
    db_max_overflow: int = 20
    default_page_size: int = 20
    max_page_size: int = 100
    jwt_secret: str = Field(
        ..., min_length=16, description="Secret key for signing JWTs."
    )
    jwt_algorithm: str = "HS256"
    access_token_exp_minutes: int = 15
    refresh_token_exp_minutes: int = 60 * 24 * 14
    refresh_token_length: int = 64
    api_key_length: int = 48
    ai_provider: Literal["gemini"] = "gemini"
    gemini_api_key: str | None = Field(
        default=None,
        description="API key for the Google Gemini platform.",
    )
    gemini_endpoint: str = Field(
        default="https://generativelanguage.googleapis.com/v1beta",
        description="Base URL for the Gemini API.",
    )
    gemini_model: str = Field(
        default="gemini-2.5-pro",
        description="Default Gemini model identifier used for generation requests.",
    )
    gemini_request_timeout_seconds: float = Field(
        default=60.0,
        ge=1.0,
        description="Timeout for outbound requests to the Gemini API.",
    )
    ai_generation_default_temperature: float = Field(
        default=0.35,
        ge=0.0,
        le=2.0,
        description="Default creativity level for AI generated study cards.",
    )
    ai_generation_default_card_count: int = Field(
        default=8,
        ge=1,
        le=40,
        description="Fallback number of cards generated when a client omits the target count.",
    )
    ai_generation_max_card_count: int = Field(
        default=20,
        ge=1,
        le=60,
        description="Hard upper bound on the number of cards generated per request.",
    )
    ai_generation_max_attempts: int = Field(
        default=3,
        ge=1,
        le=5,
        description="Maximum number of attempts when the AI output fails schema validation.",
    )
    gemini_pdf_mode: Literal["native", "ingest"] = Field(
        default="native",
        description="How PDF context should be supplied to Gemini (native inline PDFs or pre-extracted text/images).",
    )
    celery_broker_url: str = Field(
        default="redis://localhost:6379/0",
        description="Celery broker URL.",
    )
    celery_result_backend: str = Field(
        default="redis://localhost:6379/1",
        description="Celery result backend URL.",
    )
    celery_task_always_eager: bool = Field(
        default=False,
        description="Run Celery tasks synchronously (useful for tests).",
    )
    celery_loglevel: Literal["TRACE", "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"] = (
        "INFO"
    )


@lru_cache
def get_settings() -> Settings:
    """Return cached application settings."""

    return Settings()


__all__ = ["Settings", "get_settings"]


================================================================================
# FILE: src/zistudy_api/core/__init__.py
================================================================================



================================================================================
# FILE: src/zistudy_api/core/logging.py
================================================================================

from __future__ import annotations

import logging
import sys
from typing import cast

import structlog
from structlog.typing import FilteringBoundLogger

from zistudy_api.config.settings import Settings


def configure_logging(settings: Settings) -> None:
    """Configure structlog + stdlib logging."""

    shared_processors: list[structlog.types.Processor] = [
        structlog.contextvars.merge_contextvars,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.add_log_level,
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
    ]

    if settings.log_json:
        final_processor: structlog.types.Processor = structlog.processors.JSONRenderer()
    else:
        final_processor = structlog.dev.ConsoleRenderer()

    structlog.configure(
        processors=[
            *shared_processors,
            final_processor,
        ],
        wrapper_class=structlog.make_filtering_bound_logger(
            logging.getLevelName(settings.log_level)
        ),
        cache_logger_on_first_use=True,
    )

    logging.basicConfig(
        level=settings.log_level,
        format="%(message)s",
        stream=sys.stdout,
    )


def get_logger(name: str | None = None) -> FilteringBoundLogger:
    """Return a structlog logger instance."""

    logger = structlog.get_logger(name)
    return cast(FilteringBoundLogger, logger)


__all__ = ["configure_logging", "get_logger"]


================================================================================
# FILE: src/zistudy_api/core/security.py
================================================================================

from __future__ import annotations

from datetime import datetime, timedelta, timezone
from hashlib import sha256
from secrets import token_urlsafe
from typing import Any, Mapping, cast

import jwt
from passlib.context import CryptContext

from zistudy_api.config.settings import Settings

_password_context = CryptContext(schemes=["argon2"], deprecated="auto")


def hash_password(password: str) -> str:
    """Return an argon2 hash for the supplied plaintext password."""

    return cast(str, _password_context.hash(password))


def verify_password(password: str, password_hash: str) -> bool:
    """Validate a plaintext password against the stored hash."""

    return bool(_password_context.verify(password, password_hash))


def create_access_token(
    *,
    subject: str,
    settings: Settings,
    claims: Mapping[str, Any] | None = None,
    expires_delta: timedelta | None = None,
) -> str:
    """Create a signed JWT access token."""

    now = datetime.now(tz=timezone.utc)
    payload: dict[str, Any] = {
        "sub": subject,
        "iat": int(now.timestamp()),
    }
    if expires_delta is None:
        expires_delta = timedelta(minutes=settings.access_token_exp_minutes)
    payload["exp"] = int((now + expires_delta).timestamp())
    if claims:
        payload.update(claims)

    return jwt.encode(payload, settings.jwt_secret, algorithm=settings.jwt_algorithm)


def decode_token(token: str, settings: Settings) -> dict[str, Any]:
    """Decode and validate a JWT, returning the payload."""

    payload = jwt.decode(token, settings.jwt_secret, algorithms=[settings.jwt_algorithm])
    return cast(dict[str, Any], payload)


def generate_refresh_token(settings: Settings) -> str:
    """Return a random refresh token string."""

    return token_urlsafe(settings.refresh_token_length)


def generate_api_key(settings: Settings) -> str:
    """Return a random API key string."""

    return token_urlsafe(settings.api_key_length)


def hash_token(token: str) -> str:
    """Return a deterministic SHA-256 hash for storing opaque tokens."""

    return sha256(token.encode("utf-8")).hexdigest()


__all__ = [
    "create_access_token",
    "decode_token",
    "generate_api_key",
    "generate_refresh_token",
    "hash_token",
    "hash_password",
    "verify_password",
]


================================================================================
# FILE: src/zistudy_api/db/__init__.py
================================================================================

from .models import Base

__all__ = ["Base"]


================================================================================
# FILE: src/zistudy_api/db/migrations.py
================================================================================

from __future__ import annotations

import os
from pathlib import Path

from alembic import command
from alembic.config import Config

from zistudy_api.config.settings import get_settings

PROJECT_ROOT = Path(__file__).resolve().parents[3]
DEFAULT_CFG_PATH = PROJECT_ROOT / "alembic.ini"
DEFAULT_SCRIPT_PATH = PROJECT_ROOT / "alembic"


def _resolve_path(
    *,
    env_var: str,
    default: Path,
    fallback: Path,
) -> Path:
    candidate = os.environ.get(env_var)
    if candidate:
        path = Path(candidate)
        if path.exists():
            return path

    if default.exists():
        return default
    if fallback.exists():
        return fallback
    raise RuntimeError(
        f"Alembic resource not found. Checked: {default}, {fallback}. "
        f"Set {env_var} to the correct location."
    )


def run_migrations(target_revision: str = "head") -> None:
    if os.environ.get("ZISTUDY_SKIP_MIGRATIONS", "").lower() in {"1", "true"}:
        return

    settings = get_settings()

    project_cwd = Path.cwd()
    cfg_path = _resolve_path(
        env_var="ZISTUDY_ALEMBIC_CONFIG",
        default=DEFAULT_CFG_PATH,
        fallback=project_cwd / "alembic.ini",
    )
    script_location = _resolve_path(
        env_var="ZISTUDY_ALEMBIC_PATH",
        default=DEFAULT_SCRIPT_PATH,
        fallback=project_cwd / "alembic",
    )
    try:
        candidates = ", ".join(str(path) for path in script_location.iterdir())
    except Exception:
        candidates = "<unavailable>"
    print(f"[migrations] Using script directory: {script_location} (contents: {candidates})")

    config = Config(str(cfg_path))
    config.set_main_option("script_location", str(script_location))
    config.set_main_option("sqlalchemy.url", settings.database_url)

    print(f"[migrations] Running migrations using {cfg_path} -> {script_location}")
    try:
        command.upgrade(config, target_revision)
    except Exception as exc:  # pragma: no cover - surfaced during startup
        raise RuntimeError(f"Failed to apply database migrations: {exc}") from exc

    _ensure_schema_created()


def _ensure_schema_created() -> None:
    import asyncio

    import sqlalchemy as sa
    from sqlalchemy import text
    from sqlalchemy.exc import SQLAlchemyError
    from sqlalchemy.ext.asyncio import create_async_engine

    from zistudy_api.config.settings import get_settings

    settings = get_settings()

    async def _process() -> None:
        engine = create_async_engine(settings.database_url, echo=False)
        try:
            try:
                async with engine.connect() as conn:
                    result = await conn.execute(
                        text("SELECT version_num FROM alembic_version LIMIT 1")
                    )
                    has_version = result.first() is not None
            except SQLAlchemyError:
                has_version = False

            if has_version:
                return

            print("[migrations] Alembic version table missing; creating schema via metadata.")
            from zistudy_api.db import Base

            alembic_table = sa.Table(
                "alembic_version",
                sa.MetaData(),
                sa.Column("version_num", sa.String(32), primary_key=True),
            )

            async with engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all)
                await conn.run_sync(alembic_table.create, checkfirst=True)
                await conn.execute(sa.delete(alembic_table))
                await conn.execute(
                    sa.insert(alembic_table).values(version_num="0001_initial_schema")
                )
        finally:
            await engine.dispose()

    asyncio.run(_process())


if __name__ == "__main__":  # pragma: no cover
    run_migrations()


================================================================================
# FILE: src/zistudy_api/db/models.py
================================================================================

from __future__ import annotations

from datetime import datetime, timezone
from typing import Any
from uuid import uuid4

from sqlalchemy import (
    JSON,
    Boolean,
    DateTime,
    ForeignKey,
    Index,
    Integer,
    String,
    Text,
    UniqueConstraint,
)
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship

from zistudy_api.domain.enums import CardCategory, CardType


def utcnow() -> datetime:
    return datetime.now(tz=timezone.utc)


class Base(DeclarativeBase):
    pass


class UserAccount(Base):
    __tablename__ = "users"

    id: Mapped[str] = mapped_column(String(36), primary_key=True, default=lambda: str(uuid4()))
    email: Mapped[str] = mapped_column(String(320), unique=True, index=True)
    password_hash: Mapped[str] = mapped_column(String(255))
    full_name: Mapped[str | None] = mapped_column(String(255))
    is_active: Mapped[bool] = mapped_column(Boolean, default=True, nullable=False)
    is_superuser: Mapped[bool] = mapped_column(Boolean, default=False, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=utcnow)
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, onupdate=utcnow
    )

    study_sets: Mapped[list["StudySet"]] = relationship(back_populates="owner")
    refresh_tokens: Mapped[list["RefreshToken"]] = relationship(
        back_populates="user", cascade="all, delete-orphan"
    )
    api_keys: Mapped[list["ApiKey"]] = relationship(
        back_populates="user", cascade="all, delete-orphan"
    )


class StudySet(Base):
    __tablename__ = "study_sets"

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    title: Mapped[str] = mapped_column(String(255))
    description: Mapped[str | None] = mapped_column(String(1000))
    owner_id: Mapped[str | None] = mapped_column(
        String(36), ForeignKey("users.id", ondelete="SET NULL"), nullable=True
    )
    is_private: Mapped[bool] = mapped_column(Boolean, default=True, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=utcnow)
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, onupdate=utcnow
    )

    tags: Mapped[list["StudySetTag"]] = relationship(
        back_populates="study_set", cascade="all, delete-orphan"
    )
    cards: Mapped[list["StudySetCard"]] = relationship(
        back_populates="study_set", cascade="all, delete-orphan"
    )
    owner: Mapped[UserAccount | None] = relationship(back_populates="study_sets")


class Tag(Base):
    __tablename__ = "tags"

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    name: Mapped[str] = mapped_column(String(64), unique=True)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=utcnow)
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, onupdate=utcnow
    )

    study_sets: Mapped[list["StudySetTag"]] = relationship(
        back_populates="tag", cascade="all, delete-orphan"
    )


class StudySetTag(Base):
    __tablename__ = "study_set_tags"
    __table_args__ = (UniqueConstraint("study_set_id", "tag_id", name="uq_study_set_tag"),)

    study_set_id: Mapped[int] = mapped_column(
        ForeignKey("study_sets.id", ondelete="CASCADE"), primary_key=True
    )
    tag_id: Mapped[int] = mapped_column(ForeignKey("tags.id", ondelete="CASCADE"), primary_key=True)

    study_set: Mapped[StudySet] = relationship(back_populates="tags")
    tag: Mapped[Tag] = relationship(back_populates="study_sets")


class StudyCard(Base):
    __tablename__ = "study_cards"

    __table_args__ = (
        Index("ix_study_cards_card_type", "card_type"),
        Index("ix_study_cards_difficulty", "difficulty"),
        Index("ix_study_cards_created_at", "created_at"),
        Index("ix_study_cards_updated_at", "updated_at"),
    )

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    data: Mapped[dict[str, Any]] = mapped_column(
        JSONB().with_variant(JSON(), "sqlite"), nullable=False
    )
    card_type: Mapped[CardType] = mapped_column(String(50), nullable=False)
    difficulty: Mapped[int] = mapped_column(Integer, nullable=False, default=1)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=utcnow)
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, onupdate=utcnow
    )

    answers: Mapped[list["Answer"]] = relationship(
        back_populates="study_card", cascade="all, delete-orphan"
    )
    study_sets: Mapped[list["StudySetCard"]] = relationship(
        back_populates="study_card", cascade="all, delete-orphan"
    )


class StudySetCard(Base):
    __tablename__ = "study_set_cards"
    __table_args__ = (
        UniqueConstraint("study_set_id", "card_id", "card_category", name="uq_study_set_card"),
        Index("ix_study_set_cards_set_position", "study_set_id", "position"),
        Index("ix_study_set_cards_card", "card_id", "card_category"),
    )

    study_set_id: Mapped[int] = mapped_column(
        ForeignKey("study_sets.id", ondelete="CASCADE"), primary_key=True
    )
    card_id: Mapped[int] = mapped_column(
        ForeignKey("study_cards.id", ondelete="CASCADE"), primary_key=True
    )
    card_category: Mapped[CardCategory] = mapped_column(Integer, primary_key=True)
    position: Mapped[int] = mapped_column(Integer, default=0)

    study_set: Mapped[StudySet] = relationship(back_populates="cards")
    study_card: Mapped[StudyCard] = relationship(back_populates="study_sets")


class Answer(Base):
    __tablename__ = "answers"
    __table_args__ = (
        UniqueConstraint("user_id", "study_card_id", name="uq_answer_user_card"),
        Index("ix_answers_user_id", "user_id"),
        Index("ix_answers_study_card_id", "study_card_id"),
        Index("ix_answers_is_correct", "is_correct"),
        Index("ix_answers_created_at", "created_at"),
    )

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    user_id: Mapped[str] = mapped_column(
        String(36), ForeignKey("users.id", ondelete="CASCADE"), nullable=False
    )
    study_card_id: Mapped[int] = mapped_column(
        ForeignKey("study_cards.id", ondelete="CASCADE"), nullable=False
    )
    data: Mapped[dict[str, Any]] = mapped_column(
        JSONB().with_variant(JSON(), "sqlite"), nullable=False
    )
    answer_type: Mapped[str] = mapped_column(String(50), nullable=False)
    is_correct: Mapped[int] = mapped_column(Integer, default=2, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=utcnow)
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, onupdate=utcnow
    )

    study_card: Mapped[StudyCard] = relationship(back_populates="answers")
    user: Mapped[UserAccount] = relationship()


class RefreshToken(Base):
    __tablename__ = "refresh_tokens"
    __table_args__ = (Index("ix_refresh_tokens_token_hash", "token_hash"),)

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    token_hash: Mapped[str] = mapped_column(String(128), unique=True, nullable=False)
    user_id: Mapped[str] = mapped_column(
        String(36), ForeignKey("users.id", ondelete="CASCADE"), nullable=False
    )
    expires_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False)
    revoked: Mapped[bool] = mapped_column(Boolean, default=False, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=utcnow)
    revoked_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True))

    user: Mapped[UserAccount] = relationship(back_populates="refresh_tokens")


class ApiKey(Base):
    __tablename__ = "api_keys"

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    key_hash: Mapped[str] = mapped_column(String(128), unique=True, nullable=False)
    name: Mapped[str | None] = mapped_column(String(255))
    user_id: Mapped[str] = mapped_column(
        String(36), ForeignKey("users.id", ondelete="CASCADE"), nullable=False
    )
    expires_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True))
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=utcnow)
    last_used_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True))

    user: Mapped[UserAccount] = relationship(back_populates="api_keys")


class AsyncJob(Base):
    __tablename__ = "async_jobs"
    __table_args__ = (
        Index("ix_async_jobs_owner", "owner_id"),
        Index("ix_async_jobs_status", "status"),
        Index("ix_async_jobs_type", "job_type"),
    )

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    job_type: Mapped[str] = mapped_column(String(64), nullable=False)
    status: Mapped[str] = mapped_column(String(32), nullable=False)
    owner_id: Mapped[str] = mapped_column(String(36), nullable=False)
    payload: Mapped[dict[str, Any]] = mapped_column(JSONB().with_variant(JSON(), "sqlite"))
    result: Mapped[dict[str, Any] | None] = mapped_column(JSONB().with_variant(JSON(), "sqlite"))
    error: Mapped[str | None] = mapped_column(Text())
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=utcnow)
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, onupdate=utcnow
    )
    started_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True))
    completed_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True))


__all__ = [
    "AsyncJob",
    "ApiKey",
    "Answer",
    "Base",
    "RefreshToken",
    "StudyCard",
    "StudySet",
    "StudySetCard",
    "StudySetTag",
    "Tag",
    "UserAccount",
    "utcnow",
]


================================================================================
# FILE: src/zistudy_api/db/repositories/__init__.py
================================================================================



================================================================================
# FILE: src/zistudy_api/db/repositories/answers.py
================================================================================

from __future__ import annotations

from collections.abc import Sequence
from datetime import datetime

from sqlalchemy import Select, case, func, select
from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.models import Answer, StudySetCard
from zistudy_api.domain.schemas.answers import AnswerCreate, serialize_answer_data


class AnswerRepository:
    """Data access helper for answer persistence and analytics."""

    def __init__(self, session: AsyncSession):
        self._session = session

    async def create(self, *, user_id: str, payload: AnswerCreate) -> Answer:
        data_payload = serialize_answer_data(payload.data)
        entity = Answer(
            user_id=user_id,
            study_card_id=payload.study_card_id,
            data=data_payload,
            answer_type=payload.answer_type,
            is_correct=int(payload.is_correct) if payload.is_correct is not None else 2,
        )
        if payload.expected_answer is not None:
            entity.data.setdefault("expected", payload.expected_answer)
        if payload.evaluation_notes is not None:
            entity.data.setdefault("evaluation_notes", payload.evaluation_notes)
        if payload.latency_ms is not None:
            entity.data.setdefault("latency_ms", payload.latency_ms)
        self._session.add(entity)
        await self._session.flush()
        await self._session.refresh(entity)
        return entity

    async def get_by_id(self, answer_id: int) -> Answer | None:
        stmt: Select[tuple[Answer]] = select(Answer).where(Answer.id == answer_id)
        result = await self._session.execute(stmt)
        return result.scalar_one_or_none()

    async def list_for_user(
        self,
        *,
        user_id: str,
        page: int,
        page_size: int,
    ) -> tuple[int, list[Answer]]:
        stmt: Select[tuple[Answer]] = (
            select(Answer).where(Answer.user_id == user_id).order_by(Answer.created_at.desc())
        )

        total_stmt = select(func.count()).select_from(stmt.subquery())
        total = await self._session.scalar(total_stmt) or 0

        stmt = stmt.offset((page - 1) * page_size).limit(page_size)
        result = await self._session.execute(stmt)
        items = list(result.scalars().all())
        return total, items

    async def stats_for_card(
        self, *, study_card_id: int, user_id: str | None = None
    ) -> tuple[int, int]:
        stmt = select(
            func.count(),
            func.sum(case((Answer.is_correct == 1, 1), else_=0)),
        ).where(Answer.study_card_id == study_card_id)
        if user_id is not None:
            stmt = stmt.where(Answer.user_id == user_id)
        result = await self._session.execute(stmt)
        total, correct = result.one()
        return int(total or 0), int(correct or 0)

    async def per_set_progress(
        self, *, user_id: str, study_set_ids: Sequence[int]
    ) -> list[tuple[int, int, int, int, datetime | None]]:
        if not study_set_ids:
            return []

        answers_subq = (
            select(
                Answer.study_card_id.label("card_id"),
                func.max(Answer.created_at).label("last_answered"),
                func.max(Answer.is_correct == 1).label("was_correct"),
                func.count(Answer.id).label("attempts"),
            )
            .where(Answer.user_id == user_id)
            .group_by(Answer.study_card_id)
            .subquery()
        )

        stmt = (
            select(
                StudySetCard.study_set_id,
                func.count(StudySetCard.card_id).label("total_cards"),
                func.count(answers_subq.c.card_id).label("attempted"),
                func.sum(case((answers_subq.c.was_correct.is_(True), 1), else_=0)).label("correct"),
                func.max(answers_subq.c.last_answered).label("last_answered"),
            )
            .outerjoin(answers_subq, answers_subq.c.card_id == StudySetCard.card_id)
            .where(StudySetCard.study_set_id.in_(study_set_ids))
            .group_by(StudySetCard.study_set_id)
        )

        result = await self._session.execute(stmt)
        rows = result.all()
        progress: list[tuple[int, int, int, int, datetime | None]] = []
        for row in rows:
            study_set_id, total_cards, attempted, correct, last_answered = row
            progress.append(
                (
                    int(study_set_id),
                    int(total_cards or 0),
                    int(attempted or 0),
                    int(correct or 0),
                    last_answered,
                )
            )
        return progress


__all__ = ["AnswerRepository"]


================================================================================
# FILE: src/zistudy_api/db/repositories/api_keys.py
================================================================================

from __future__ import annotations

from datetime import datetime, timedelta, timezone

from sqlalchemy import Select, delete, select, update
from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.models import ApiKey


class ApiKeyRepository:
    """Persistence operations for API keys."""

    def __init__(self, session: AsyncSession):
        self._session = session

    async def create(
        self,
        *,
        user_id: str,
        key_hash: str,
        name: str | None,
        expires_in_hours: int | None,
    ) -> ApiKey:
        expires_at = (
            datetime.now(tz=timezone.utc) + timedelta(hours=expires_in_hours)
            if expires_in_hours is not None
            else None
        )
        entity = ApiKey(
            user_id=user_id,
            key_hash=key_hash,
            name=name,
            expires_at=expires_at,
        )
        self._session.add(entity)
        await self._session.flush()
        await self._session.refresh(entity)
        return entity

    async def list_for_user(self, user_id: str) -> list[ApiKey]:
        stmt: Select[tuple[ApiKey]] = select(ApiKey).where(ApiKey.user_id == user_id)
        result = await self._session.execute(stmt)
        return list(result.scalars().all())

    async def get_by_hash(self, key_hash: str) -> ApiKey | None:
        stmt: Select[tuple[ApiKey]] = select(ApiKey).where(ApiKey.key_hash == key_hash)
        result = await self._session.execute(stmt)
        return result.scalar_one_or_none()

    async def delete(self, api_key_id: int) -> None:
        await self._session.execute(delete(ApiKey).where(ApiKey.id == api_key_id))

    async def touch_last_used(self, api_key_id: int) -> None:
        await self._session.execute(
            update(ApiKey)
            .where(ApiKey.id == api_key_id)
            .values(last_used_at=datetime.now(tz=timezone.utc))
        )


__all__ = ["ApiKeyRepository"]


================================================================================
# FILE: src/zistudy_api/db/repositories/jobs.py
================================================================================

from __future__ import annotations

from datetime import datetime, timezone

from sqlalchemy import Select, select, update
from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.models import AsyncJob


class JobRepository:
    """Persistence layer for tracking async jobs."""

    def __init__(self, session: AsyncSession):
        self._session = session

    async def create(
        self,
        *,
        job_type: str,
        owner_id: str,
        payload: dict,
        status: str = "pending",
    ) -> AsyncJob:
        entity = AsyncJob(
            job_type=job_type,
            owner_id=owner_id,
            payload=payload,
            status=status,
        )
        self._session.add(entity)
        await self._session.flush()
        await self._session.refresh(entity)
        return entity

    async def get(self, job_id: int) -> AsyncJob | None:
        stmt: Select[tuple[AsyncJob]] = select(AsyncJob).where(AsyncJob.id == job_id)
        result = await self._session.execute(stmt)
        return result.scalar_one_or_none()

    async def list_for_owner(self, owner_id: str) -> list[AsyncJob]:
        stmt: Select[tuple[AsyncJob]] = (
            select(AsyncJob)
            .where(AsyncJob.owner_id == owner_id)
            .order_by(AsyncJob.created_at.desc())
        )
        result = await self._session.execute(stmt)
        return list(result.scalars().all())

    async def set_status(
        self,
        job_id: int,
        *,
        status: str,
        started_at: datetime | None = None,
        completed_at: datetime | None = None,
        error: str | None = None,
    ) -> None:
        values: dict[str, datetime | str | None] = {
            "status": status,
            "updated_at": datetime.now(tz=timezone.utc),
        }
        if started_at is not None:
            values["started_at"] = started_at
        if completed_at is not None:
            values["completed_at"] = completed_at
        if error is not None:
            values["error"] = error
        await self._session.execute(update(AsyncJob).where(AsyncJob.id == job_id).values(**values))

    async def set_result(self, job_id: int, result: dict) -> None:
        await self._session.execute(
            update(AsyncJob)
            .where(AsyncJob.id == job_id)
            .values(result=result, updated_at=datetime.now(tz=timezone.utc))
        )


__all__ = ["JobRepository"]


================================================================================
# FILE: src/zistudy_api/db/repositories/refresh_tokens.py
================================================================================

from __future__ import annotations

from datetime import datetime, timezone

from sqlalchemy import Select, delete, select, update
from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.models import RefreshToken


class RefreshTokenRepository:
    """Persistence layer for refresh tokens."""

    def __init__(self, session: AsyncSession):
        self._session = session

    async def create(
        self,
        *,
        token_hash: str,
        user_id: str,
        expires_at: datetime,
    ) -> RefreshToken:
        entity = RefreshToken(
            token_hash=token_hash,
            user_id=user_id,
            expires_at=expires_at,
        )
        self._session.add(entity)
        await self._session.flush()
        await self._session.refresh(entity)
        return entity

    async def get_by_hash(self, token_hash: str) -> RefreshToken | None:
        stmt: Select[tuple[RefreshToken]] = select(RefreshToken).where(
            RefreshToken.token_hash == token_hash
        )
        result = await self._session.execute(stmt)
        return result.scalar_one_or_none()

    async def revoke(self, token_id: int) -> None:
        await self._session.execute(
            update(RefreshToken)
            .where(RefreshToken.id == token_id)
            .values(revoked=True, revoked_at=datetime.now(tz=timezone.utc))
        )

    async def revoke_all_for_user(self, user_id: str) -> None:
        await self._session.execute(
            update(RefreshToken)
            .where(RefreshToken.user_id == user_id)
            .values(revoked=True, revoked_at=datetime.now(tz=timezone.utc))
        )

    async def delete_expired(self) -> None:
        await self._session.execute(
            delete(RefreshToken).where(RefreshToken.expires_at < datetime.now(tz=timezone.utc))
        )

    async def delete(self, token_id: int) -> None:
        await self._session.execute(delete(RefreshToken).where(RefreshToken.id == token_id))


__all__ = ["RefreshTokenRepository"]


================================================================================
# FILE: src/zistudy_api/db/repositories/study_cards.py
================================================================================

from __future__ import annotations

from collections.abc import Sequence
from typing import Any, Iterable

from sqlalchemy import Select, String, func, select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from zistudy_api.db.models import StudyCard, StudySetCard
from zistudy_api.domain.enums import CardType
from zistudy_api.domain.schemas.base import BaseSchema
from zistudy_api.domain.schemas.study_cards import (
    CardData,
    CardSearchRequest,
    StudyCardCreate,
    StudyCardUpdate,
)


def _serialize_card_data(data: CardData | dict[str, Any]) -> dict[str, Any]:
    if isinstance(data, BaseSchema):
        return data.model_dump(mode="json")
    return data


class StudyCardRepository:
    """Data access layer for study cards."""

    def __init__(self, session: AsyncSession):
        self._session = session

    async def create(self, payload: StudyCardCreate) -> StudyCard:
        entity = StudyCard(
            card_type=payload.card_type,
            data=_serialize_card_data(payload.data),
            difficulty=payload.difficulty,
        )
        self._session.add(entity)
        await self._session.flush()
        await self._session.refresh(entity)
        return entity

    async def get_by_id(self, card_id: int) -> StudyCard | None:
        stmt: Select[tuple[StudyCard]] = select(StudyCard).where(StudyCard.id == card_id)
        result = await self._session.execute(stmt)
        return result.scalar_one_or_none()

    async def update(self, card_id: int, payload: StudyCardUpdate) -> StudyCard | None:
        entity = await self.get_by_id(card_id)
        if entity is None:
            return None

        if payload.data is not None:
            entity.data = _serialize_card_data(payload.data)
        if payload.difficulty is not None:
            entity.difficulty = payload.difficulty
        await self._session.flush()
        await self._session.refresh(entity)
        return entity

    async def delete(self, card_id: int) -> bool:
        entity = await self.get_by_id(card_id)
        if entity is None:
            return False
        await self._session.delete(entity)
        return True

    async def list_cards(
        self,
        card_type: CardType | None,
        page: int,
        page_size: int,
    ) -> tuple[int, list[StudyCard]]:
        stmt: Select[tuple[StudyCard]] = select(StudyCard).order_by(StudyCard.created_at.desc())
        if card_type is not None:
            stmt = stmt.where(StudyCard.card_type == card_type.value)

        total_stmt = select(func.count()).select_from(stmt.subquery())
        total = await self._session.scalar(total_stmt) or 0

        stmt = stmt.options(selectinload(StudyCard.answers))
        stmt = stmt.offset((page - 1) * page_size).limit(page_size)
        result = await self._session.execute(stmt)
        items = list(result.scalars().all())
        return total, items

    async def get_many(self, card_ids: Sequence[int]) -> list[StudyCard]:
        if not card_ids:
            return []

        stmt: Select[tuple[StudyCard]] = select(StudyCard).where(StudyCard.id.in_(card_ids))
        result = await self._session.execute(stmt)
        return list(result.scalars().all())

    async def search_cards(self, request: CardSearchRequest) -> tuple[int, list[StudyCard]]:
        stmt: Select[tuple[StudyCard]] = select(StudyCard)

        if request.query:
            like_term = f"%{request.query.strip()}%"
            stmt = stmt.where(StudyCard.data.cast(String).ilike(like_term))

        filters = request.filters
        if filters.card_types:
            stmt = stmt.where(
                StudyCard.card_type.in_([card_type.value for card_type in filters.card_types])
            )

        if filters.min_difficulty is not None:
            stmt = stmt.where(StudyCard.difficulty >= filters.min_difficulty)
        if filters.max_difficulty is not None:
            stmt = stmt.where(StudyCard.difficulty <= filters.max_difficulty)

        if filters.study_set_ids:
            stmt = stmt.join(StudySetCard, StudySetCard.card_id == StudyCard.id).where(
                StudySetCard.study_set_id.in_(filters.study_set_ids)
            )

        stmt = stmt.order_by(StudyCard.created_at.desc()).distinct()

        total_stmt = select(func.count()).select_from(stmt.subquery())
        total = await self._session.scalar(total_stmt) or 0

        stmt = stmt.options(selectinload(StudyCard.answers))
        stmt = stmt.offset((request.page - 1) * request.page_size).limit(request.page_size)
        result = await self._session.execute(stmt)
        items = list(result.scalars().all())
        return total, items

    async def list_not_in_set(
        self,
        *,
        study_set_id: int,
        card_type: CardType | None,
        page: int,
        page_size: int,
    ) -> tuple[int, list[StudyCard]]:
        excluded_cards = select(StudySetCard.card_id).where(
            StudySetCard.study_set_id == study_set_id
        )

        stmt: Select[tuple[StudyCard]] = (
            select(StudyCard)
            .where(~StudyCard.id.in_(excluded_cards))
            .order_by(StudyCard.created_at.desc())
        )

        if card_type is not None:
            stmt = stmt.where(StudyCard.card_type == card_type.value)

        total_stmt = select(func.count()).select_from(stmt.subquery())
        total = await self._session.scalar(total_stmt) or 0

        stmt = stmt.options(selectinload(StudyCard.answers))
        stmt = stmt.offset((page - 1) * page_size).limit(page_size)
        result = await self._session.execute(stmt)
        items = list(result.scalars().all())
        return total, items

    async def import_cards(self, cards: Iterable[StudyCardCreate]) -> list[StudyCard]:
        payload = list(cards)
        if not payload:
            return []

        entities = [
            StudyCard(
                card_type=item.card_type,
                data=_serialize_card_data(item.data),
                difficulty=item.difficulty,
            )
            for item in payload
        ]
        self._session.add_all(entities)
        await self._session.flush()
        for entity in entities:
            await self._session.refresh(entity)
        return entities


__all__ = ["StudyCardRepository"]


================================================================================
# FILE: src/zistudy_api/db/repositories/study_sets.py
================================================================================

from __future__ import annotations

from collections.abc import Sequence

from sqlalchemy import Select, func, select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from zistudy_api.db.models import StudyCard, StudySet, StudySetCard, StudySetTag, Tag
from zistudy_api.domain.enums import CardCategory, CardType
from zistudy_api.domain.schemas.study_sets import StudySetCreate, StudySetUpdate


class StudySetRepository:
    """Data access operations for study sets."""

    def __init__(self, session: AsyncSession):
        self._session = session

    async def create(self, payload: StudySetCreate, owner_id: str | None) -> StudySet:
        entity = StudySet(
            title=payload.title,
            description=payload.description,
            is_private=payload.is_private,
            owner_id=owner_id,
        )
        self._session.add(entity)
        await self._session.flush()
        await self._session.refresh(entity)
        return entity

    async def get_by_id(self, study_set_id: int) -> StudySet | None:
        stmt: Select[tuple[StudySet]] = (
            select(StudySet)
            .options(
                selectinload(StudySet.tags).joinedload(StudySetTag.tag),
                selectinload(StudySet.cards),
                selectinload(StudySet.owner),
            )
            .where(StudySet.id == study_set_id)
        )
        result = await self._session.execute(stmt)
        return result.scalar_one_or_none()

    async def attach_tags(self, entity: StudySet, tags: Sequence[Tag]) -> None:
        await self._session.execute(
            select(StudySetTag).where(StudySetTag.study_set_id == entity.id)
        )
        await self._session.refresh(entity, attribute_names=["tags"])
        entity.tags.clear()
        for tag in tags:
            entity.tags.append(StudySetTag(tag=tag))
        await self._session.flush()

    async def update(self, entity: StudySet, payload: StudySetUpdate) -> StudySet:
        if payload.title is not None:
            entity.title = payload.title
        if payload.description is not None:
            entity.description = payload.description
        if payload.is_private is not None:
            entity.is_private = payload.is_private
        await self._session.flush()
        await self._session.refresh(entity)
        return entity

    async def delete(self, entity: StudySet) -> None:
        await self._session.delete(entity)

    async def list_accessible(
        self,
        *,
        current_user: str | None,
        show_only_owned: bool,
        search_query: str | None,
        page: int,
        page_size: int,
    ) -> tuple[int, list[StudySet]]:
        stmt: Select[tuple[StudySet]] = (
            select(StudySet)
            .options(
                selectinload(StudySet.tags).joinedload(StudySetTag.tag),
                selectinload(StudySet.owner),
            )
            .order_by(StudySet.created_at.desc())
        )

        if show_only_owned and current_user:
            stmt = stmt.where(StudySet.owner_id == current_user)
        elif current_user:
            stmt = stmt.where(
                (StudySet.is_private.is_(False))
                | (StudySet.owner_id == current_user)
                | (StudySet.owner_id.is_(None))
            )
        else:
            stmt = stmt.where(StudySet.is_private.is_(False))

        if search_query:
            like_term = f"%{search_query.strip()}%"
            stmt = stmt.where(
                (StudySet.title.ilike(like_term)) | (StudySet.description.ilike(like_term))
            )

        total_stmt = select(func.count()).select_from(stmt.subquery())
        total = await self._session.scalar(total_stmt)
        total = total or 0

        stmt = stmt.offset((page - 1) * page_size).limit(page_size)
        result = await self._session.execute(stmt)
        items = list(result.scalars().all())
        return total, items

    async def get_card_counts(self, study_set_id: int) -> dict[str, int]:
        stmt_total = (
            select(func.count())
            .select_from(StudySetCard)
            .where(StudySetCard.study_set_id == study_set_id)
        )
        total = await self._session.scalar(stmt_total) or 0

        stmt_questions = (
            select(func.count())
            .select_from(StudySetCard)
            .where(
                StudySetCard.study_set_id == study_set_id,
                StudySetCard.card_category == CardCategory.QUESTION,
            )
        )
        question_count = await self._session.scalar(stmt_questions) or 0

        return {"total": total, "questions": question_count}

    async def add_cards(
        self,
        *,
        study_set_id: int,
        card_ids: Sequence[int],
        card_category: CardCategory,
    ) -> int:
        if not card_ids:
            return 0

        existing_stmt: Select[tuple[int]] = select(StudySetCard.card_id).where(
            StudySetCard.study_set_id == study_set_id,
            StudySetCard.card_category == card_category,
            StudySetCard.card_id.in_(card_ids),
        )
        existing_result = await self._session.execute(existing_stmt)
        existing_ids = set(existing_result.scalars().all())

        filtered_ids = [card_id for card_id in card_ids if card_id not in existing_ids]
        if not filtered_ids:
            return 0

        max_position_stmt = select(func.coalesce(func.max(StudySetCard.position), 0)).where(
            StudySetCard.study_set_id == study_set_id,
            StudySetCard.card_category == card_category,
        )
        start_position = await self._session.scalar(max_position_stmt) or 0

        for index, card_id in enumerate(filtered_ids, start=1):
            self._session.add(
                StudySetCard(
                    study_set_id=study_set_id,
                    card_id=card_id,
                    card_category=card_category,
                    position=start_position + index,
                )
            )

        return len(filtered_ids)

    async def remove_cards(
        self,
        *,
        study_set_id: int,
        card_ids: Sequence[int],
        card_category: CardCategory,
    ) -> int:
        if not card_ids:
            return 0

        delete_stmt = (
            select(StudySetCard)
            .where(
                StudySetCard.study_set_id == study_set_id,
                StudySetCard.card_category == card_category,
                StudySetCard.card_id.in_(card_ids),
            )
            .with_for_update()
        )

        result = await self._session.execute(delete_stmt)
        records = list(result.scalars().all())
        for record in records:
            await self._session.delete(record)

        return len(records)

    async def list_cards(
        self,
        *,
        study_set_id: int,
        card_type: CardType | None,
        page: int,
        page_size: int,
    ) -> tuple[int, list[tuple[StudyCard, int]]]:
        count_stmt = (
            select(func.count())
            .select_from(StudySetCard)
            .join(StudyCard, StudySetCard.card_id == StudyCard.id)
            .where(StudySetCard.study_set_id == study_set_id)
        )

        data_stmt = (
            select(StudyCard, StudySetCard.position)
            .select_from(StudySetCard)
            .join(StudyCard, StudySetCard.card_id == StudyCard.id)
            .where(StudySetCard.study_set_id == study_set_id)
            .order_by(StudySetCard.position.asc(), StudyCard.id.asc())
        )

        if card_type is not None:
            count_stmt = count_stmt.where(StudyCard.card_type == card_type.value)
            data_stmt = data_stmt.where(StudyCard.card_type == card_type.value)

        total = await self._session.scalar(count_stmt)
        total = total or 0

        data_stmt = data_stmt.offset((page - 1) * page_size).limit(page_size)
        result = await self._session.execute(data_stmt)
        rows = result.all()
        items = [(row[0], row[1]) for row in rows]
        return total, items

    async def list_for_card(self, card_id: int) -> list[StudySet]:
        stmt = (
            select(StudySet)
            .join(StudySetCard, StudySetCard.study_set_id == StudySet.id)
            .options(
                selectinload(StudySet.tags).joinedload(StudySetTag.tag),
                selectinload(StudySet.owner),
            )
            .where(StudySetCard.card_id == card_id)
            .order_by(StudySet.title.asc())
        )
        result = await self._session.execute(stmt)
        return list(result.scalars().unique().all())

    async def get_cards_with_details(
        self, study_set_id: int
    ) -> list[tuple[StudySetCard, StudyCard]]:
        stmt: Select[tuple[StudySetCard, StudyCard]] = (
            select(StudySetCard, StudyCard)
            .join(StudyCard, StudyCard.id == StudySetCard.card_id)
            .where(StudySetCard.study_set_id == study_set_id)
            .order_by(StudySetCard.position.asc())
        )
        result = await self._session.execute(stmt)
        rows = result.all()
        return [(row[0], row[1]) for row in rows]


__all__ = ["StudySetRepository"]


================================================================================
# FILE: src/zistudy_api/db/repositories/tags.py
================================================================================

from __future__ import annotations

from collections.abc import Iterable

from sqlalchemy import Select, func, select
from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.models import StudySetTag, Tag


class TagRepository:
    """Repository providing CRUD operations for tags."""

    def __init__(self, session: AsyncSession):
        self._session = session

    async def list_by_names(self, names: Iterable[str]) -> list[Tag]:
        normalized = {name.strip() for name in names if name.strip()}
        if not normalized:
            return []

        stmt: Select[tuple[Tag]] = select(Tag).where(Tag.name.in_(normalized))
        result = await self._session.execute(stmt)
        return list(result.scalars().all())

    async def list_all(self) -> list[Tag]:
        stmt: Select[tuple[Tag]] = select(Tag).order_by(Tag.name.asc())
        result = await self._session.execute(stmt)
        return list(result.scalars().all())

    async def ensure_tags(self, names: Iterable[str]) -> list[Tag]:
        normalized = [name.strip() for name in names if name.strip()]
        if not normalized:
            return []

        existing = await self.list_by_names(normalized)
        existing_map = {tag.name: tag for tag in existing}

        ordered: list[Tag] = []
        created: dict[str, Tag] = {}

        for name in normalized:
            if name in existing_map:
                ordered.append(existing_map[name])
                continue

            tag = Tag(name=name)
            self._session.add(tag)
            created[name] = tag
            ordered.append(tag)

        if created:
            await self._session.flush()
            for tag in created.values():
                await self._session.refresh(tag)

        return ordered

    async def search(self, query: str, limit: int = 20) -> tuple[int, list[Tag]]:
        pattern = f"%{query.strip()}%"
        stmt = select(Tag).where(Tag.name.ilike(pattern)).order_by(Tag.name.asc())
        count_stmt = select(func.count()).select_from(stmt.subquery())
        total = await self._session.scalar(count_stmt) or 0
        stmt = stmt.limit(limit)
        result = await self._session.execute(stmt)
        return total, list(result.scalars().all())

    async def popular(self, limit: int = 10) -> list[tuple[Tag, int]]:
        stmt = (
            select(Tag, func.count(StudySetTag.study_set_id).label("usage"))
            .join(StudySetTag, StudySetTag.tag_id == Tag.id)
            .group_by(Tag.id)
            .order_by(func.count(StudySetTag.study_set_id).desc(), Tag.name.asc())
            .limit(limit)
        )
        result = await self._session.execute(stmt)
        rows = result.all()
        return [(row[0], int(row[1])) for row in rows]


__all__ = ["TagRepository"]


================================================================================
# FILE: src/zistudy_api/db/repositories/users.py
================================================================================

from __future__ import annotations

from datetime import datetime, timezone

from sqlalchemy import Select, select, update
from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.models import UserAccount


class UserRepository:
    """Persistence layer for user accounts."""

    def __init__(self, session: AsyncSession):
        self._session = session

    async def create(
        self,
        *,
        email: str,
        password_hash: str,
        full_name: str | None,
        is_superuser: bool = False,
    ) -> UserAccount:
        entity = UserAccount(
            email=email,
            password_hash=password_hash,
            full_name=full_name,
            is_superuser=is_superuser,
        )
        self._session.add(entity)
        await self._session.flush()
        await self._session.refresh(entity)
        return entity

    async def get_by_email(self, email: str) -> UserAccount | None:
        stmt: Select[tuple[UserAccount]] = select(UserAccount).where(UserAccount.email == email)
        result = await self._session.execute(stmt)
        return result.scalar_one_or_none()

    async def get_by_id(self, user_id: str) -> UserAccount | None:
        stmt: Select[tuple[UserAccount]] = select(UserAccount).where(UserAccount.id == user_id)
        result = await self._session.execute(stmt)
        return result.scalar_one_or_none()

    async def touch_last_login(self, user_id: str) -> None:
        await self._session.execute(
            update(UserAccount)
            .where(UserAccount.id == user_id)
            .values(updated_at=datetime.now(tz=timezone.utc))
        )


__all__ = ["UserRepository"]


================================================================================
# FILE: src/zistudy_api/db/session.py
================================================================================

from __future__ import annotations

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from typing import Callable

from sqlalchemy.ext.asyncio import (
    AsyncEngine,
    AsyncSession,
    async_sessionmaker,
    create_async_engine,
)

from zistudy_api.config.settings import Settings, get_settings

EngineFactory = Callable[[Settings], AsyncEngine]

_engine: AsyncEngine | None = None
_sessionmaker: async_sessionmaker[AsyncSession] | None = None
_engine_factory: EngineFactory | None = None


def configure_engine_factory(factory: EngineFactory) -> None:
    """Allow tests to configure a custom engine factory."""

    global _engine_factory, _engine, _sessionmaker
    _engine_factory = factory
    _engine = None
    _sessionmaker = None


def get_engine(settings: Settings | None = None) -> AsyncEngine:
    """Return the singleton async engine."""

    global _engine, _sessionmaker

    if _engine is not None:
        return _engine

    settings = settings or get_settings()
    factory = _engine_factory or _create_engine
    _engine = factory(settings)
    _sessionmaker = async_sessionmaker(_engine, expire_on_commit=False)
    return _engine


def _create_engine(settings: Settings) -> AsyncEngine:
    return create_async_engine(
        settings.database_url,
        echo=settings.db_echo,
        pool_size=settings.db_pool_size,
        max_overflow=settings.db_max_overflow,
    )


def get_sessionmaker(settings: Settings | None = None) -> async_sessionmaker[AsyncSession]:
    """Return the async sessionmaker, initialising it if necessary."""

    global _sessionmaker
    if _sessionmaker is None:
        get_engine(settings)
        assert _sessionmaker is not None  # For type-checkers
    return _sessionmaker


async def get_session() -> AsyncIterator[AsyncSession]:
    """FastAPI dependency yielding a database session."""

    session_factory = get_sessionmaker()
    async with session_factory() as session:
        yield session


@asynccontextmanager
async def lifespan_context() -> AsyncIterator[None]:
    """Gracefully dispose engine during FastAPI lifespan events."""

    try:
        yield
    finally:
        if _engine is not None:
            await _engine.dispose()


async def reset_engine() -> None:
    """Dispose the current engine/sessionmaker for tests."""

    global _engine, _sessionmaker
    if _engine is not None:
        await _engine.dispose()
    _engine = None
    _sessionmaker = None


__all__ = [
    "configure_engine_factory",
    "get_engine",
    "get_session",
    "get_sessionmaker",
    "lifespan_context",
    "reset_engine",
]


================================================================================
# FILE: src/zistudy_api/domain/__init__.py
================================================================================



================================================================================
# FILE: src/zistudy_api/domain/enums.py
================================================================================

from __future__ import annotations

from enum import Enum


class CardType(str, Enum):
    MCQ_SINGLE = "mcq_single"
    MCQ_MULTI = "mcq_multi"
    WRITTEN = "written"
    TRUE_FALSE = "true_false"
    CLOZE = "cloze"
    EMQ = "emq"
    NOTE = "note"
    FLASHCARD = "flashcard"

    @property
    def is_question(self) -> bool:
        return self in {
            CardType.MCQ_SINGLE,
            CardType.MCQ_MULTI,
            CardType.WRITTEN,
            CardType.TRUE_FALSE,
            CardType.CLOZE,
            CardType.EMQ,
        }

    @property
    def category(self) -> "CardCategory":
        return CardCategory.QUESTION if self.is_question else CardCategory.NOTE


class CardCategory(int, Enum):
    QUESTION = 1
    NOTE = 2


__all__ = ["CardCategory", "CardType"]


================================================================================
# FILE: src/zistudy_api/domain/schemas/__init__.py
================================================================================



================================================================================
# FILE: src/zistudy_api/domain/schemas/ai.py
================================================================================

"""Schema objects capturing AI generation inputs and outputs."""

from __future__ import annotations

from typing import Literal

from pydantic import Field

from zistudy_api.domain.enums import CardType
from zistudy_api.domain.schemas.base import BaseSchema
from zistudy_api.domain.schemas.study_cards import Difficulty, StudyCardRead


class StudyCardGenerationRequest(BaseSchema):
    """Payload submitted when requesting AI generated cards."""

    topics: list[str] = Field(default_factory=list, description="Core subjects to emphasise.")
    clinical_focus: list[str] = Field(
        default_factory=list,
        description="Specific diseases, systems, or patient populations to prioritise.",
    )
    learning_objectives: list[str] = Field(
        default_factory=list,
        description="Granular competencies the learner wants to master.",
    )
    target_card_count: int | None = Field(
        default=None,
        ge=1,
        le=60,
        description="Desired number of cards; falls back to configuration defaults when omitted.",
    )
    preferred_card_types: list[CardType] = Field(
        default_factory=list, description="Optional restriction on generated card types."
    )
    difficulty_profile: Literal["balanced", "advanced", "foundational"] = Field(
        default="balanced",
        description="Controls the overall distribution of difficulty values.",
    )
    temperature: float | None = Field(
        default=None,
        ge=0.0,
        le=2.0,
        description="Optional creativity override for the LLM call.",
    )
    model: str | None = Field(
        default=None,
        description="Override model identifier for the generation request.",
    )
    include_retention_aid: bool = Field(
        default=True,
        description="Include a markdown retention aid in the response.",
    )
    learner_level: str | None = Field(
        default=None,
        description="Short descriptor of the learner's current training stage.",
    )
    context_hints: str | None = Field(
        default=None,
        description="Additional free-form hints or priorities for the generator.",
    )
    existing_card_ids: list[int] = Field(
        default_factory=list,
        description="Identifiers of previously generated cards to reference and avoid duplicating.",
    )


class StudyCardGenerationSummary(BaseSchema):
    """Summary of the generation run returned to clients."""

    card_count: int
    sources: list[str] = Field(
        default_factory=list,
        description="Names of ingested files or other context sources.",
    )
    model_used: str
    temperature_applied: float


class AiGeneratedCardOption(BaseSchema):
    """Option emitted by the AI model for MCQ style cards."""

    id: str = Field(..., description="Stable identifier for the option (e.g. letter).")
    text: str = Field(..., description="Option text rendered to the learner.")


class AiGeneratedRationale(BaseSchema):
    """Structured teaching content for a generated question."""

    primary: str = Field(..., description="Primary explanation for the correct answer.")
    alternatives: dict[str, str] = Field(
        default_factory=dict,
        description="Explanation keyed by option id for why alternatives are incorrect.",
    )


class AiGeneratedPayload(BaseSchema):
    """Raw payload returned by the language model."""

    question: str = Field(..., description="Full exam-style stem or prompt.")
    options: list[AiGeneratedCardOption] | None = Field(
        default=None, description="Options for MCQ/EMQ cards when relevant."
    )
    correct_answers: list[str] = Field(
        default_factory=list,
        description="Identifiers or textual answers considered correct.",
    )
    rationale: AiGeneratedRationale = Field(..., description="Detailed explanation bundle.")
    connections: list[str] = Field(
        default_factory=list,
        description="Cross-links to related concepts or clinical pearls.",
    )
    glossary: dict[str, str] = Field(
        default_factory=dict,
        description="Definitions for uncommon terminology mentioned in the card.",
    )
    numerical_ranges: list[str] = Field(
        default_factory=list,
        description="Reference ranges or quantitative anchors mentioned in the stem.",
    )
    references: list[str] = Field(
        default_factory=list,
        description="Optional citations or guideline references.",
    )


class AiGeneratedCard(BaseSchema):
    """Generated card enriched with metadata."""

    card_type: CardType
    difficulty: Difficulty
    payload: AiGeneratedPayload


class AiRetentionAid(BaseSchema):
    """Markdown retention summary returned alongside questions."""

    markdown: str = Field(..., description="Markdown-formatted retention summary.")


class AiGeneratedStudyCardSet(BaseSchema):
    """Complete response bundle returned by Gemini."""

    cards: list[AiGeneratedCard]
    retention_aid: AiRetentionAid | None = Field(
        default=None,
        description="Optional markdown retention aid for the generated set.",
    )


class StudyCardGenerationResponse(BaseSchema):
    """API response structure exposed to clients."""

    cards: list[AiGeneratedCard]
    retention_aid: AiRetentionAid | None = None
    summary: StudyCardGenerationSummary


class StudyCardGenerationResult(BaseSchema):
    """Internal persistence result paired with the raw generation payload."""

    cards: list[StudyCardRead]
    retention_aid: AiRetentionAid | None = None
    summary: StudyCardGenerationSummary
    raw_generation: AiGeneratedStudyCardSet


__all__ = [
    "AiGeneratedCard",
    "AiGeneratedCardOption",
    "AiGeneratedPayload",
    "AiGeneratedRationale",
    "AiGeneratedStudyCardSet",
    "AiRetentionAid",
    "StudyCardGenerationRequest",
    "StudyCardGenerationResponse",
    "StudyCardGenerationSummary",
    "StudyCardGenerationResult",
]


================================================================================
# FILE: src/zistudy_api/domain/schemas/answers.py
================================================================================

"""Typed answer payloads plus helpers for normalising historic data."""

from __future__ import annotations

from datetime import datetime
from typing import Any, TypeVar

from pydantic import Field, ValidationError, model_validator

from zistudy_api.domain.schemas.base import ALLOW_EXTRA_SCHEMA_CONFIG, BaseSchema
from zistudy_api.domain.schemas.study_cards import EmqMatch

T = TypeVar("T", bound=BaseSchema)


def _maybe_model(model: type[T], value: Any) -> T | None:
    """Best-effort validator used by the normalisation helpers."""

    if value is None or isinstance(value, model):
        return value if isinstance(value, model) else None
    if isinstance(value, dict):
        try:
            return model.model_validate(value)
        except ValidationError:
            return None
    return None


class AnswerData(BaseSchema):
    """Base class for structured answer payloads."""

    model_config = ALLOW_EXTRA_SCHEMA_CONFIG


class GenericAnswerData(AnswerData):
    """Fallback payload for legacy or untyped answer submissions."""

    payload: dict[str, Any] | None = Field(default=None)


class McqSingleAnswerData(AnswerData):
    """Learner answer for single-choice MCQ cards."""

    selected_option_id: str


class McqMultiAnswerData(AnswerData):
    """Learner answer for multi-select MCQ cards."""

    selected_option_ids: list[str] = Field(default_factory=list)


class WrittenAnswerData(AnswerData):
    """Learner answer containing free text."""

    text: str


class TrueFalseAnswerData(AnswerData):
    """Learner answer for true/false cards."""

    selected: bool


class ClozeAnswerData(AnswerData):
    """Learner responses for cloze deletions."""

    answers: list[str] = Field(default_factory=list)


class EmqAnswerData(AnswerData):
    """Learner selections for extended matching questions."""

    matches: list[EmqMatch] = Field(default_factory=list)


AnswerDataUnion = (
    McqSingleAnswerData
    | McqMultiAnswerData
    | WrittenAnswerData
    | TrueFalseAnswerData
    | ClozeAnswerData
    | EmqAnswerData
    | GenericAnswerData
)


ANSWER_TYPE_TO_MODEL: dict[str, type[AnswerData]] = {
    "mcq_single": McqSingleAnswerData,
    "mcq_multi": McqMultiAnswerData,
    "written": WrittenAnswerData,
    "true_false": TrueFalseAnswerData,
    "cloze": ClozeAnswerData,
    "emq": EmqAnswerData,
}


MODEL_TO_ANSWER_TYPE: dict[type[AnswerData], str] = {
    model: answer_type for answer_type, model in ANSWER_TYPE_TO_MODEL.items()
}


def _normalise_answer_type(answer_type: str | None) -> str:
    """Collapse aliases and missing answer types into a canonical discriminator."""

    if not answer_type:
        return "generic"
    return answer_type.strip().lower()


def parse_answer_data(answer_type: str | None, raw_data: Any) -> AnswerData:
    """Normalise persisted answer payloads into typed models."""

    if isinstance(raw_data, AnswerData):
        inferred = MODEL_TO_ANSWER_TYPE.get(type(raw_data))
        if inferred and answer_type is None:
            answer_type = inferred
        return raw_data

    if not isinstance(raw_data, dict):
        raise TypeError("Answer payload must be a JSON object.")

    normalised_type = _normalise_answer_type(answer_type)
    model = ANSWER_TYPE_TO_MODEL.get(normalised_type)
    if model is None:
        return GenericAnswerData(payload=raw_data)

    try:
        return model.model_validate(raw_data)
    except ValidationError as exc:  # pragma: no cover - surfaced to caller
        raise ValueError(f"Invalid answer payload for type {normalised_type}.") from exc


def canonical_answer_type(answer_type: str | None, data: AnswerData) -> str:
    """Return the canonical answer type string for a parsed answer payload."""
    inferred = MODEL_TO_ANSWER_TYPE.get(type(data))
    if inferred:
        return inferred
    return _normalise_answer_type(answer_type)


def serialize_answer_data(data: AnswerData | dict[str, Any]) -> dict[str, Any]:
    """Render payloads back to primitives for persistence."""

    if isinstance(data, BaseSchema):
        return data.model_dump(mode="json")
    if isinstance(data, dict):
        return data
    return {}


class AnswerPayload(BaseSchema):
    """Envelope for storing learner answers."""

    study_card_id: int
    answer_type: str = Field(default="generic", description="Answer type discriminator.")
    data: AnswerData | dict[str, Any] = Field(default_factory=dict)
    expected_answer: dict[str, Any] | None = Field(default=None)
    evaluation_notes: str | None = None
    is_correct: bool | None = Field(default=None)
    latency_ms: int | None = Field(default=None, ge=0)

    @model_validator(mode="before")
    @classmethod
    def _normalise(cls, values: Any) -> Any:
        if not isinstance(values, dict):
            return values
        raw_data = values.get("data")
        answer_type = values.get("answer_type")
        parsed = parse_answer_data(answer_type, raw_data)
        values["data"] = parsed
        values["answer_type"] = canonical_answer_type(answer_type, parsed)
        return values


class AnswerCreate(AnswerPayload):
    pass


class AnswerRead(AnswerPayload):
    """Answer payload returned by the API."""

    id: int
    user_id: str
    is_correct: bool | None = Field(default=None)
    created_at: datetime
    updated_at: datetime


class AnswerHistory(BaseSchema):
    """Paginated answer history for a learner."""

    items: list[AnswerRead]
    total: int
    page: int
    page_size: int


class AnswerStats(BaseSchema):
    """Aggregate answer statistics for a card."""

    study_card_id: int
    attempts: int
    correct: int
    accuracy: float


class StudySetProgress(BaseSchema):
    """Progress indicators for a learner within a study set."""

    study_set_id: int
    total_cards: int
    attempted_cards: int
    correct_cards: int
    accuracy: float
    last_answered_at: datetime | None


__all__ = [
    "AnswerCreate",
    "AnswerData",
    "AnswerDataUnion",
    "AnswerHistory",
    "AnswerPayload",
    "AnswerRead",
    "AnswerStats",
    "canonical_answer_type",
    "ClozeAnswerData",
    "EmqAnswerData",
    "GenericAnswerData",
    "McqMultiAnswerData",
    "McqSingleAnswerData",
    "parse_answer_data",
    "serialize_answer_data",
    "StudySetProgress",
    "TrueFalseAnswerData",
    "WrittenAnswerData",
]


================================================================================
# FILE: src/zistudy_api/domain/schemas/auth.py
================================================================================

from __future__ import annotations

from datetime import datetime

from pydantic import EmailStr, Field, SecretStr

from zistudy_api.domain.schemas.base import BaseSchema


class UserCreate(BaseSchema):
    email: EmailStr
    password: SecretStr = Field(..., min_length=8)
    full_name: str | None = Field(default=None, max_length=255)


class UserLogin(BaseSchema):
    email: EmailStr
    password: SecretStr


class UserRead(BaseSchema):
    id: str
    email: EmailStr
    full_name: str | None = None
    is_active: bool = True
    is_superuser: bool = False
    created_at: datetime
    updated_at: datetime


class TokenPair(BaseSchema):
    access_token: str
    refresh_token: str
    token_type: str = "bearer"
    expires_in: int = Field(..., description="Access token lifetime in seconds")


class RefreshRequest(BaseSchema):
    refresh_token: str


class TokenResponse(BaseSchema):
    access_token: str
    token_type: str = "bearer"
    expires_in: int


class SessionUser(BaseSchema):
    id: str
    email: EmailStr
    full_name: str | None = None
    is_superuser: bool = False
    scopes: list[str] = Field(default_factory=list)


class APIKeyCreate(BaseSchema):
    name: str | None = Field(default=None, max_length=255)
    expires_in_hours: int | None = Field(default=None, ge=1, le=24 * 365)


class APIKeyRead(BaseSchema):
    id: int
    key: str
    name: str | None = None
    created_at: datetime
    expires_at: datetime | None = None
    last_used_at: datetime | None = None


__all__ = [
    "APIKeyCreate",
    "APIKeyRead",
    "RefreshRequest",
    "SessionUser",
    "TokenPair",
    "TokenResponse",
    "UserCreate",
    "UserLogin",
    "UserRead",
]


================================================================================
# FILE: src/zistudy_api/domain/schemas/base.py
================================================================================

from __future__ import annotations

from datetime import datetime

from pydantic import BaseModel, ConfigDict, Field
from pydantic.generics import GenericModel

SCHEMA_CONFIG = ConfigDict(
    frozen=True,
    from_attributes=True,
    populate_by_name=True,
    extra="ignore",
)

ALLOW_EXTRA_SCHEMA_CONFIG = ConfigDict(
    frozen=True,
    from_attributes=True,
    populate_by_name=True,
    extra="allow",
)


class BaseSchema(BaseModel):
    """Base schema with shared configuration."""

    model_config = SCHEMA_CONFIG


class GenericSchema(GenericModel):
    """Generic-compatible schema base."""

    model_config = SCHEMA_CONFIG


class TimestampedSchema(BaseSchema):
    """Mixin schema providing timestamp fields."""

    created_at: datetime = Field(..., description="Creation timestamp in UTC.")
    updated_at: datetime = Field(..., description="Last update timestamp in UTC.")


__all__ = ["ALLOW_EXTRA_SCHEMA_CONFIG", "BaseSchema", "GenericSchema", "TimestampedSchema"]


================================================================================
# FILE: src/zistudy_api/domain/schemas/common.py
================================================================================

from __future__ import annotations

from typing import Any, Generic, Sequence, TypeVar

from pydantic import Field

from zistudy_api.domain.schemas.base import BaseSchema, GenericSchema

T = TypeVar("T")


class Pagination(BaseSchema):
    page: int = Field(1, ge=1)
    page_size: int = Field(20, ge=1, le=100)


class PaginatedResponse(GenericSchema, Generic[T]):
    items: Sequence[T] = Field(default_factory=list)
    total: int = Field(..., ge=0)
    page: int = Field(..., ge=1)
    page_size: int = Field(..., ge=1)


class ErrorBody(BaseSchema):
    code: int
    message: str
    details: dict[str, Any] | None = None


class ErrorEnvelope(BaseSchema):
    error: ErrorBody


__all__ = ["ErrorBody", "ErrorEnvelope", "Pagination", "PaginatedResponse"]


================================================================================
# FILE: src/zistudy_api/domain/schemas/jobs.py
================================================================================

from __future__ import annotations

from datetime import datetime
from enum import Enum

from zistudy_api.domain.schemas.base import BaseSchema


class JobStatus(str, Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"


class JobSummary(BaseSchema):
    id: int
    job_type: str
    status: JobStatus
    created_at: datetime
    updated_at: datetime
    started_at: datetime | None = None
    completed_at: datetime | None = None
    error: str | None = None
    result: dict | None = None


class JobCreateResponse(BaseSchema):
    job: JobSummary


__all__ = ["JobStatus", "JobSummary", "JobCreateResponse"]


================================================================================
# FILE: src/zistudy_api/domain/schemas/study_cards.py
================================================================================

"""Typed study card payloads and normalisation helpers."""

from __future__ import annotations

from typing import Annotated, Any, cast

from pydantic import Field, ValidationError, model_validator

from zistudy_api.domain.enums import CardType
from zistudy_api.domain.schemas.base import ALLOW_EXTRA_SCHEMA_CONFIG, BaseSchema, TimestampedSchema
from zistudy_api.domain.schemas.common import PaginatedResponse

CARD_GENERATOR_SCHEMA_VERSION = "1.0.0"
"""Semantic version identifier for the typed study card payload contract.

Increment this value whenever any card payload structure or metadata changes in a
backward-incompatible way so downstream consumers can react to schema upgrades.
"""

Difficulty = Annotated[int, Field(ge=1, le=5)]


class CardGeneratorMetadata(BaseSchema):
    """Describe the AI run responsible for generating a card payload."""

    model: str = Field(..., description="Identifier of the model that produced the card.")
    temperature: float | None = Field(default=None)
    requested_card_count: int | None = Field(default=None)
    topics: list[str] | None = Field(default=None)
    clinical_focus: list[str] | None = Field(default=None)
    learning_objectives: list[str] | None = Field(default=None)
    preferred_card_types: list[str] | None = Field(default=None)
    existing_card_ids: list[int] | None = Field(default=None)
    sources: list[str] | None = Field(default=None)
    schema_version: str = Field(
        default=CARD_GENERATOR_SCHEMA_VERSION,
        description="Semantic version of the structured study card payload schema.",
    )

    model_config = ALLOW_EXTRA_SCHEMA_CONFIG


class CardOption(BaseSchema):
    """Single multiple-choice option."""

    id: str
    text: str

    model_config = ALLOW_EXTRA_SCHEMA_CONFIG


class CardRationale(BaseSchema):
    """Structured explanation describing the correct and alternative answers."""

    primary: str
    alternatives: dict[str, str] = Field(default_factory=dict)

    model_config = ALLOW_EXTRA_SCHEMA_CONFIG


class BaseCardData(BaseSchema):
    """Common fields shared by all card payloads."""

    generator: CardGeneratorMetadata | None = Field(
        default=None, description="Metadata describing how the card was produced."
    )

    model_config = ALLOW_EXTRA_SCHEMA_CONFIG


class NoteCardData(BaseCardData):
    """Markdown note that complements active recall questions."""

    title: str
    markdown: str


class QuestionCardData(BaseCardData):
    """Base payload for questions that expect an answer from the learner."""

    prompt: str
    rationale: CardRationale | None = None
    glossary: dict[str, str] = Field(default_factory=dict)
    connections: list[str] = Field(default_factory=list)
    references: list[str] = Field(default_factory=list)
    numerical_ranges: list[str] = Field(default_factory=list)


class MultipleChoiceCardData(QuestionCardData):
    """Shared structure for MCQ style cards."""

    options: list[CardOption] = Field(default_factory=list)
    correct_option_ids: list[str] = Field(default_factory=list)

    @model_validator(mode="after")
    def _validate_option_ids(self) -> MultipleChoiceCardData:
        if not self.correct_option_ids:
            raise ValueError("At least one correct option identifier is required.")
        option_ids = {option.id for option in self.options}
        missing = [identifier for identifier in self.correct_option_ids if identifier not in option_ids]
        if missing:
            raise ValueError(f"Unknown option identifiers referenced: {missing}")
        return self


class McqSingleCardData(MultipleChoiceCardData):
    """Single-answer multiple choice question."""

    @model_validator(mode="after")
    def _validate_single(self) -> McqSingleCardData:
        if len(self.correct_option_ids) != 1:
            raise ValueError("MCQ single cards must have exactly one correct option identifier.")
        return self


class McqMultiCardData(MultipleChoiceCardData):
    """Multiple-answer multiple choice question."""

    @model_validator(mode="after")
    def _validate_multi(self) -> McqMultiCardData:
        if len(self.correct_option_ids) < 2:
            raise ValueError("MCQ multi cards must have at least two correct option identifiers.")
        return self


class WrittenCardData(QuestionCardData):
    """Written response question expecting a free-text answer."""

    expected_answer: str | None = Field(default=None)


class TrueFalseCardData(QuestionCardData):
    """True/false card."""

    correct_answer: bool


class ClozeCardData(QuestionCardData):
    """Cloze deletion card capturing the hidden tokens."""

    cloze_answers: list[str] = Field(default_factory=list)


class EmqMatch(BaseSchema):
    """Mapping between EMQ premise and option."""

    premise_index: int
    option_index: int

    model_config = ALLOW_EXTRA_SCHEMA_CONFIG


class EmqCardData(QuestionCardData):
    """Extended matching question payload."""

    instructions: str | None = Field(default=None)
    premises: list[str] = Field(default_factory=list)
    options: list[str] = Field(default_factory=list)
    matches: list[EmqMatch] = Field(default_factory=list)


class GenericCardData(BaseCardData):
    """Fallback container for payloads that we cannot normalise."""

    payload: dict[str, Any] | None = Field(default=None)


CardData = (
    NoteCardData
    | McqSingleCardData
    | McqMultiCardData
    | WrittenCardData
    | TrueFalseCardData
    | ClozeCardData
    | EmqCardData
    | GenericCardData
)


_CARD_TYPE_TO_MODEL: dict[CardType, type[BaseCardData]] = {
    CardType.NOTE: NoteCardData,
    CardType.MCQ_SINGLE: McqSingleCardData,
    CardType.MCQ_MULTI: McqMultiCardData,
    CardType.WRITTEN: WrittenCardData,
    CardType.TRUE_FALSE: TrueFalseCardData,
    CardType.CLOZE: ClozeCardData,
    CardType.EMQ: EmqCardData,
}


def _coerce_generator(value: Any) -> CardGeneratorMetadata | None:
    if value is None or isinstance(value, CardGeneratorMetadata):
        return value if isinstance(value, CardGeneratorMetadata) else None
    if isinstance(value, dict):
        return CardGeneratorMetadata.model_validate(value)
    return None


def parse_card_data(card_type: CardType | None, raw_data: Any) -> CardData:
    """Normalise stored payloads into the strongly typed card data models."""

    if isinstance(raw_data, BaseCardData):
        return cast(CardData, raw_data)

    if not isinstance(raw_data, dict):
        raise TypeError("Study card payload must be a JSON object.")

    if card_type is None:
        return GenericCardData(generator=_coerce_generator(raw_data.get("generator")), payload=raw_data)

    model = _CARD_TYPE_TO_MODEL.get(card_type)
    if model is None:
        return GenericCardData(generator=_coerce_generator(raw_data.get("generator")), payload=raw_data)

    try:
        return cast(CardData, model.model_validate(raw_data))
    except ValidationError as exc:  # pragma: no cover - surfaced to caller
        raise ValueError(f"Invalid study card payload for type {card_type.value}.") from exc


class StudyCardBase(BaseSchema):
    """Common fields for writable and readable study card representations."""

    card_type: CardType = Field(..., description="Discriminator for the study card type.")
    data: CardData = Field(..., description="Structured card content.")
    difficulty: Difficulty = Field(1, description="Difficulty on a 1â€“5 scale.")

    @model_validator(mode="before")
    @classmethod
    def _coerce_data_model(cls, values: Any) -> Any:
        if not isinstance(values, dict):
            return values
        raw_data = values.get("data")
        card_type_value = values.get("card_type")
        try:
            card_type_enum = (
                card_type_value
                if isinstance(card_type_value, CardType)
                else CardType(card_type_value)
            )
        except Exception:  # noqa: BLE001
            card_type_enum = None
        values["data"] = parse_card_data(card_type_enum, raw_data)
        return values


class StudyCardCreate(StudyCardBase):
    pass


class StudyCardUpdate(BaseSchema):
    """Partial update payload for study cards."""

    data: CardData | dict[str, Any] | None = None
    difficulty: Difficulty | None = None


class StudyCardRead(StudyCardBase, TimestampedSchema):
    """Card payload returned by the API and persistence layer."""

    id: int = Field(..., description="Primary identifier for the study card.")


class CardSearchFilters(BaseSchema):
    """Optional filters applied during study card search."""

    card_types: list[CardType] | None = Field(default=None)
    min_difficulty: Difficulty | None = Field(default=None)
    max_difficulty: Difficulty | None = Field(default=None)
    study_set_ids: list[int] | None = Field(default=None)


class CardSearchRequest(BaseSchema):
    """Request payload for full-text search across study cards."""

    query: str | None = Field(default=None, description="Free text to match against card data.")
    filters: CardSearchFilters = Field(default_factory=CardSearchFilters)
    page: int = Field(1, ge=1)
    page_size: int = Field(20, ge=1, le=100)


class CardSearchResult(BaseSchema):
    """Container bundling a card and its relevance metadata."""

    card: StudyCardRead
    score: float | None = Field(
        default=None, description="Relative relevance score when available."
    )
    snippet: str | None = Field(default=None, description="Optional highlighted snippet.")


class PaginatedStudyCardResults(PaginatedResponse[CardSearchResult]):
    pass


class StudyCardCollection(PaginatedResponse[StudyCardRead]):
    pass


class StudyCardImportPayload(BaseSchema):
    """Batch import payload used for AI generated cards."""

    cards: list[StudyCardCreate] = Field(..., min_length=1)


__all__ = [
    "CARD_GENERATOR_SCHEMA_VERSION",
    "CardSearchFilters",
    "CardSearchRequest",
    "CardSearchResult",
    "CardData",
    "CardGeneratorMetadata",
    "CardOption",
    "CardRationale",
    "QuestionCardData",
    "MultipleChoiceCardData",
    "McqSingleCardData",
    "McqMultiCardData",
    "WrittenCardData",
    "TrueFalseCardData",
    "ClozeCardData",
    "EmqCardData",
    "EmqMatch",
    "PaginatedStudyCardResults",
    "StudyCardCollection",
    "StudyCardCreate",
    "StudyCardImportPayload",
    "StudyCardRead",
    "StudyCardUpdate",
    "NoteCardData",
    "GenericCardData",
    "parse_card_data",
]


================================================================================
# FILE: src/zistudy_api/domain/schemas/study_sets.py
================================================================================

from __future__ import annotations

from typing import Annotated

from pydantic import Field, StringConstraints

from zistudy_api.domain.enums import CardType
from zistudy_api.domain.schemas.base import BaseSchema, TimestampedSchema
from zistudy_api.domain.schemas.common import PaginatedResponse
from zistudy_api.domain.schemas.study_cards import StudyCardRead
from zistudy_api.domain.schemas.tags import TagRead

TitleStr = Annotated[
    str,
    StringConstraints(strip_whitespace=True, min_length=1, max_length=255),
]

DescriptionStr = Annotated[
    str,
    StringConstraints(strip_whitespace=True, max_length=1000),
]


class StudySetBase(BaseSchema):
    title: TitleStr
    description: DescriptionStr | None = None
    is_private: bool = True


class StudySetCreate(StudySetBase):
    tag_names: list[str] = Field(default_factory=list, max_length=20)


class StudySetUpdate(BaseSchema):
    title: TitleStr | None = None
    description: DescriptionStr | None = None
    is_private: bool | None = None
    tag_names: list[str] | None = Field(default=None, description="Replace existing tags.")


class StudySetRead(StudySetBase, TimestampedSchema):
    id: int = Field(..., description="Study set identifier.", gt=0, examples=[1])
    owner_id: str | None = Field(default=None, description="Identifier of the owner if any.")

    def can_access(self, user_id: str | None) -> bool:
        if not self.is_private:
            return True
        if not self.owner_id or not user_id:
            return False
        return self.owner_id == user_id

    def can_modify(self, user_id: str | None) -> bool:
        if not self.owner_id and not user_id:
            return True
        if not self.owner_id or not user_id:
            return False
        return self.owner_id == user_id


class StudySetWithMeta(BaseSchema):
    study_set: StudySetRead
    tags: list[TagRead]
    card_count: int = Field(0, ge=0)
    question_count: int = Field(0, ge=0)
    owner_email: str | None = None


class StudySetForCard(BaseSchema):
    study_set: StudySetRead
    contains_card: bool
    card_count: int
    owner_email: str | None = None
    tags: list[TagRead] = Field(default_factory=list)


class AddCardsToSet(BaseSchema):
    study_set_id: int
    card_ids: list[int]
    card_type: CardType


class RemoveCardsFromSet(BaseSchema):
    study_set_id: int
    card_ids: list[int]
    card_type: CardType


class BulkAddToSets(BaseSchema):
    study_set_ids: list[int]
    card_ids: list[int]
    card_type: CardType


class PaginatedStudySets(PaginatedResponse[StudySetWithMeta]):
    pass


class StudySetCardEntry(BaseSchema):
    card: StudyCardRead
    position: int = Field(..., ge=0)


class StudySetCardsPage(PaginatedResponse[StudySetCardEntry]):
    pass


class BulkOperationResult(BaseSchema):
    success_count: int = Field(0, ge=0)
    error_count: int = Field(0, ge=0)
    errors: list[str] = Field(default_factory=list)
    affected_ids: list[int] = Field(default_factory=list)


class BulkDeleteStudySets(BaseSchema):
    study_set_ids: list[int] = Field(..., min_length=1)


class CloneStudySetsRequest(BaseSchema):
    study_set_ids: list[int] = Field(..., min_length=1)
    title_prefix: str | None = Field(default=None, max_length=255)


class ExportStudySetsRequest(BaseSchema):
    study_set_ids: list[int] = Field(..., min_length=1)


__all__ = [
    "AddCardsToSet",
    "BulkDeleteStudySets",
    "BulkAddToSets",
    "BulkOperationResult",
    "PaginatedStudySets",
    "StudySetCardsPage",
    "StudySetCardEntry",
    "RemoveCardsFromSet",
    "StudySetCreate",
    "StudySetForCard",
    "StudySetRead",
    "StudySetUpdate",
    "StudySetWithMeta",
    "CloneStudySetsRequest",
    "ExportStudySetsRequest",
]


================================================================================
# FILE: src/zistudy_api/domain/schemas/tags.py
================================================================================

from __future__ import annotations

from typing import Annotated

from pydantic import Field, StringConstraints

from zistudy_api.domain.schemas.base import BaseSchema, TimestampedSchema

TagName = Annotated[str, StringConstraints(strip_whitespace=True, min_length=1, max_length=64)]


class TagCreate(BaseSchema):
    name: TagName = Field(..., description="Human-friendly tag name.")


class TagRead(TimestampedSchema):
    id: int = Field(..., description="Tag identifier.")
    name: str = Field(..., description="Human-friendly tag name.")


class TagUsage(BaseSchema):
    tag: TagRead
    usage_count: int = Field(..., ge=0)


class TagSearchResponse(BaseSchema):
    items: list[TagRead]
    total: int


__all__ = ["TagCreate", "TagRead", "TagUsage", "TagSearchResponse"]


================================================================================
# FILE: src/zistudy_api/services/__init__.py
================================================================================



================================================================================
# FILE: src/zistudy_api/services/ai/__init__.py
================================================================================

from __future__ import annotations

from .agents import AgentConfiguration, AgentResult, StudyCardGenerationAgent
from .clients import GeminiGenerativeClient, GenerativeClient
from .generation_service import AiStudyCardService
from .pdf import DocumentIngestionService, PDFIngestionResult, UploadedPDF
from .pdf_strategies import IngestedPDFContextStrategy, NativePDFContextStrategy, PDFContextStrategy
from .prompts import STUDY_CARD_SYSTEM_PROMPT

__all__ = [
    "AiStudyCardService",
    "AgentConfiguration",
    "AgentResult",
    "DocumentIngestionService",
    "GeminiGenerativeClient",
    "GenerativeClient",
    "IngestedPDFContextStrategy",
    "NativePDFContextStrategy",
    "PDFContextStrategy",
    "PDFIngestionResult",
    "STUDY_CARD_SYSTEM_PROMPT",
    "UploadedPDF",
    "StudyCardGenerationAgent",
]


================================================================================
# FILE: src/zistudy_api/services/ai/agents.py
================================================================================

"""High level orchestration for Gemini driven study card generation."""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from typing import Iterable, Sequence

from pydantic import ValidationError

from zistudy_api.domain.schemas.ai import (
    AiGeneratedCard,
    AiGeneratedStudyCardSet,
    AiRetentionAid,
    StudyCardGenerationRequest,
)
from zistudy_api.services.ai.clients import (
    GeminiClientError,
    GeminiFilePart,
    GeminiInlineDataPart,
    GeminiMessage,
    GeminiTextPart,
    GenerationConfig,
    GenerativeClient,
)
from zistudy_api.services.ai.pdf import PDFIngestionResult, PDFTextSegment
from zistudy_api.services.ai.prompts import STUDY_CARD_SYSTEM_PROMPT

logger = logging.getLogger(__name__)


@dataclass(frozen=True, slots=True)
class AgentConfiguration:
    """Static defaults and safety bounds applied to each agent invocation."""

    default_model: str
    default_temperature: float
    default_card_count: int
    max_card_count: int
    max_attempts: int = 3


@dataclass(frozen=True, slots=True)
class AgentResult:
    """Outcome of a generation run, including any retention content."""

    cards: list[AiGeneratedCard]
    retention_aid: AiRetentionAid | None
    model_used: str
    temperature_applied: float
    requested_card_count: int


class StudyCardGenerationAgent:
    """Coordinates context preparation and Gemini invocation to build study cards."""

    def __init__(
        self,
        *,
        client: GenerativeClient,
        config: AgentConfiguration,
    ) -> None:
        self._client = client
        self._config = config

    @property
    def client(self) -> GenerativeClient:
        return self._client

    async def generate(
        self,
        request: StudyCardGenerationRequest,
        *,
        documents: Sequence[PDFIngestionResult],
        existing_questions: Sequence[str] | None = None,
        extra_parts: Sequence[GeminiTextPart | GeminiInlineDataPart | GeminiFilePart] = (),
    ) -> AgentResult:
        """Invoke Gemini with structured prompts and return the parsed result set.

        The agent preserves incremental feedback between attempts, enriches the prompt
        with PDF context, and now accepts both question and note cards emitted by the
        model while still tracking question stems to minimise duplication.
        """
        target_card_count = request.target_card_count or self._config.default_card_count
        target_card_count = min(target_card_count, self._config.max_card_count)
        temperature = request.temperature or self._config.default_temperature
        model = request.model or self._config.default_model

        schema_json = json.dumps(AiGeneratedStudyCardSet.model_json_schema(), indent=2, sort_keys=True)
        context_summaries = [
            f"- Existing question: {question.strip()}"
            for question in (existing_questions or [])
            if isinstance(question, str) and question.strip()
        ]

        generated_cards: list[AiGeneratedCard] = []
        retention_aid: AiRetentionAid | None = None
        feedback: str | None = None
        last_error: Exception | None = None

        logger.info(
            "Dispatching Gemini generation",
            extra={
                "requested_cards": target_card_count,
                "documents": len(documents),
                "existing_context": len(existing_questions or []),
                "model": model,
                "temperature": temperature,
            },
        )

        for attempt in range(1, self._config.max_attempts + 1):
            remaining = target_card_count - len(generated_cards)
            if remaining <= 0:
                break

            instructions = self._render_instruction_block(
                request,
                remaining,
                feedback,
            )
            schema_instruction = (
                f"{instructions}\n\nReturn a JSON document that matches the following schema:\n```json\n{schema_json}\n```"
            )
            parts: list[GeminiTextPart | GeminiInlineDataPart | GeminiFilePart] = [GeminiTextPart(schema_instruction)]
            parts.extend(self._render_document_parts(documents))
            if context_summaries:
                parts.append(GeminiTextPart(self._render_existing_cards_section(context_summaries)))
            parts.extend(extra_parts)

            logger.debug(
                "Invoking Gemini generate_json",
                extra={
                    "attempt": attempt,
                    "remaining": remaining,
                    "feedback_supplied": feedback is not None,
                    "extra_parts": len(extra_parts),
                },
            )

            try:
                generation_config = GenerationConfig(
                    temperature=temperature,
                    top_p=0.9,
                    top_k=32,
                    candidate_count=1,
                    max_output_tokens=6000,
                )
                payload = await self._client.generate_json(
                    system_instruction=STUDY_CARD_SYSTEM_PROMPT,
                    messages=[GeminiMessage(role="user", parts=parts)],
                    generation_config=generation_config,
                    model=model,
                )
                parsed = AiGeneratedStudyCardSet.model_validate(payload)
            except (GeminiClientError, ValidationError) as exc:
                last_error = exc
                feedback = (
                    "The previous response could not be processed. "
                    f"Error: {exc}. Please return valid JSON that matches the schema and contains new questions."
                )
                logger.warning(
                    "Gemini response invalid",
                    extra={
                        "attempt": attempt,
                        "reason": str(exc),
                        "remaining": remaining,
                    },
                )
                continue

            batch_new_cards: list[AiGeneratedCard] = []
            for card in parsed.cards:
                batch_new_cards.append(card)
                if card.card_type.is_question:
                    question = self._extract_question(card)
                    if question:
                        context_summaries.append(self._format_card_summary(card, question))

            if batch_new_cards:
                generated_cards.extend(batch_new_cards)
                logger.debug(
                    "Accepted new cards",
                    extra={
                        "attempt": attempt,
                        "batch_size": len(batch_new_cards),
                        "total_generated": len(generated_cards),
                    },
                )
                if request.include_retention_aid and parsed.retention_aid:
                    retention_aid = parsed.retention_aid
                remaining = target_card_count - len(generated_cards)
                if remaining <= 0:
                    break
                feedback = (
                    f"Received {len(batch_new_cards)} new card(s). "
                    f"{remaining} additional distinct card(s) are still required."
                )
            else:
                logger.debug(
                    "No distinct cards returned",
                    extra={
                        "attempt": attempt,
                        "feedback": True,
                    },
                )
                feedback = "No new distinct cards were produced. Provide entirely new questions that are not duplicates of the context."

        if len(generated_cards) < target_card_count:
            if last_error:
                logger.error(
                    "Gemini did not produce required cards",
                    extra={
                        "received": len(generated_cards),
                        "requested": target_card_count,
                        "error": str(last_error),
                    },
                )
                raise last_error
            raise GeminiClientError(
                "Failed to generate the requested number of distinct study cards."
            )

        cards = self._enforce_count(generated_cards, target_card_count)
        result_set = AiGeneratedStudyCardSet(cards=cards, retention_aid=retention_aid)
        logger.info(
            "Gemini generation completed",
            extra={
                "produced": len(cards),
                "retention_aid": retention_aid is not None,
                "model": model,
            },
        )
        return AgentResult(
            cards=list(result_set.cards),
            retention_aid=result_set.retention_aid,
            model_used=model,
            temperature_applied=temperature,
            requested_card_count=target_card_count,
        )

    def _render_instruction_block(
        self,
        request: StudyCardGenerationRequest,
        remaining_count: int,
        feedback: str | None,
    ) -> str:
        """Assemble the instruction header for the next Gemini attempt."""
        lines: list[str] = []
        lines.append(f"Generate {remaining_count} additional exam-ready study cards.")
        lines.append(f"Difficulty profile: {request.difficulty_profile}.")
        if request.preferred_card_types:
            joined_types = ", ".join(card_type.value for card_type in request.preferred_card_types)
            lines.append(f"Allowed card types: {joined_types}.")
        if request.topics:
            lines.append("Topics of emphasis:")
            lines.extend(f"- {topic}" for topic in request.topics)
        if request.clinical_focus:
            lines.append("Clinical focus areas:")
            lines.extend(f"- {focus}" for focus in request.clinical_focus)
        if request.learning_objectives:
            lines.append("Learning objectives:")
            lines.extend(f"- {objective}" for objective in request.learning_objectives)
        if request.learner_level:
            lines.append(f"Learner level: {request.learner_level}. Adapt nuance accordingly.")
        if request.context_hints:
            lines.append("Additional priorities:")
            lines.append(request.context_hints.strip())
        lines.append(
            "Ensure retention aids use expressive markdown with headings and emphasised cues."
        )
        lines.append("Return only JSON matching the enforced schema.")
        lines.append("Avoid duplicating any questions already shared in the context.")
        if feedback:
            lines.append(f"Previous feedback: {feedback}")
        return "\n".join(lines)

    def _render_document_parts(
        self,
        documents: Sequence[PDFIngestionResult],
    ) -> Iterable[GeminiTextPart | GeminiInlineDataPart]:
        """Render PDF segments and images into Gemini message parts."""
        for document in documents:
            if document.text_segments:
                text_buffer = [
                    f"# Source document: {document.filename or 'uploaded.pdf'} "
                    f"(pages={document.page_count})"
                ]
                text_buffer.extend(self._render_text_segments(document.text_segments))
                yield GeminiTextPart("\n".join(text_buffer))

            for image in document.images:
                yield GeminiInlineDataPart(
                    mime_type=image.mime_type,
                    data=image.data_base64,
                )

    @staticmethod
    def _render_text_segments(segments: Sequence[PDFTextSegment]) -> Iterable[str]:
        for segment in segments:
            yield f"[page {segment.page_index}] {segment.content}"

    @staticmethod
    def _enforce_count(cards: Sequence[AiGeneratedCard], target: int) -> list[AiGeneratedCard]:
        if len(cards) <= target:
            return list(cards)
        return list(cards[:target])

    @staticmethod
    def _render_existing_cards_section(context: Sequence[str]) -> str:
        buffer = ["Existing cards to avoid repeating:"]
        buffer.extend(context)
        return "\n".join(buffer)

    @staticmethod
    def _format_card_summary(card: AiGeneratedCard, question: str) -> str:
        """Return a summary used to avoid duplicates in later attempts."""
        return f"- {card.card_type.value} | difficulty {card.difficulty} | {question.strip()}"

    @staticmethod
    def _extract_question(card: AiGeneratedCard) -> str | None:
        """Extract the canonical question stem when available."""
        question = card.payload.question
        if isinstance(question, str) and question.strip():
            return question.strip()
        return None


__all__ = ["AgentConfiguration", "AgentResult", "StudyCardGenerationAgent"]


================================================================================
# FILE: src/zistudy_api/services/ai/clients.py
================================================================================

"""Gemini client abstractions used by the study card generation pipeline."""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from typing import (
    Mapping,
    MutableMapping,
    Protocol,
    Sequence,
    TypeAlias,
    Union,
    cast,
)

import httpx

MAX_INLINE_BYTES = 20 * 1024 * 1024

logger = logging.getLogger(__name__)

JSONPrimitive: TypeAlias = str | int | float | bool | None
JSONValue: TypeAlias = JSONPrimitive | list["JSONValue"] | dict[str, "JSONValue"]
JSONObject: TypeAlias = dict[str, JSONValue]


class GeminiClientError(RuntimeError):
    """Raised when a Gemini response cannot be parsed or indicates failure."""


@dataclass(frozen=True, slots=True)
class GeminiMessage:
    """Single Gemini message consisting of role-tagged parts."""

    role: str
    parts: Sequence["GeminiContentPart"]


@dataclass(frozen=True, slots=True)
class GeminiTextPart:
    """Plain text content part."""

    text: str


@dataclass(frozen=True, slots=True)
class GeminiInlineDataPart:
    """Inline base64 encoded payload part."""

    mime_type: str
    data: str


@dataclass(frozen=True, slots=True)
class GeminiFilePart:
    """Reference to a Gemini uploaded file."""

    mime_type: str
    file_uri: str


GeminiContentPart = Union[GeminiTextPart, GeminiInlineDataPart, GeminiFilePart]


class GenerationConfig:
    """Typed container for Gemini generation configuration parameters."""

    __slots__ = (
        "_temperature",
        "_top_p",
        "_top_k",
        "_candidate_count",
        "_max_output_tokens",
        "_response_mime_type",
        "_additional_parameters",
    )

    def __init__(
        self,
        *,
        temperature: float | None = None,
        top_p: float | None = None,
        top_k: int | None = None,
        candidate_count: int | None = None,
        max_output_tokens: int | None = None,
        response_mime_type: str | None = "application/json",
        additional_parameters: Mapping[str, JSONValue] | None = None,
    ) -> None:
        self._temperature = temperature
        self._top_p = top_p
        self._top_k = top_k
        self._candidate_count = candidate_count
        self._max_output_tokens = max_output_tokens
        self._response_mime_type = response_mime_type
        self._additional_parameters = dict(additional_parameters or {})

    def as_payload(self) -> JSONObject:
        """Render the config as the JSON payload expected by Gemini."""
        payload: JSONObject = {}
        if self._temperature is not None:
            payload["temperature"] = self._temperature
        if self._top_p is not None:
            payload["topP"] = self._top_p
        if self._top_k is not None:
            payload["topK"] = self._top_k
        if self._candidate_count is not None:
            payload["candidateCount"] = self._candidate_count
        if self._max_output_tokens is not None:
            payload["maxOutputTokens"] = self._max_output_tokens
        if self._response_mime_type:
            payload["responseMimeType"] = self._response_mime_type
        if self._additional_parameters:
            payload.update(ensure_json_object(self._additional_parameters))
        return payload


class GenerativeClient(Protocol):
    """Protocol representing the subset of Gemini client behaviour we rely on."""

    @property
    def default_model(self) -> str: ...

    @property
    def supports_file_uploads(self) -> bool: ...

    async def generate_json(
        self,
        *,
        system_instruction: str,
        messages: Sequence[GeminiMessage],
        response_schema: Mapping[str, JSONValue] | None = None,
        generation_config: GenerationConfig | None = None,
        model: str | None = None,
    ) -> Mapping[str, JSONValue]:
        ...

    async def upload_file(
        self,
        *,
        data: bytes,
        mime_type: str,
        display_name: str | None = None,
    ) -> str:
        ...

    async def aclose(self) -> None:
        ...


class GeminiGenerativeClient:
    """Thin async client for Google Gemini models."""

    def __init__(
        self,
        *,
        api_key: str,
        model: str,
        endpoint: str = "https://generativelanguage.googleapis.com/v1beta",
        timeout: float = 60.0,
        http_client: httpx.AsyncClient | None = None,
    ) -> None:
        if not api_key:
            raise ValueError("A Gemini API key is required.")
        self._api_key = api_key
        self._model = model
        self._client = http_client or httpx.AsyncClient(
            base_url=endpoint,
            timeout=httpx.Timeout(timeout),
            headers={
                "Content-Type": "application/json",
            },
        )
        self._owns_client = http_client is None
        base_url = httpx.URL(self._client.base_url if http_client else endpoint)
        self._root_url = base_url.copy_with(path="/")

    @property
    def default_model(self) -> str:
        return self._model

    @property
    def supports_file_uploads(self) -> bool:
        return True

    async def aclose(self) -> None:
        if self._owns_client:
            await self._client.aclose()

    async def generate_json(
        self,
        *,
        system_instruction: str,
        messages: Sequence[GeminiMessage],
        response_schema: Mapping[str, JSONValue] | None = None,
        generation_config: GenerationConfig | None = None,
        model: str | None = None,
    ) -> Mapping[str, JSONValue]:
        """Send a structured JSON generation request and return the decoded payload."""
        target_model = model or self._model
        path_component = target_model if target_model.startswith("models/") else f"models/{target_model}"
        url = f"/{path_component}:generateContent"
        instruction_payload = ensure_json_object({
            "parts": [{"text": system_instruction}],
        })
        contents_payload: list[JSONValue] = []
        for message in messages:
            part_payloads = [ensure_json_object(dict(self._serialise_part(part))) for part in message.parts]
            message_payload = ensure_json_object(
                {
                    "role": message.role,
                    "parts": part_payloads,
                }
            )
            contents_payload.append(message_payload)

        config_payload: JSONObject = ensure_json_object(
            generation_config.as_payload() if generation_config else {}
        )
        if "responseMimeType" not in config_payload:
            config_payload["responseMimeType"] = "application/json"
        if response_schema:
            config_payload["responseJsonSchema"] = _resolve_schema(ensure_json_object(response_schema))

        payload_dict: dict[str, JSONValue] = {
            "system_instruction": instruction_payload,
            "contents": contents_payload,
        }
        if config_payload:
            payload_dict["generationConfig"] = config_payload
        payload: JSONObject = ensure_json_object(payload_dict)

        response = await self._client.post(
            url,
            headers={"x-goog-api-key": self._api_key},
            json=payload,
        )
        try:
            response.raise_for_status()
        except httpx.HTTPStatusError as exc:  # pragma: no cover - relies on remote service
            error_summary = _summarize_response_error(exc.response)
            error_body = _extract_error_body(exc.response)
            logger.error(
                "Gemini request failed",
                extra={
                    "status_code": exc.response.status_code,
                    "url": str(exc.request.url),
                    "model": target_model,
                    "error_summary": error_summary,
                    "error_body": error_body,
                    "request_id": exc.response.headers.get("x-request-id"),
                },
            )
            raise GeminiClientError(
                f"Gemini request failed ({exc.response.status_code}): {error_summary}"
            ) from exc
        data = response.json()
        feedback = data.get("promptFeedback")
        if feedback and feedback.get("blockReason"):
            raise GeminiClientError(f"Gemini blocked the request: {feedback['blockReason']}")

        candidates = data.get("candidates", [])
        if not candidates:
            raise GeminiClientError("Gemini response did not contain any candidates.")

        candidate = candidates[0]
        finish_reason = candidate.get("finishReason")
        if finish_reason and finish_reason not in {"STOP", "FINISH"}:
            raise GeminiClientError(
                f"Gemini did not finish successfully (finishReason={finish_reason})."
            )

        content = candidate.get("content", {})
        parts = content.get("parts", [])
        for part in parts:
            if "json" in part:
                json_payload = part["json"]
                if isinstance(json_payload, dict):
                    return json_payload
            if "text" in part and isinstance(part["text"], str):
                return self._parse_text_json(part["text"])

        raise GeminiClientError("Unable to locate JSON payload in Gemini response.")

    async def upload_file(
        self,
        *,
        data: bytes,
        mime_type: str,
        display_name: str | None = None,
    ) -> str:
        """Upload binary data to Gemini's file API and return the resulting URI."""
        start_headers = {
            "x-goog-api-key": self._api_key,
            "X-Goog-Upload-Protocol": "resumable",
            "X-Goog-Upload-Command": "start",
            "X-Goog-Upload-Header-Content-Length": str(len(data)),
            "X-Goog-Upload-Header-Content-Type": mime_type,
            "Content-Type": "application/json",
        }
        start_payload = {
            "file": {
                "display_name": display_name or "uploaded.pdf",
            }
        }
        start_url = str(self._root_url.join("upload/v1beta/files"))
        start_resp = await self._client.post(start_url, headers=start_headers, json=start_payload)
        try:
            start_resp.raise_for_status()
        except httpx.HTTPStatusError as exc:  # pragma: no cover - relies on remote service
            error_summary = _summarize_response_error(exc.response)
            logger.error(
                "Gemini upload initialisation failed",
                extra={
                    "status_code": exc.response.status_code,
                    "url": str(exc.request.url),
                    "mime_type": mime_type,
                    "display_name": display_name,
                    "request_id": exc.response.headers.get("x-request-id"),
                    "error_summary": error_summary,
                },
            )
            raise GeminiClientError(
                f"Gemini upload initialisation failed ({exc.response.status_code}): {error_summary}"
            ) from exc
        upload_url = start_resp.headers.get("x-goog-upload-url")
        if not upload_url:
            raise GeminiClientError("Gemini did not provide an upload URL.")

        upload_headers = {
            "x-goog-api-key": self._api_key,
            "Content-Length": str(len(data)),
            "Content-Type": mime_type,
            "X-Goog-Upload-Offset": "0",
            "X-Goog-Upload-Command": "upload, finalize",
        }
        upload_resp = await self._client.post(upload_url, headers=upload_headers, content=data)
        try:
            upload_resp.raise_for_status()
        except httpx.HTTPStatusError as exc:  # pragma: no cover - relies on remote service
            error_summary = _summarize_response_error(exc.response)
            logger.error(
                "Gemini upload failed",
                extra={
                    "status_code": exc.response.status_code,
                    "url": str(exc.request.url),
                    "mime_type": mime_type,
                    "display_name": display_name,
                    "request_id": exc.response.headers.get("x-request-id"),
                    "error_summary": error_summary,
                },
            )
            raise GeminiClientError(
                f"Gemini upload failed ({exc.response.status_code}): {error_summary}"
            ) from exc
        payload = upload_resp.json()
        file_info = payload.get("file")
        if not file_info or "uri" not in file_info:
            raise GeminiClientError("Gemini upload response missing file URI.")
        return cast(str, file_info["uri"])

    def _parse_text_json(self, payload: str) -> Mapping[str, JSONValue]:
        """Parse a JSON object embedded inside a Gemini text part."""
        try:
            parsed = json.loads(payload)
        except json.JSONDecodeError as exc:  # pragma: no cover - defensive guard
            raise GeminiClientError("Gemini response was not valid JSON.") from exc
        if not isinstance(parsed, MutableMapping):
            raise GeminiClientError("Gemini response did not contain a JSON object.")
        return ensure_json_object(parsed)

    @staticmethod
    def _serialise_part(part: GeminiContentPart) -> Mapping[str, JSONValue]:
        """Convert strongly typed part structures into Gemini payload dictionaries."""
        if isinstance(part, GeminiTextPart):
            return {"text": part.text}
        if isinstance(part, GeminiInlineDataPart):
            return {
                "inlineData": {
                    "mimeType": part.mime_type,
                    "data": part.data,
                }
            }
        if isinstance(part, GeminiFilePart):
            return {
                "fileData": {
                    "mimeType": part.mime_type,
                    "fileUri": part.file_uri,
                }
            }
        raise GeminiClientError(f"Unsupported part type: {type(part)!r}")


__all__ = [
    "GeminiClientError",
    "GeminiGenerativeClient",
    "GenerativeClient",
    "GenerationConfig",
    "JSONValue",
    "JSONObject",
    "GeminiContentPart",
    "GeminiFilePart",
    "GeminiInlineDataPart",
    "GeminiMessage",
    "GeminiTextPart",
    "MAX_INLINE_BYTES",
    "ensure_json_object",
]


def _ensure_json_value(value: JSONValue | object, *, path: str = "root") -> JSONValue:
    """Validate nested JSON content and raise descriptive errors when invalid."""
    if isinstance(value, (str, int, float, bool)) or value is None:
        return value
    if isinstance(value, list):
        return [_ensure_json_value(item, path=f"{path}[{index}]") for index, item in enumerate(value)]
    if isinstance(value, tuple):
        return [
            _ensure_json_value(item, path=f"{path}[{index}]") for index, item in enumerate(value)
        ]
    if isinstance(value, dict):
        return ensure_json_object(cast(Mapping[str, JSONValue | object], value), path=path)
    raise GeminiClientError(f"Unsupported JSON value at {path}: {type(value)!r}")


def ensure_json_object(
    payload: Mapping[str, JSONValue | object],
    *,
    path: str = "root",
) -> JSONObject:
    """Coerce mappings into JSON dictionaries while validating keys and values."""
    result: JSONObject = {}
    for key, value in payload.items():
        if not isinstance(key, str):
            raise GeminiClientError(f"JSON keys must be strings (found {type(key)!r} at {path})")
        result[key] = _ensure_json_value(value, path=f"{path}.{key}")
    return result


def _resolve_schema(schema: JSONObject) -> JSONObject:
    """Inline $ref references so Gemini receives a fully-expanded schema."""

    if "$defs" not in schema and "$ref" not in schema:
        return schema

    defs_obj: dict[str, JSONValue] | None = None
    raw_defs = schema.get("$defs")
    if raw_defs is not None:
        if not isinstance(raw_defs, dict):
            raise GeminiClientError("Invalid JSON schema: $defs must be an object.")
        defs_obj = ensure_json_object(raw_defs)

    def _resolve(obj: JSONValue, *, trail: tuple[str, ...] = ()) -> JSONValue:
        if isinstance(obj, dict):
            if "$ref" in obj:
                ref_value = obj["$ref"]
                if not isinstance(ref_value, str):
                    raise GeminiClientError("Invalid JSON schema: $ref must be a string.")
                if not ref_value.startswith("#/$defs/"):
                    raise GeminiClientError(f"Unsupported $ref target: {ref_value}")
                ref_key = ref_value.split("/")[-1]
                if defs_obj is None or ref_key not in defs_obj:
                    raise GeminiClientError(f"Missing $defs entry for {ref_value}")
                if ref_value in trail:
                    raise GeminiClientError(f"Circular $ref detected for {ref_value}")
                return _resolve(defs_obj[ref_key], trail=trail + (ref_value,))

            return {
                key: _resolve(value, trail=trail)
                for key, value in obj.items()
                if key != "$defs"
            }
        if isinstance(obj, list):
            return [_resolve(item, trail=trail) for item in obj]
        return obj

    resolved: JSONObject = {
        key: _resolve(value)
        for key, value in schema.items()
        if key != "$defs"
    }
    return resolved


def _summarize_response_error(response: httpx.Response) -> str:
    """Provide a concise textual summary for logging Gemini HTTP errors."""
    try:
        payload = response.json()
    except ValueError:
        text = response.text.strip()
        return text or "No response body"

    if isinstance(payload, dict):
        error = payload.get("error")
        if isinstance(error, dict):
            message = error.get("message")
            status = error.get("status") or error.get("code")
            summary_parts = []
            if isinstance(status, str) and status:
                summary_parts.append(status)
            if isinstance(message, str) and message:
                summary_parts.append(message)
            return ": ".join(summary_parts) or "Gemini returned an error"
        message = payload.get("message")
        if isinstance(message, str) and message:
            return message
    if isinstance(payload, list):
        return f"Response contained {len(payload)} error item(s)"
    return json.dumps(payload)


def _extract_error_body(response: httpx.Response) -> JSONValue | str | None:
    """Return a JSON-serialisable body for failed Gemini responses."""
    try:
        payload = response.json()
    except ValueError:
        text = response.text.strip()
        return text or None

    try:
        if isinstance(payload, dict):
            return ensure_json_object(payload)
        if isinstance(payload, list):
            return [_ensure_json_value(item, path=f"[error][{index}]") for index, item in enumerate(payload)]
        return _ensure_json_value(payload)
    except GeminiClientError:
        return json.dumps(payload)


================================================================================
# FILE: src/zistudy_api/services/ai/generation_service.py
================================================================================

"""Service layer that glues PDF ingestion, Gemini agents, and persistence."""

from __future__ import annotations

import logging
from typing import Sequence

from pydantic import ValidationError
from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.repositories.study_cards import StudyCardRepository
from zistudy_api.domain.enums import CardType
from zistudy_api.domain.schemas.ai import (
    AiGeneratedCard,
    AiGeneratedStudyCardSet,
    AiRetentionAid,
    StudyCardGenerationRequest,
    StudyCardGenerationResult,
    StudyCardGenerationSummary,
)
from zistudy_api.domain.schemas.study_cards import (
    CARD_GENERATOR_SCHEMA_VERSION,
    CardData,
    CardGeneratorMetadata,
    CardOption,
    CardRationale,
    ClozeCardData,
    EmqCardData,
    EmqMatch,
    GenericCardData,
    McqMultiCardData,
    McqSingleCardData,
    NoteCardData,
    QuestionCardData,
    StudyCardCreate,
    StudyCardImportPayload,
    StudyCardRead,
    TrueFalseCardData,
    WrittenCardData,
    parse_card_data,
)
from zistudy_api.services.ai.agents import AgentResult, StudyCardGenerationAgent
from zistudy_api.services.ai.clients import GenerativeClient
from zistudy_api.services.ai.pdf import PDFIngestionResult, UploadedPDF
from zistudy_api.services.ai.pdf_strategies import PDFContextStrategy
from zistudy_api.services.study_cards import StudyCardService

logger = logging.getLogger(__name__)


class AiStudyCardService:
    """High-level faÃ§ade orchestrating ingestion, generation, and persistence."""

    def __init__(
        self,
        *,
        session: AsyncSession,
        agent: StudyCardGenerationAgent,
        pdf_strategy: PDFContextStrategy,
    ) -> None:
        self._session = session
        self._agent = agent
        self._pdf_strategy = pdf_strategy
        self._card_service = StudyCardService(session)
        self._client: GenerativeClient = agent.client

    async def generate_from_pdfs(
        self,
        request: StudyCardGenerationRequest,
        files: Sequence[UploadedPDF],
    ) -> StudyCardGenerationResult:
        """Ingest PDFs, invoke the generation agent, and persist the resulting cards."""
        logger.info(
            "Preparing AI study card generation",
            extra={
                "file_count": len(files),
                "strategy": type(self._pdf_strategy).__name__,
                "topics": request.topics,
                "target_cards": request.target_card_count,
            },
        )
        pdf_context = await self._pdf_strategy.build_context(files, client=self._client)
        documents = list(pdf_context.documents)
        logger.debug(
            "PDF context prepared",
            extra={
                "documents": len(documents),
                "extra_parts": len(pdf_context.extra_parts),
            },
        )
        existing_questions = await self._load_existing_questions(request.existing_card_ids)
        logger.debug(
            "Loaded existing questions",
            extra={"existing_questions": len(existing_questions)},
        )
        agent_result = await self._agent.generate(
            request,
            documents=documents,
            existing_questions=existing_questions,
            extra_parts=tuple(pdf_context.extra_parts),
        )
        logger.info(
            "AI agent returned result",
            extra={
                "generated_cards": len(agent_result.cards),
                "retention_aid": agent_result.retention_aid is not None,
            },
        )
        cards = await self._persist_generated_cards(
            agent_result.cards,
            request=request,
            documents=documents,
            meta=agent_result,
        )
        logger.info(
            "Persisted generated cards",
            extra={"created_cards": len(cards)},
        )

        summary = self._build_summary(documents, agent_result, cards_count=len(cards))
        retention_aid = agent_result.retention_aid if request.include_retention_aid else None
        raw = AiGeneratedStudyCardSet(
            cards=agent_result.cards,
            retention_aid=agent_result.retention_aid,
        )
        return StudyCardGenerationResult(
            cards=cards,
            retention_aid=retention_aid,
            summary=summary,
            raw_generation=raw,
        )

    async def _load_existing_questions(self, card_ids: Sequence[int]) -> list[str]:
        """Fetch question stems used to prevent duplicates."""
        if not card_ids:
            return []
        repository = StudyCardRepository(self._session)
        records = await repository.get_many(card_ids)
        questions: list[str] = []
        for record in records:
            card_type = CardType(record.card_type) if isinstance(record.card_type, str) else record.card_type
            question = self._extract_question_from_data(card_type, record.data)
            if question:
                questions.append(question)
        return questions

    async def _persist_generated_cards(
        self,
        generated_cards: Sequence[AiGeneratedCard],
        *,
        request: StudyCardGenerationRequest,
        documents: Sequence[PDFIngestionResult],
        meta: AgentResult,
    ) -> list[StudyCardRead]:
        """Persist AI generated cards and optional retention notes."""
        card_payloads = []
        generator_meta = self._card_generator_metadata(request, documents, meta)
        for card in generated_cards:
            data_model = self._map_card_to_data(card, generator_meta)
            card_payloads.append(
                StudyCardCreate(
                    card_type=card.card_type,
                    difficulty=card.difficulty,
                    data=data_model,
                )
            )
        include_retention_note = (
            meta.retention_aid is not None
            and request.include_retention_aid
            and (not request.preferred_card_types or CardType.NOTE in request.preferred_card_types)
        )
        if include_retention_note and meta.retention_aid:
            card_payloads.append(self._build_retention_note(meta.retention_aid, generator_meta))

        payload = StudyCardImportPayload(cards=card_payloads)
        created = await self._card_service.import_card_batch(payload)
        return created

    def _map_card_to_data(
        self,
        card: AiGeneratedCard,
        generator: CardGeneratorMetadata,
    ) -> CardData:
        """Transform a generated card into the typed domain payload.

        Raises:
            ValueError: If the model returns a note without markdown content or a
                resolvable title, ensuring invalid notes cannot be persisted silently.
        """
        payload = card.payload
        try:
            rationale = CardRationale.model_validate(payload.rationale.model_dump(mode="json"))
            glossary = payload.glossary or {}
            connections = payload.connections or []
            references = payload.references or []
            numerical_ranges = payload.numerical_ranges or []

            if card.card_type == CardType.MCQ_SINGLE:
                options = [CardOption(id=option.id, text=option.text) for option in payload.options or []]
                correct_ids = payload.correct_answers or ([options[0].id] if options else [])
                return McqSingleCardData(
                    generator=generator,
                    prompt=payload.question,
                    rationale=rationale,
                    glossary=glossary,
                    connections=connections,
                    references=references,
                    numerical_ranges=numerical_ranges,
                    options=options,
                    correct_option_ids=correct_ids,
                )

            if card.card_type == CardType.MCQ_MULTI:
                options = [CardOption(id=option.id, text=option.text) for option in payload.options or []]
                correct_ids = payload.correct_answers or [option.id for option in options[:2]]
                return McqMultiCardData(
                    generator=generator,
                    prompt=payload.question,
                    rationale=rationale,
                    glossary=glossary,
                    connections=connections,
                    references=references,
                    numerical_ranges=numerical_ranges,
                    options=options,
                    correct_option_ids=correct_ids,
                )

            if card.card_type == CardType.WRITTEN:
                expected = payload.correct_answers[0] if payload.correct_answers else None
                return WrittenCardData(
                    generator=generator,
                    prompt=payload.question,
                    rationale=rationale,
                    glossary=glossary,
                    connections=connections,
                    references=references,
                    numerical_ranges=numerical_ranges,
                    expected_answer=expected,
                )

            if card.card_type == CardType.TRUE_FALSE:
                answer = self._parse_boolean_answer(payload.correct_answers)
                if answer is None:
                    answer = True
                return TrueFalseCardData(
                    generator=generator,
                    prompt=payload.question,
                    rationale=rationale,
                    glossary=glossary,
                    connections=connections,
                    references=references,
                    numerical_ranges=numerical_ranges,
                    correct_answer=answer,
                )

            if card.card_type == CardType.CLOZE:
                return ClozeCardData(
                    generator=generator,
                    prompt=payload.question,
                    rationale=rationale,
                    glossary=glossary,
                    connections=connections,
                    references=references,
                    numerical_ranges=numerical_ranges,
                    cloze_answers=payload.correct_answers or [],
                )

            if card.card_type == CardType.EMQ:
                matches = []
                for index, option_id in enumerate(payload.correct_answers or []):
                    try:
                        matches.append(
                            EmqMatch.model_validate(
                                {"premise_index": index, "option_index": int(option_id)}
                            )
                        )
                    except Exception:  # noqa: BLE001
                        continue
                return EmqCardData(
                    generator=generator,
                    prompt=payload.question,
                    rationale=rationale,
                    glossary=glossary,
                    connections=connections,
                    references=references,
                    numerical_ranges=numerical_ranges,
                    instructions=payload.references[0] if payload.references else None,
                    premises=payload.connections,
                    options=[option.text for option in payload.options or []],
                    matches=matches,
                )

            if card.card_type == CardType.NOTE:
                markdown_raw = payload.rationale.primary or ""
                markdown = markdown_raw.strip()
                if not markdown:
                    raise ValueError("Generated note card contained empty markdown content.")
                title_value = payload.glossary.get("title") if payload.glossary else None
                if isinstance(title_value, str) and title_value.strip():
                    title = title_value.strip()
                else:
                    title = self._extract_heading(markdown) or "Note"
                title = title.strip()
                if not title:
                    raise ValueError("Generated note card resolved to a blank title.")
                return NoteCardData(generator=generator, title=title, markdown=markdown)
        except ValidationError as exc:  # pragma: no cover - defensive guard
            logger.debug(
                "Falling back to generic card data",
                extra={"card_type": card.card_type, "error": str(exc)},
            )

        return GenericCardData(
            generator=generator,
            payload=card.payload.model_dump(mode="json"),
        )

    @staticmethod
    def _parse_boolean_answer(correct_answers: Sequence[str]) -> bool | None:
        """Parse boolean answers returned as strings."""
        if not correct_answers:
            return None
        candidate = correct_answers[0].strip().lower()
        if candidate in {"true", "t", "1", "yes", "y"}:
            return True
        if candidate in {"false", "f", "0", "no", "n"}:
            return False
        return None

    def _build_retention_note(
        self,
        retention: AiRetentionAid,
        generator: CardGeneratorMetadata,
    ) -> StudyCardCreate:
        """Build a persistent note from the retention aid markdown."""
        markdown = (retention.markdown or "").strip()
        if not markdown:
            raise ValueError("Retention aids must include markdown content.")
        title = self._extract_heading(markdown) or "Retention Aid"
        data = NoteCardData(generator=generator, title=title.strip(), markdown=markdown)
        return StudyCardCreate(card_type=CardType.NOTE, difficulty=1, data=data)

    @staticmethod
    def _build_summary(
        documents: Sequence[PDFIngestionResult],
        meta: AgentResult,
        cards_count: int,
    ) -> StudyCardGenerationSummary:
        """Summarise the generation run for API consumers."""
        sources = [
            document.filename or f"uploaded-{index + 1}.pdf"
            for index, document in enumerate(documents)
        ]
        return StudyCardGenerationSummary(
            card_count=cards_count,
            sources=sources,
            model_used=meta.model_used,
            temperature_applied=meta.temperature_applied,
        )

    def _card_generator_metadata(
        self,
        request: StudyCardGenerationRequest,
        documents: Sequence[PDFIngestionResult],
        meta: AgentResult,
    ) -> CardGeneratorMetadata:
        """Produce generator metadata stored alongside each persisted card."""
        return CardGeneratorMetadata(
            model=meta.model_used,
            temperature=meta.temperature_applied,
            requested_card_count=meta.requested_card_count,
            topics=request.topics,
            clinical_focus=request.clinical_focus,
            learning_objectives=request.learning_objectives,
            preferred_card_types=[card_type.value for card_type in request.preferred_card_types],
            existing_card_ids=request.existing_card_ids,
            sources=[
                document.filename or f"uploaded-{index + 1}.pdf"
                for index, document in enumerate(documents)
            ],
            schema_version=CARD_GENERATOR_SCHEMA_VERSION,
        )

    @staticmethod
    def _extract_question_from_data(card_type: CardType | None, data: dict | None) -> str | None:
        """Extract the question stem from persisted card rows."""
        if not isinstance(data, dict):
            return None
        parsed = parse_card_data(card_type, data)
        if isinstance(parsed, QuestionCardData):
            return parsed.prompt.strip()
        return None

    @staticmethod
    def _extract_heading(markdown: str | None) -> str | None:
        """Return the first non-empty heading or line from markdown text."""
        if not markdown:
            return None
        for line in markdown.splitlines():
            stripped = line.strip()
            if stripped.startswith("#"):
                return stripped.lstrip("#").strip()
            if stripped:
                return stripped[:80].strip()
        return None


__all__ = ["AiStudyCardService", "UploadedPDF"]


================================================================================
# FILE: src/zistudy_api/services/ai/pdf.py
================================================================================

"""Utilities for reading PDFs into chunks suitable for LLM prompting."""

from __future__ import annotations

import base64
from dataclasses import dataclass
from typing import Iterable, Sequence

import anyio
import fitz  # type: ignore[import-untyped]


@dataclass(frozen=True, slots=True)
class PDFTextSegment:
    """Extracted text snippet paired with its originating page."""

    page_index: int
    content: str


@dataclass(frozen=True, slots=True)
class PDFImageFragment:
    """Inline-friendly representation of an image extracted from a PDF."""

    page_index: int
    mime_type: str
    data_base64: str


@dataclass(frozen=True, slots=True)
class PDFIngestionResult:
    """Result bundle returned by the ingestion pipeline."""

    filename: str | None
    text_segments: Sequence[PDFTextSegment]
    images: Sequence[PDFImageFragment]
    page_count: int


@dataclass(frozen=True, slots=True)
class UploadedPDF:
    """In-memory representation of an uploaded PDF file."""

    filename: str | None
    payload: bytes


class DocumentIngestionService:
    """Utilities for extracting structured context from PDF documents."""

    def __init__(
        self,
        *,
        text_chunk_size: int = 1_200,
        max_text_length: int = 18_000,
    ) -> None:
        self._text_chunk_size = text_chunk_size
        self._max_text_length = max_text_length

    async def ingest_pdf(
        self, payload: bytes, *, filename: str | None = None
    ) -> PDFIngestionResult:
        """Extract text and images from the provided PDF payload."""
        return await anyio.to_thread.run_sync(self._extract, payload, filename)

    def _extract(self, payload: bytes, filename: str | None) -> PDFIngestionResult:
        """Run the synchronous extraction process inside a worker thread."""
        document = fitz.open(stream=payload, filetype="pdf")
        try:
            text_segments: list[PDFTextSegment] = []
            images: list[PDFImageFragment] = []
            for page_index, page in enumerate(document, start=1):
                text = page.get_text("text").strip()
                if text:
                    text_segments.extend(self._chunk(page_index, text))

                for image in page.get_images(full=True):
                    xref = image[0]
                    pixmap = fitz.Pixmap(document, xref)
                    if pixmap.alpha or pixmap.colorspace is None:
                        pixmap = fitz.Pixmap(fitz.csRGB, pixmap)
                    data = pixmap.tobytes("png")

                    encoded = base64.b64encode(data).decode("ascii")
                    images.append(
                        PDFImageFragment(
                            page_index=page_index,
                            mime_type="image/png",
                            data_base64=encoded,
                        )
                    )

            cropped_segments = self._truncate(text_segments)
            return PDFIngestionResult(
                filename=filename,
                text_segments=cropped_segments,
                images=tuple(images),
                page_count=document.page_count,
            )
        finally:
            document.close()

    def _chunk(self, page_index: int, content: str) -> Iterable[PDFTextSegment]:
        """Split text into bounded segments to keep prompts concise."""
        sanitized = " ".join(content.split())
        size = self._text_chunk_size
        for start in range(0, len(sanitized), size):
            segment = sanitized[start : start + size].strip()
            if segment:
                yield PDFTextSegment(page_index=page_index, content=segment)

    def _truncate(self, segments: Sequence[PDFTextSegment]) -> Sequence[PDFTextSegment]:
        """Limit total text length to avoid overwhelming downstream prompts."""
        total = 0
        output: list[PDFTextSegment] = []
        for segment in segments:
            if total >= self._max_text_length:
                break
            remaining = self._max_text_length - total
            content = (
                segment.content
                if len(segment.content) <= remaining
                else segment.content[:remaining]
            )
            output.append(PDFTextSegment(page_index=segment.page_index, content=content))
            total += len(content)
        return tuple(output)


__all__ = [
    "DocumentIngestionService",
    "PDFImageFragment",
    "PDFIngestionResult",
    "UploadedPDF",
    "PDFTextSegment",
]


================================================================================
# FILE: src/zistudy_api/services/ai/pdf_strategies.py
================================================================================

"""Strategies that prepare Gemini-ready context from uploaded PDFs."""

from __future__ import annotations

import base64
import logging
from dataclasses import dataclass
from typing import Protocol, Sequence

from zistudy_api.services.ai.clients import (
    MAX_INLINE_BYTES,
    GeminiClientError,
    GeminiFilePart,
    GeminiInlineDataPart,
    GeminiTextPart,
    GenerativeClient,
)
from zistudy_api.services.ai.pdf import DocumentIngestionService, PDFIngestionResult, UploadedPDF

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class PDFContext:
    """Context bundle returned from a PDF strategy."""

    documents: Sequence[PDFIngestionResult]
    extra_parts: Sequence[GeminiTextPart | GeminiInlineDataPart | GeminiFilePart]


class PDFContextStrategy(Protocol):
    async def build_context(
        self,
        files: Sequence[UploadedPDF],
        *,
        client: GenerativeClient,
    ) -> PDFContext:
        """Create a context payload for the specified PDFs."""


class IngestedPDFContextStrategy(PDFContextStrategy):
    def __init__(self, ingestor: DocumentIngestionService) -> None:
        self._ingestor = ingestor

    async def build_context(
        self,
        files: Sequence[UploadedPDF],
        *,
        client: GenerativeClient,
    ) -> PDFContext:
        """Extract text from PDFs and ignore binary assets."""
        logger.info(
            "Building ingested PDF context",
            extra={"file_count": len(files)},
        )
        documents: list[PDFIngestionResult] = []
        for item in files:
            document = await self._ingestor.ingest_pdf(item.payload, filename=item.filename)
            logger.debug(
                "Ingested PDF",
                extra={
                    "pdf_filename": item.filename,
                    "pages": document.page_count,
                    "segments": len(document.text_segments),
                    "images": len(document.images),
                },
            )
            documents.append(document)
        return PDFContext(documents=tuple(documents), extra_parts=())


class NativePDFContextStrategy(PDFContextStrategy):
    def __init__(self, ingestor: DocumentIngestionService, inline_threshold: int = MAX_INLINE_BYTES) -> None:
        self._ingestor = ingestor
        self._inline_threshold = inline_threshold

    async def build_context(
        self,
        files: Sequence[UploadedPDF],
        *,
        client: GenerativeClient,
    ) -> PDFContext:
        """Embed small PDFs inline and upload large PDFs to Gemini's file API."""
        logger.info(
            "Building native PDF context",
            extra={
                "file_count": len(files),
                "inline_threshold": self._inline_threshold,
            },
        )
        documents: list[PDFIngestionResult] = []
        extras: list[GeminiInlineDataPart | GeminiFilePart] = []

        for item in files:
            document = await self._ingestor.ingest_pdf(item.payload, filename=item.filename)
            size_bytes = len(item.payload)
            if len(item.payload) > self._inline_threshold:
                try:
                    file_uri = await client.upload_file(
                        data=item.payload,
                        mime_type="application/pdf",
                        display_name=item.filename or "uploaded.pdf",
                    )
                except GeminiClientError as exc:
                    logger.warning(
                        "Gemini upload failed; falling back to extracted text",
                        extra={
                            "pdf_filename": item.filename,
                            "pdf_bytes": size_bytes,
                            "reason": str(exc),
                        },
                    )
                else:
                    logger.debug(
                        "Uploaded PDF via File API",
                        extra={
                            "pdf_filename": item.filename,
                            "pdf_bytes": size_bytes,
                            "file_uri": file_uri,
                        },
                    )
                    extras.append(
                        GeminiFilePart(
                            mime_type="application/pdf",
                            file_uri=file_uri,
                        )
                    )
            else:
                encoded = base64.b64encode(item.payload).decode("ascii")
                extras.append(
                    GeminiInlineDataPart(
                        mime_type="application/pdf",
                        data=encoded,
                    )
                )
                logger.debug(
                    "Embedded PDF inline",
                    extra={
                        "pdf_filename": item.filename,
                        "pdf_bytes": size_bytes,
                    },
                )
            documents.append(document)

        return PDFContext(documents=tuple(documents), extra_parts=tuple(extras))


__all__ = [
    "PDFContext",
    "PDFContextStrategy",
    "IngestedPDFContextStrategy",
    "NativePDFContextStrategy",
]


================================================================================
# FILE: src/zistudy_api/services/ai/prompts.py
================================================================================

"""Prompt templates used by the study card generation agents."""

from __future__ import annotations

STUDY_CARD_SYSTEM_PROMPT = """
You are ZiStudy's senior clinical educator. Generate high-yield study materials that maximise exam performance for advanced medical learners.

Core objectives:
- Prioritise essential topics and clinical presentations physicians repeatedly encounter or emphasise in board-style exams.
- Create multi-layered case studies that probe differential diagnoses, pathophysiology, investigation choices, management, and follow-up.
- Design questions that demand clinical reasoning and connections across organ systems, guidelines, and emerging evidence.
- Ensure retention aids highlight the must-remember elements that drive scores.

Content guidelines:
- Obey the JSON schema supplied with each request; populate every field thoughtfully rather than mechanically.
- Render the retention aid as expressive markdown (headings, tables, callouts, mnemonics) that learners can memorise quickly.
- For question cards, write concise yet information-dense stems, include realistic vitals/labs when relevant, and ensure the rationale teaches differential reasoning.
- Offer brief alternative rationales explaining why each distractor is incorrect, and surface cross-links or pitfalls in `connections`.
- When numeric values appear, annotate a clinically accepted range or threshold.
- Define uncommon terminology inside the glossary to compress revision time.
- Integrate any supplied excerpts or images directly into the teaching points; reference images explicitly if they affect the answer.

Tone and structure:
- Use active voice and exam-ready phrasing.
- Balance difficulty to test understanding, not trivia. Escalate complexity across the generated set.
- When guidelines differ internationally, name the guideline body or year to anchor the recommendation.

Safety and integrity:
- Never fabricate laboratory reference ranges; favour well-established values and note variability when relevant.
- Admit uncertainty if source material conflicts or is insufficient, and flag follow-up reading suggestions inside `payload.connections`.
""".strip()


__all__ = ["STUDY_CARD_SYSTEM_PROMPT"]


================================================================================
# FILE: src/zistudy_api/services/answers.py
================================================================================

from __future__ import annotations

from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.models import Answer
from zistudy_api.db.repositories.answers import AnswerRepository
from zistudy_api.db.repositories.study_cards import StudyCardRepository
from zistudy_api.domain.schemas.answers import (
    AnswerCreate,
    AnswerHistory,
    AnswerRead,
    AnswerStats,
    StudySetProgress,
    canonical_answer_type,
    parse_answer_data,
)


class AnswerService:
    """Business logic for recording and analysing answers."""

    def __init__(self, session: AsyncSession):
        self._session = session
        self._answers = AnswerRepository(session)
        self._cards = StudyCardRepository(session)

    async def submit_answer(self, *, user_id: str, payload: AnswerCreate) -> AnswerRead:
        """Persist an answer for a study card and return the typed read model."""
        card = await self._cards.get_by_id(payload.study_card_id)
        if card is None:
            raise KeyError(f"Study card {payload.study_card_id} not found")

        entity = await self._answers.create(user_id=user_id, payload=payload)
        await self._session.commit()
        return self._to_read_model(entity)

    async def get_answer(self, answer_id: int, *, user_id: str) -> AnswerRead:
        """Fetch a specific answer belonging to the requesting user."""
        entity = await self._answers.get_by_id(answer_id)
        if entity is None or entity.user_id != user_id:
            raise KeyError("Answer not found")
        return self._to_read_model(entity)

    async def list_history(
        self,
        *,
        user_id: str,
        page: int,
        page_size: int,
    ) -> AnswerHistory:
        """Return a paginated history of answers for the requesting user."""
        total, items = await self._answers.list_for_user(
            user_id=user_id,
            page=page,
            page_size=page_size,
        )
        return AnswerHistory(
            items=[self._to_read_model(item) for item in items],
            total=total,
            page=page,
            page_size=page_size,
        )

    async def stats_for_card(
        self, *, study_card_id: int, user_id: str | None = None
    ) -> AnswerStats:
        """Compute aggregate accuracy metrics for a study card."""
        total, correct = await self._answers.stats_for_card(
            study_card_id=study_card_id, user_id=user_id
        )
        accuracy = (correct / total) if total else 0.0
        return AnswerStats(
            study_card_id=study_card_id,
            attempts=total,
            correct=correct,
            accuracy=accuracy,
        )

    async def study_set_progress(
        self,
        *,
        user_id: str,
        study_set_ids: list[int],
    ) -> list[StudySetProgress]:
        """Summarise answer progress for the requested study sets."""
        aggregates = await self._answers.per_set_progress(
            user_id=user_id, study_set_ids=study_set_ids
        )
        progress: list[StudySetProgress] = []
        for set_id, total_cards, attempted, correct, last_answered in aggregates:
            accuracy = (correct / attempted) if attempted else 0.0
            progress.append(
                StudySetProgress(
                    study_set_id=set_id,
                    total_cards=total_cards,
                    attempted_cards=attempted,
                    correct_cards=correct,
                    accuracy=accuracy,
                    last_answered_at=last_answered,
                )
            )
        return progress

    def _to_read_model(self, entity: Answer) -> AnswerRead:
        """Convert a persisted answer entity into its typed API representation."""
        payload = entity.data if isinstance(entity.data, dict) else {}
        expected = payload.get("expected") if isinstance(payload, dict) else None
        notes = payload.get("evaluation_notes") if isinstance(payload, dict) else None
        latency = payload.get("latency_ms") if isinstance(payload, dict) else None
        answer_data = parse_answer_data(entity.answer_type, payload)
        answer_type = canonical_answer_type(entity.answer_type, answer_data)
        return AnswerRead(
            id=entity.id,
            user_id=entity.user_id,
            study_card_id=entity.study_card_id,
            answer_type=answer_type,
            data=answer_data,
            expected_answer=expected if isinstance(expected, dict) else None,
            evaluation_notes=notes if isinstance(notes, str) else None,
            is_correct=self._is_correct(entity.is_correct),
            latency_ms=int(latency) if isinstance(latency, (int, float)) else None,
            created_at=entity.created_at,
            updated_at=entity.updated_at,
        )

    @staticmethod
    def _is_correct(value: int | None) -> bool | None:
        if value is None:
            return None
        if value == 2:
            return None
        return value == 1


__all__ = ["AnswerService"]


================================================================================
# FILE: src/zistudy_api/services/auth.py
================================================================================

from __future__ import annotations

from datetime import datetime, timedelta, timezone

from fastapi import HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.config.settings import Settings, get_settings
from zistudy_api.core.security import (
    create_access_token,
    generate_api_key,
    generate_refresh_token,
    hash_password,
    hash_token,
    verify_password,
)
from zistudy_api.db.repositories.api_keys import ApiKeyRepository
from zistudy_api.db.repositories.refresh_tokens import RefreshTokenRepository
from zistudy_api.db.repositories.users import UserRepository
from zistudy_api.domain.schemas.auth import (
    APIKeyCreate,
    APIKeyRead,
    RefreshRequest,
    SessionUser,
    TokenPair,
    UserCreate,
    UserLogin,
    UserRead,
)


class AuthService:
    """Coordinate authentication, authorization, and token flows."""

    def __init__(
        self,
        *,
        session: AsyncSession,
        user_repository: UserRepository,
        refresh_tokens: RefreshTokenRepository,
        api_keys: ApiKeyRepository,
        settings: Settings | None = None,
    ) -> None:
        self._session = session
        self._users = user_repository
        self._refresh_tokens = refresh_tokens
        self._api_keys = api_keys
        self._settings = settings or get_settings()

    async def register_user(self, payload: UserCreate) -> UserRead:
        """Register a user account with hashed credentials."""
        existing = await self._users.get_by_email(payload.email)
        if existing is not None:
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT,
                detail="Email already in use",
            )

        password_hash = hash_password(payload.password.get_secret_value())
        entity = await self._users.create(
            email=payload.email,
            password_hash=password_hash,
            full_name=payload.full_name,
        )
        await self._session.commit()
        return UserRead.model_validate(entity)

    async def authenticate(self, credentials: UserLogin) -> TokenPair:
        """Validate credentials and issue an access/refresh token pair."""
        user = await self._users.get_by_email(credentials.email)
        if user is None or not verify_password(
            credentials.password.get_secret_value(), user.password_hash
        ):
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid credentials",
            )
        if not user.is_active:
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="User disabled")

        await self._users.touch_last_login(user.id)
        tokens = await self._issue_tokens(user_id=user.id, email=user.email, scopes=[])
        await self._session.commit()
        return tokens

    async def refresh(self, payload: RefreshRequest) -> TokenPair:
        """Rotate refresh tokens and return fresh access credentials."""
        token_str = payload.refresh_token
        token_hash = hash_token(token_str)
        record = await self._refresh_tokens.get_by_hash(token_hash)
        if record is None or record.revoked:
            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")

        expires_at = record.expires_at
        if expires_at.tzinfo is None:
            expires_at = expires_at.replace(tzinfo=timezone.utc)
        if expires_at <= datetime.now(tz=timezone.utc):
            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Token expired")

        await self._refresh_tokens.revoke(record.id)
        tokens = await self._issue_tokens(user_id=record.user_id)
        await self._session.commit()
        return tokens

    async def revoke_refresh_tokens(self, user_id: str) -> None:
        """Revoke all refresh tokens belonging to the specified user."""
        await self._refresh_tokens.revoke_all_for_user(user_id)
        await self._session.commit()

    async def create_api_key(self, user_id: str, payload: APIKeyCreate) -> APIKeyRead:
        """Create and return a new API key, including the plaintext secret."""
        key = generate_api_key(self._settings)
        key_hash = hash_token(key)
        entity = await self._api_keys.create(
            user_id=user_id,
            key_hash=key_hash,
            name=payload.name,
            expires_in_hours=payload.expires_in_hours,
        )
        await self._session.commit()
        return APIKeyRead(
            id=entity.id,
            key=key,
            name=entity.name,
            created_at=entity.created_at,
            expires_at=entity.expires_at,
            last_used_at=entity.last_used_at,
        )

    async def list_api_keys(self, user_id: str) -> list[APIKeyRead]:
        """List API keys for a user with masked secrets."""
        records = await self._api_keys.list_for_user(user_id)
        return [
            APIKeyRead(
                id=record.id,
                key="***masked***",
                name=record.name,
                created_at=record.created_at,
                expires_at=record.expires_at,
                last_used_at=record.last_used_at,
            )
            for record in records
        ]

    async def delete_api_key(self, user_id: str, api_key_id: int) -> None:
        """Delete an API key owned by the specified user."""
        records = await self._api_keys.list_for_user(user_id)
        if not any(record.id == api_key_id for record in records):
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="API key not found")
        await self._api_keys.delete(api_key_id)
        await self._session.commit()

    async def authenticate_api_key(self, api_key: str) -> SessionUser:
        """Resolve an API key into a session user, enforcing expiry rules."""
        key_hash = hash_token(api_key)
        record = await self._api_keys.get_by_hash(key_hash)
        if record is None:
            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid API key")
        if record.expires_at:
            expires_at = record.expires_at
            if expires_at.tzinfo is None:
                expires_at = expires_at.replace(tzinfo=timezone.utc)
            if expires_at <= datetime.now(tz=timezone.utc):
                raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="API key expired")
        await self._api_keys.touch_last_used(record.id)
        await self._session.commit()
        user = await self._users.get_by_id(record.user_id)
        if user is None or not user.is_active:
            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid API key")
        return SessionUser(
            id=user.id,
            email=user.email,
            full_name=user.full_name,
            is_superuser=user.is_superuser,
            scopes=["api:read", "api:write"],
        )

    async def parse_access_token(self, token: str) -> SessionUser:
        """Parse a JWT access token into a session user."""
        from zistudy_api.core.security import decode_token

        payload = decode_token(token, self._settings)
        user_id = payload.get("sub")
        if not isinstance(user_id, str):
            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")

        user = await self._users.get_by_id(user_id)
        if user is None or not user.is_active:
            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")

        scopes = payload.get("scopes", [])
        if not isinstance(scopes, list):
            scopes = []

        return SessionUser(
            id=user.id,
            email=user.email,
            full_name=user.full_name,
            is_superuser=user.is_superuser,
            scopes=[str(scope) for scope in scopes],
        )

    async def _issue_tokens(
        self,
        *,
        user_id: str,
        email: str | None = None,
        scopes: list[str] | None = None,
    ) -> TokenPair:
        if email is None:
            user = await self._users.get_by_id(user_id)
            if user is None:
                raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid user")
            email = user.email
            scopes = scopes or []
        expires_delta = timedelta(minutes=self._settings.access_token_exp_minutes)
        access_token = create_access_token(
            subject=user_id,
            settings=self._settings,
            claims={"email": email, "scopes": scopes or []},
            expires_delta=expires_delta,
        )
        refresh_token = generate_refresh_token(self._settings)
        refresh_hash = hash_token(refresh_token)
        refresh_exp = datetime.now(tz=timezone.utc) + timedelta(
            minutes=self._settings.refresh_token_exp_minutes
        )
        await self._refresh_tokens.create(
            token_hash=refresh_hash,
            user_id=user_id,
            expires_at=refresh_exp,
        )
        await self._session.flush()
        return TokenPair(
            access_token=access_token,
            refresh_token=refresh_token,
            token_type="bearer",
            expires_in=int(expires_delta.total_seconds()),
        )


__all__ = ["AuthService"]


================================================================================
# FILE: src/zistudy_api/services/job_processors.py
================================================================================

from __future__ import annotations

import asyncio
import base64
import logging
from datetime import datetime, timezone
from threading import Thread

from sqlalchemy.ext.asyncio import async_sessionmaker

from zistudy_api.celery_app import celery_app
from zistudy_api.config.settings import get_settings
from zistudy_api.db.repositories.jobs import JobRepository
from zistudy_api.db.session import get_sessionmaker
from zistudy_api.domain.schemas.ai import StudyCardGenerationRequest
from zistudy_api.domain.schemas.jobs import JobStatus
from zistudy_api.services.ai import (
    AgentConfiguration,
    AiStudyCardService,
    DocumentIngestionService,
    GeminiGenerativeClient,
    IngestedPDFContextStrategy,
    NativePDFContextStrategy,
    PDFContextStrategy,
    StudyCardGenerationAgent,
    UploadedPDF,
)
from zistudy_api.services.study_sets import StudySetService

SESSION_FACTORY: async_sessionmaker | None = None

logger = logging.getLogger(__name__)


def _factory() -> async_sessionmaker:
    global SESSION_FACTORY
    if SESSION_FACTORY is None:
        SESSION_FACTORY = get_sessionmaker()
    return SESSION_FACTORY


def _execute_async(coro) -> None:
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        asyncio.run(coro)
    else:
        result: dict[str, Exception | None] = {"error": None}

        def runner() -> None:
            try:
                asyncio.run(coro)
            except Exception as exc:  # pragma: no cover - propagated below
                result["error"] = exc

        thread = Thread(target=runner, daemon=True)
        thread.start()
        thread.join()
        if result["error"] is not None:
            raise result["error"]


@celery_app.task(name="jobs.process_clone_job")
def process_clone_job(job_id: int) -> None:
    _execute_async(_process_clone_job(job_id))


async def _process_clone_job(job_id: int) -> None:
    session_factory = _factory()
    async with session_factory() as session:
        job_repo = JobRepository(session)
        job = await job_repo.get(job_id)
        if job is None:
            return

        logger.info("AI generation job fetched", extra={"job_id": job_id, "owner_id": job.owner_id})
        payload = job.payload
        owner_id: str = payload["owner_id"]
        study_set_ids: list[int] = payload["study_set_ids"]
        title_prefix: str | None = payload.get("title_prefix")

        await job_repo.set_status(
            job_id,
            status=JobStatus.IN_PROGRESS.value,
            started_at=datetime.now(tz=timezone.utc),
        )
        await session.commit()

        study_set_service = StudySetService(session)
        try:
            new_ids = await study_set_service.clone_study_sets(
                study_set_ids=study_set_ids,
                owner_id=owner_id,
                title_prefix=title_prefix,
            )
            await job_repo.set_result(job_id, {"created_set_ids": new_ids})
            await job_repo.set_status(
                job_id,
                status=JobStatus.COMPLETED.value,
                completed_at=datetime.now(tz=timezone.utc),
            )
            await session.commit()
        except Exception as exc:  # pragma: no cover
            await job_repo.set_status(
                job_id,
                status=JobStatus.FAILED.value,
                completed_at=datetime.now(tz=timezone.utc),
                error=str(exc),
            )
            await session.commit()
            raise


@celery_app.task(name="jobs.process_export_job")
def process_export_job(job_id: int) -> None:
    _execute_async(_process_export_job(job_id))


async def _process_export_job(job_id: int) -> None:
    session_factory = _factory()
    async with session_factory() as session:
        job_repo = JobRepository(session)
        job = await job_repo.get(job_id)
        if job is None:
            return
        payload = job.payload
        owner_id: str = payload["owner_id"]
        study_set_ids: list[int] = payload["study_set_ids"]

        await job_repo.set_status(
            job_id,
            status=JobStatus.IN_PROGRESS.value,
            started_at=datetime.now(tz=timezone.utc),
        )
        await session.commit()

        study_set_service = StudySetService(session)
        try:
            export_payload = await study_set_service.export_study_sets(
                study_set_ids=study_set_ids,
                user_id=owner_id,
            )
            await job_repo.set_result(job_id, {"study_sets": export_payload})
            await job_repo.set_status(
                job_id,
                status=JobStatus.COMPLETED.value,
                completed_at=datetime.now(tz=timezone.utc),
            )
            await session.commit()
        except Exception as exc:  # pragma: no cover
            await job_repo.set_status(
                job_id,
                status=JobStatus.FAILED.value,
                completed_at=datetime.now(tz=timezone.utc),
                error=str(exc),
            )
            await session.commit()
            raise


@celery_app.task(name="jobs.process_ai_generation_job")
def process_ai_generation_job(job_id: int) -> None:
    _execute_async(_process_ai_generation_job(job_id))


async def _process_ai_generation_job(job_id: int) -> None:
    session_factory = _factory()
    async with session_factory() as session:
        job_repo = JobRepository(session)
        job = await job_repo.get(job_id)
        if job is None:
            return

        settings = get_settings()
        if not settings.gemini_api_key:
            await job_repo.set_status(
                job_id,
                status=JobStatus.FAILED.value,
                completed_at=datetime.now(tz=timezone.utc),
                error="Gemini API is not configured.",
            )
            await session.commit()
            return

        await job_repo.set_status(
            job_id,
            status=JobStatus.IN_PROGRESS.value,
            started_at=datetime.now(tz=timezone.utc),
        )
        await session.commit()

        payload = job.payload or {}
        request_payload = payload.get("request", {})
        document_payload = payload.get("documents", [])

        ingestion_service = DocumentIngestionService()
        client = GeminiGenerativeClient(
            api_key=settings.gemini_api_key,
            model=settings.gemini_model,
            endpoint=settings.gemini_endpoint,
            timeout=settings.gemini_request_timeout_seconds,
        )
        agent_config = AgentConfiguration(
            default_model=settings.gemini_model,
            default_temperature=settings.ai_generation_default_temperature,
            default_card_count=settings.ai_generation_default_card_count,
            max_card_count=settings.ai_generation_max_card_count,
            max_attempts=settings.ai_generation_max_attempts,
        )
        agent = StudyCardGenerationAgent(client=client, config=agent_config)
        pdf_strategy: PDFContextStrategy
        if settings.gemini_pdf_mode == "native":
            pdf_strategy = NativePDFContextStrategy(ingestor=ingestion_service)
        else:
            pdf_strategy = IngestedPDFContextStrategy(ingestor=ingestion_service)
        ai_service = AiStudyCardService(
            session=session,
            agent=agent,
            pdf_strategy=pdf_strategy,
        )

        try:
            request_model = StudyCardGenerationRequest.model_validate(request_payload)
            documents = [
                UploadedPDF(
                    filename=item.get("filename"),
                    payload=base64.b64decode(item["content"]),
                )
                for item in document_payload
                if isinstance(item, dict)
                and item.get("content")
            ]
            result = await ai_service.generate_from_pdfs(request_model, documents)
            await job_repo.set_result(job_id, result.model_dump(mode="json"))
            await job_repo.set_status(
                job_id,
                status=JobStatus.COMPLETED.value,
                completed_at=datetime.now(tz=timezone.utc),
            )
            await session.commit()
            card_count = getattr(getattr(result, "summary", None), "card_count", None)
            logger.info(
                "AI generation job completed",
                extra={"job_id": job_id, "card_count": card_count},
            )
        except Exception as exc:  # pragma: no cover
            await job_repo.set_status(
                job_id,
                status=JobStatus.FAILED.value,
                completed_at=datetime.now(tz=timezone.utc),
                error=str(exc),
            )
            await session.commit()
            logger.exception(
                "AI generation job failed",
                extra={"job_id": job_id, "error": str(exc)},
            )
            raise
        finally:
            await client.aclose()


__all__ = ["process_clone_job", "process_export_job", "process_ai_generation_job"]


================================================================================
# FILE: src/zistudy_api/services/jobs.py
================================================================================

from __future__ import annotations

from typing import Protocol

from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.repositories.jobs import JobRepository
from zistudy_api.domain.schemas.jobs import JobStatus, JobSummary


class ProcessorTask(Protocol):
    def delay(self, job_id: int) -> object: ...


class JobService:
    """Coordinate background job creation and retrieval."""

    def __init__(self, session: AsyncSession):
        self._session = session
        self._repository = JobRepository(session)

    async def enqueue(
        self,
        *,
        job_type: str,
        owner_id: str,
        payload: dict,
        processor_task: ProcessorTask,
    ) -> JobSummary:
        """Persist a job request and dispatch the configured processor task."""
        job = await self._repository.create(job_type=job_type, owner_id=owner_id, payload=payload)
        await self._repository.set_status(job.id, status=JobStatus.PENDING.value)
        await self._session.commit()

        processor_task.delay(job.id)

        return JobSummary(
            id=job.id,
            job_type=job.job_type,
            status=JobStatus.PENDING,
            created_at=job.created_at,
            updated_at=job.updated_at,
            started_at=job.started_at,
            completed_at=job.completed_at,
            error=job.error,
            result=None,
        )

    async def get_job(self, job_id: int, *, owner_id: str) -> JobSummary:
        """Fetch a job if it belongs to the supplied owner."""
        job = await self._repository.get(job_id)
        if job is None or job.owner_id != owner_id:
            raise KeyError(f"Job {job_id} not found")
        return JobSummary(
            id=job.id,
            job_type=job.job_type,
            status=JobStatus(job.status),
            created_at=job.created_at,
            updated_at=job.updated_at,
            started_at=job.started_at,
            completed_at=job.completed_at,
            error=job.error,
            result=job.result,
        )


__all__ = ["JobService"]


================================================================================
# FILE: src/zistudy_api/services/study_cards.py
================================================================================

from __future__ import annotations

from pydantic import TypeAdapter, ValidationError
from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.repositories.study_cards import StudyCardRepository
from zistudy_api.domain.enums import CardType
from zistudy_api.domain.schemas.study_cards import (
    CardSearchRequest,
    CardSearchResult,
    PaginatedStudyCardResults,
    StudyCardCollection,
    StudyCardCreate,
    StudyCardImportPayload,
    StudyCardRead,
    StudyCardUpdate,
)


class StudyCardService:
    """Business logic for study card operations."""

    def __init__(self, session: AsyncSession):
        self._session = session
        self._repository = StudyCardRepository(session)

    async def create_card(self, payload: StudyCardCreate) -> StudyCardRead:
        """Create a study card and return the typed read model."""
        entity = await self._repository.create(payload)
        await self._session.commit()
        return StudyCardRead.model_validate(entity)

    async def get_card(self, card_id: int) -> StudyCardRead:
        """Retrieve a single study card by identifier."""
        entity = await self._repository.get_by_id(card_id)
        if entity is None:
            raise KeyError(f"Study card {card_id} not found")
        return StudyCardRead.model_validate(entity)

    async def update_card(self, card_id: int, payload: StudyCardUpdate) -> StudyCardRead:
        """Apply updates to a study card and return the refreshed read model."""
        entity = await self._repository.update(card_id, payload)
        if entity is None:
            raise KeyError(f"Study card {card_id} not found")
        await self._session.commit()
        return StudyCardRead.model_validate(entity)

    async def delete_card(self, card_id: int) -> None:
        """Delete a study card by identifier."""
        deleted = await self._repository.delete(card_id)
        if not deleted:
            raise KeyError(f"Study card {card_id} not found")
        await self._session.commit()

    async def list_cards(
        self,
        *,
        card_type: CardType | None,
        page: int,
        page_size: int,
    ) -> StudyCardCollection:
        """Return a paginated collection of study cards filtered by type."""
        total, entities = await self._repository.list_cards(
            card_type=card_type, page=page, page_size=page_size
        )
        cards = [StudyCardRead.model_validate(entity) for entity in entities]
        return StudyCardCollection(
            items=cards,
            total=total,
            page=page,
            page_size=page_size,
        )

    async def search_cards(self, request: CardSearchRequest) -> PaginatedStudyCardResults:
        """Search study cards and include relevance metadata when available."""
        total, entities = await self._repository.search_cards(request)
        results = [
            CardSearchResult(
                card=StudyCardRead.model_validate(entity),
                score=None,
                snippet=None,
            )
            for entity in entities
        ]
        return PaginatedStudyCardResults(
            items=results,
            total=total,
            page=request.page,
            page_size=request.page_size,
        )

    async def list_cards_not_in_set(
        self,
        *,
        study_set_id: int,
        card_type: CardType | None,
        page: int,
        page_size: int,
    ) -> StudyCardCollection:
        """List cards not currently associated with the specified study set."""
        total, entities = await self._repository.list_not_in_set(
            study_set_id=study_set_id,
            card_type=card_type,
            page=page,
            page_size=page_size,
        )
        cards = [StudyCardRead.model_validate(entity) for entity in entities]
        return StudyCardCollection(items=cards, total=total, page=page, page_size=page_size)

    async def import_card_batch(self, payload: StudyCardImportPayload) -> list[StudyCardRead]:
        """Persist a batch of typed study cards."""
        entities = await self._repository.import_cards(payload.cards)
        await self._session.commit()
        return [StudyCardRead.model_validate(entity) for entity in entities]

    async def import_cards_from_json(self, json_data: str) -> list[StudyCardRead]:
        """Deserialize ``StudyCardCreate`` records from JSON and persist them."""
        adapter = TypeAdapter(list[StudyCardCreate])
        try:
            cards = adapter.validate_json(json_data)
        except ValidationError as exc:
            raise ValueError("Invalid card payload") from exc
        payload = StudyCardImportPayload(cards=list(cards))
        return await self.import_card_batch(payload)


__all__ = ["StudyCardService"]


================================================================================
# FILE: src/zistudy_api/services/study_sets.py
================================================================================

from __future__ import annotations

from collections.abc import Sequence
from typing import Any

from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.models import StudySet
from zistudy_api.db.repositories.study_cards import StudyCardRepository
from zistudy_api.db.repositories.study_sets import StudySetRepository
from zistudy_api.db.repositories.tags import TagRepository
from zistudy_api.domain.enums import CardCategory, CardType
from zistudy_api.domain.schemas.study_cards import StudyCardRead
from zistudy_api.domain.schemas.study_sets import (
    AddCardsToSet,
    BulkAddToSets,
    BulkOperationResult,
    StudySetCardEntry,
    StudySetCardsPage,
    StudySetCreate,
    StudySetForCard,
    StudySetRead,
    StudySetUpdate,
    StudySetWithMeta,
)
from zistudy_api.domain.schemas.tags import TagRead


class StudySetService:
    """Domain services orchestrating study set operations."""

    def __init__(self, session: AsyncSession):
        self._session = session
        self._study_sets = StudySetRepository(session)
        self._tags = TagRepository(session)
        self._cards = StudyCardRepository(session)

    async def create_study_set(
        self, payload: StudySetCreate, user_id: str | None
    ) -> StudySetWithMeta:
        """Create a study set with optional tags and return its metadata-rich view."""
        entity = await self._study_sets.create(payload, user_id)
        if payload.tag_names:
            tags = await self._tags.ensure_tags(payload.tag_names)
            await self._study_sets.attach_tags(entity, tags)
        await self._session.commit()
        return await self.get_study_set(entity.id)

    async def get_study_set(self, study_set_id: int) -> StudySetWithMeta:
        """Fetch a study set and expand associated metadata."""
        entity = await self._require_study_set(study_set_id)
        return await self._build_meta_response(entity)

    async def update_study_set(
        self, study_set_id: int, payload: StudySetUpdate
    ) -> StudySetWithMeta:
        """Update study set details and return the refreshed metadata wrapper."""
        entity = await self._require_study_set(study_set_id)
        await self._study_sets.update(entity, payload)
        if payload.tag_names is not None:
            tags = await self._tags.ensure_tags(payload.tag_names)
            await self._study_sets.attach_tags(entity, tags)
        await self._session.commit()
        await self._session.refresh(entity)
        return await self._build_meta_response(entity)

    async def delete_study_set(self, study_set_id: int) -> None:
        """Remove a study set from the repository."""
        entity = await self._require_study_set(study_set_id)
        await self._study_sets.delete(entity)
        await self._session.commit()

    async def list_accessible_study_sets(
        self,
        *,
        user_id: str | None,
        show_only_owned: bool,
        search_query: str | None,
        page: int,
        page_size: int,
    ) -> tuple[int, list[StudySetWithMeta]]:
        """Return accessible study sets for the caller with pagination metadata."""
        total, entities = await self._study_sets.list_accessible(
            current_user=user_id,
            show_only_owned=show_only_owned,
            search_query=search_query,
            page=page,
            page_size=page_size,
        )
        meta = [await self._build_meta_response(entity) for entity in entities]
        return total, meta

    async def can_modify(self, study_set_id: int, user_id: str | None) -> bool:
        """Determine whether a user is permitted to modify a study set."""
        entity = await self._study_sets.get_by_id(study_set_id)
        if entity is None:
            raise KeyError(f"Study set {study_set_id} not found")
        read_model = StudySetRead.model_validate(entity)
        return read_model.can_modify(user_id)

    async def add_cards(self, payload: AddCardsToSet) -> int:
        """Add cards to a study set, ensuring all IDs are valid."""
        entity = await self._require_study_set(payload.study_set_id)
        unique_ids = list(dict.fromkeys(payload.card_ids))
        cards = await self._cards.get_many(unique_ids)
        found_ids = {card.id for card in cards}
        if len(found_ids) != len(unique_ids):
            missing = set(unique_ids) - found_ids
            raise ValueError(f"Unknown card ids: {sorted(missing)}")

        category = payload.card_type.category
        added = await self._study_sets.add_cards(
            study_set_id=entity.id,
            card_ids=unique_ids,
            card_category=category,
        )
        await self._session.commit()
        return added

    async def remove_cards(
        self,
        study_set_id: int,
        card_ids: Sequence[int],
        card_type: CardType,
    ) -> int:
        """Remove cards from a study set by identifier."""
        await self._require_study_set(study_set_id)
        unique_ids = list(dict.fromkeys(card_ids))
        removed = await self._study_sets.remove_cards(
            study_set_id=study_set_id,
            card_ids=unique_ids,
            card_category=card_type.category,
        )
        await self._session.commit()
        return removed

    async def list_cards_in_set(
        self,
        *,
        study_set_id: int,
        card_type: CardType | None,
        page: int,
        page_size: int,
    ) -> StudySetCardsPage:
        """List the cards inside a study set with pagination support."""
        await self._require_study_set(study_set_id)
        total, entries = await self._study_sets.list_cards(
            study_set_id=study_set_id,
            card_type=card_type,
            page=page,
            page_size=page_size,
        )
        card_entries = [
            StudySetCardEntry(
                card=StudyCardRead.model_validate(card),
                position=position,
            )
            for card, position in entries
        ]
        return StudySetCardsPage(items=card_entries, total=total, page=page, page_size=page_size)

    async def bulk_add_cards(self, payload: BulkAddToSets) -> BulkOperationResult:
        """Add cards to multiple study sets and aggregate success/error counts."""
        errors: list[str] = []
        success = 0

        for study_set_id in payload.study_set_ids:
            try:
                await self._require_study_set(study_set_id)
                await self.add_cards(
                    AddCardsToSet(
                        study_set_id=study_set_id,
                        card_ids=payload.card_ids,
                        card_type=payload.card_type,
                    )
                )
                success += 1
            except (KeyError, ValueError) as exc:
                errors.append(f"Set {study_set_id}: {exc}")

        return BulkOperationResult(
            success_count=success,
            error_count=len(errors),
            errors=errors,
            affected_ids=payload.study_set_ids,
        )

    async def bulk_delete_study_sets(
        self,
        *,
        study_set_ids: Sequence[int],
        user_id: str | None,
    ) -> BulkOperationResult:
        """Delete multiple study sets, skipping sets the caller cannot modify."""
        errors: list[str] = []
        success = 0
        deleted: list[int] = []

        for study_set_id in study_set_ids:
            try:
                if not await self.can_modify(study_set_id, user_id):
                    raise PermissionError("Forbidden")
                await self.delete_study_set(study_set_id)
                deleted.append(study_set_id)
                success += 1
            except PermissionError as exc:
                errors.append(f"Set {study_set_id}: {exc}")
            except KeyError as exc:
                errors.append(f"Set {study_set_id}: {exc}")

        return BulkOperationResult(
            success_count=success,
            error_count=len(errors),
            errors=errors,
            affected_ids=deleted,
        )

    async def get_study_sets_for_card(
        self,
        *,
        card_id: int,
        user_id: str | None,
    ) -> list[StudySetForCard]:
        """Return study sets that contain the specified card and are accessible."""
        sets = await self._study_sets.list_for_card(card_id)
        responses: list[StudySetForCard] = []
        for entity in sets:
            meta = await self._build_meta_response(entity)
            read_model = meta.study_set
            if not read_model.can_access(user_id):
                continue
            contains_card = True
            responses.append(
                StudySetForCard(
                    study_set=read_model,
                    contains_card=contains_card,
                    card_count=meta.card_count,
                    owner_email=meta.owner_email,
                    tags=meta.tags,
                )
            )

        return responses

    async def _require_study_set(self, study_set_id: int) -> StudySet:
        entity = await self._study_sets.get_by_id(study_set_id)
        if entity is None:
            raise KeyError(f"Study set {study_set_id} not found")
        return entity

    async def _build_meta_response(self, entity: StudySet) -> StudySetWithMeta:
        await self._session.refresh(entity, attribute_names=["tags", "owner"])
        tags = [TagRead.model_validate(tag.tag) for tag in entity.tags if tag.tag]
        study_set = StudySetRead.model_validate(entity)
        counts = await self._study_sets.get_card_counts(entity.id)

        owner = entity.__dict__.get("owner")
        owner_email = owner.email if owner is not None else None

        return StudySetWithMeta(
            study_set=study_set,
            tags=tags,
            card_count=counts["total"],
            question_count=counts["questions"],
            owner_email=owner_email,
        )

    async def clone_study_sets(
        self,
        *,
        study_set_ids: Sequence[int],
        owner_id: str,
        title_prefix: str | None = None,
    ) -> list[int]:
        new_ids: list[int] = []

        for study_set_id in study_set_ids:
            original = await self._study_sets.get_by_id(study_set_id)
            if original is None:
                raise KeyError(f"Study set {study_set_id} not found")
            if not StudySetRead.model_validate(original).can_access(owner_id):
                raise PermissionError("Forbidden")

            new_title = original.title
            if title_prefix:
                new_title = f"{title_prefix}{new_title}"

            create_payload = StudySetCreate(
                title=new_title,
                description=original.description,
                is_private=original.is_private,
                tag_names=[],
            )

            clone = await self._study_sets.create(create_payload, owner_id)

            tag_names = [tag.tag.name for tag in original.tags if tag.tag]
            if tag_names:
                tags = await self._tags.ensure_tags(tag_names)
                await self._study_sets.attach_tags(clone, tags)

            cards_by_category: dict[CardCategory, list[int]] = {}
            for relation in original.cards:
                category = relation.card_category
                if not isinstance(category, CardCategory):
                    category = CardCategory(category)
                cards_by_category.setdefault(category, []).append(relation.card_id)

            for category, card_ids in cards_by_category.items():
                await self._study_sets.add_cards(
                    study_set_id=clone.id,
                    card_ids=card_ids,
                    card_category=category,
                )

            new_ids.append(clone.id)

        await self._session.commit()
        return new_ids

    async def export_study_sets(
        self,
        *,
        study_set_ids: Sequence[int],
        user_id: str,
    ) -> list[dict[str, Any]]:
        exports: list[dict[str, Any]] = []
        for study_set_id in study_set_ids:
            entity = await self._study_sets.get_by_id(study_set_id)
            if entity is None:
                raise KeyError(f"Study set {study_set_id} not found")
            read_model = StudySetRead.model_validate(entity)
            if not read_model.can_access(user_id):
                raise PermissionError("Forbidden")

            cards_with_meta = await self._study_sets.get_cards_with_details(study_set_id)
            meta = await self._build_meta_response(entity)
            exports.append(
                {
                    "study_set": meta.model_dump(mode="json"),
                    "cards": [
                        {
                            "card": StudyCardRead.model_validate(card).model_dump(mode="json"),
                            "position": relation.position,
                            "card_category": relation.card_category,
                        }
                        for relation, card in cards_with_meta
                    ],
                }
            )
        return exports


__all__ = ["StudySetService"]


================================================================================
# FILE: src/zistudy_api/services/tags.py
================================================================================

from __future__ import annotations

from sqlalchemy.ext.asyncio import AsyncSession

from zistudy_api.db.repositories.tags import TagRepository
from zistudy_api.domain.schemas.tags import TagRead, TagUsage


class TagService:
    """Expose tag operations."""

    def __init__(self, session: AsyncSession):
        self._session = session
        self._repository = TagRepository(session)

    async def ensure_tags(self, tag_names: list[str], *, commit: bool = False) -> list[TagRead]:
        """Create tags that do not yet exist and return the canonical set."""
        tags = await self._repository.ensure_tags(tag_names)
        if commit:
            await self._session.commit()
        return [TagRead.model_validate(tag) for tag in tags]

    async def list_tags(self, tag_names: list[str] | None = None) -> list[TagRead]:
        """List tags, optionally filtering by specific names."""
        tags = (
            await self._repository.list_all()
            if tag_names is None
            else await self._repository.list_by_names(tag_names)
        )
        return [TagRead.model_validate(tag) for tag in tags]

    async def search_tags(self, query: str, limit: int = 20) -> tuple[int, list[TagRead]]:
        """Search tags by query and return total hits plus the current page of tags."""
        total, tags = await self._repository.search(query=query, limit=limit)
        return total, [TagRead.model_validate(tag) for tag in tags]

    async def popular_tags(self, limit: int = 10) -> list[TagUsage]:
        """Return the most popular tags ranked by usage count."""
        rows = await self._repository.popular(limit=limit)
        return [TagUsage(tag=TagRead.model_validate(tag), usage_count=usage) for tag, usage in rows]


__all__ = ["TagService"]


================================================================================
# FILE: src/zistudy_api/tools.py
================================================================================

"""Developer utility commands exposed as CLI entry points."""

from __future__ import annotations

import argparse
import subprocess
import sys
from pathlib import Path
from typing import Iterable, Sequence

PROJECT_ROOT = Path(__file__).resolve().parents[2]
PYPROJECT = PROJECT_ROOT / "pyproject.toml"


def _run_command(command: Sequence[str]) -> int:
    result = subprocess.run(command, cwd=PROJECT_ROOT, check=False)
    return result.returncode


def _run_sequence(commands: Iterable[Sequence[str]]) -> int:
    for command in commands:
        exit_code = _run_command(command)
        if exit_code != 0:
            return exit_code
    return 0


def run_format() -> int:
    """Format the entire project using Ruff."""

    return _run_command([sys.executable, "-m", "ruff", "format", str(PROJECT_ROOT)])


def run_lint() -> int:
    """Run Ruff checks across the repository."""

    return _run_command([sys.executable, "-m", "ruff", "check", str(PROJECT_ROOT)])


def run_typecheck() -> int:
    """Execute mypy with the repository configuration."""

    return _run_command(
        [
            sys.executable,
            "-m",
            "mypy",
            "--config-file",
            str(PYPROJECT),
            "src",
            "tests",
        ]
    )


def run_test() -> int:
    """Lint, type-check, and run the test suite."""

    return _run_sequence(
        [
            [sys.executable, "-m", "ruff", "check", str(PROJECT_ROOT)],
            [
                sys.executable,
                "-m",
                "mypy",
                "--config-file",
                str(PYPROJECT),
                "src",
                "tests",
            ],
            [sys.executable, "-m", "pytest"],
        ]
    )


def main() -> None:
    """Dispatch helper allowing invocation via ``python -m zistudy_api.tools``."""

    parser = argparse.ArgumentParser(description="ZiStudy developer utilities")
    parser.add_argument(
        "command",
        choices={"format", "lint", "typecheck", "test"},
        help="Utility command to run",
    )
    args = parser.parse_args()

    mapping = {
        "format": run_format,
        "lint": run_lint,
        "typecheck": run_typecheck,
        "test": run_test,
    }
    sys.exit(mapping[args.command]())


if __name__ == "__main__":  # pragma: no cover - module invocation
    main()

